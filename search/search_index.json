{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Getting Started","text":"Wordcab Transcribe <p>\ud83d\udcac Speech recognition is now a commodity</p> <p>FastAPI based API for transcribing audio files using <code>faster-whisper</code> and Auto-Tuning-Spectral-Clustering for diarization (based on this GitHub implementation).</p> <p>Important</p> <p>If you want to see the great performance of Wordcab-Transcribe compared to all the available ASR tools on the market, please check out our benchmark project: Rate that ASR.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>\u26a1 Fast: The faster-whisper library and CTranslate2 make audio processing incredibly fast compared to other implementations.</li> <li>\ud83d\udc33 Easy to deploy: You can deploy the project on your workstation or in the cloud using Docker.</li> <li>\ud83d\udd25 Batch requests: You can transcribe multiple audio files at once because batch requests are implemented in the API.</li> <li>\ud83d\udcb8 Cost-effective: As an open-source solution, you won't have to pay for costly ASR platforms.</li> <li>\ud83e\udef6 Easy-to-use API: With just a few lines of code, you can use the API to transcribe audio files or even YouTube videos.</li> <li>\ud83e\udd17 Open-source (commercial-use under WTLv0.1 license, please reach out to <code>info@wordcab.com</code>): Our project is open-source and based on open-source libraries, allowing you to customize and extend it as needed until you don't sell this as a hosted service.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#local-development","title":"Local development","text":"<ul> <li>Linux (tested on Ubuntu Server 20.04/22.04)</li> <li>Python &gt;=3.8, &lt;3.12</li> <li>Hatch</li> <li>FFmpeg</li> </ul>"},{"location":"#deployment","title":"Deployment","text":"<ul> <li>Docker (optional for deployment)</li> <li>NVIDIA GPU + NVIDIA Container Toolkit (optional for deployment)</li> </ul>"},{"location":"#how-to-start","title":"How to start?","text":"<p>You need to clone the repository and install the dependencies:</p> <pre><code>git clone https://github.com/Wordcab/wordcab-transcribe.git\n\ncd wordcab-transcribe\n\nhatch env create\n</code></pre> <p>Then, you can start using the API. Head to the Usage section to learn more.</p>"},{"location":"contributing/","title":"\ud83d\ude80 Contributing","text":""},{"location":"contributing/#getting-started","title":"Getting started","text":"<ol> <li> <p>Ensure you have the <code>Hatch</code> installed (with pipx for example):</p> </li> <li> <p>hatch</p> </li> <li> <p>Clone the repo</p> </li> </ol> <pre><code>git clone\ncd wordcab-transcribe\n</code></pre> <ol> <li>Install dependencies and start coding</li> </ol> <pre><code>hatch env create\n</code></pre> <ol> <li>Run tests</li> </ol> <pre><code># Quality checks without modifying the code\nhatch run quality:check\n\n# Quality checks and auto-formatting\nhatch run quality:format\n\n# Run tests with coverage\nhatch run tests:run\n</code></pre>"},{"location":"contributing/#working-workflow","title":"Working workflow","text":"<ol> <li>Create an issue for the feature or bug you want to work on.</li> <li>Create a branch using the left panel on GitHub.</li> <li><code>git fetch</code>and <code>git checkout</code> the branch.</li> <li>Make changes and commit.</li> <li>Push the branch to GitHub.</li> <li>Create a pull request and ask for review.</li> <li>Merge the pull request when it's approved and CI passes.</li> <li>Delete the branch.</li> <li>Update your local repo with <code>git fetch</code> and <code>git pull</code>.</li> </ol>"},{"location":"license/","title":"\ud83d\udcdd License","text":"<p>The License prevents anyone from using this project after v0.4.0 (included) to sell a self-hosted version of this software without any agreements from Wordcab.</p> <p>Tip</p> <p>You can still use the project for research, personal use, or even as a backend tool for your projects.</p> <pre><code>Wordcab Transcribe License 0.1 (WTLv0.1)\n\nThis License Agreement governs the use of the Software and its Modifications.\nIt is a binding agreement between the Licensor and You.\n\nThis License Agreement shall be referred to as Wordcab Transcribe License 0.1\nor WTLv0.1. We may publish revised versions of this License Agreement from time\nto time. Each version will be given a distinguished number.\n\nBy downloading, accessing, modifying, distributing or otherwise using the\nSoftware, You consent to all of the terms and conditions below. So, if You do\nnot agree with those, please do not download, access, modify, distribute, or\nuse the Software.\n\n\n1. PERMISSIONS\n\nYou may use, modify and distribute the Software pursuant to the following terms\nand conditions:\n\nCopyright License. Subject to the terms and conditions of this License Agreement\nand where and as applicable, each Contributor hereby grants You a perpetual,\nworldwide, non-exclusive, royalty-free, copyright license to reproduce, prepare,\npublicly display, publicly perform, sublicense under the terms herein, and\ndistribute the Software and Modifications of the Software.\n\nPatent License. Subject to the terms and conditions of this License Agreement\nand where and as applicable, each Contributor hereby grants You a perpetual,\nworldwide, non-exclusive, royalty-free patent license to make, have made, Use,\nimport, and otherwise transfer the Software, where such license applies only to\nthose patent claims licensable by such Contributor that are necessarily\ninfringed by their Contribution(s) alone or by combination of their\nContribution(s) with the Software to which such Contribution(s) was submitted.\n\nIf You institute patent litigation against any entity (including a cross-claim\nor counterclaim in a lawsuit) alleging that the Software or a Contribution\nincorporated within the Software constitutes direct or contributory patent\ninfringement, then any rights granted to You under this License Agreement for\nthe Software shall terminate as of the date such litigation is filed.\n\nNo other rights. All rights not expressly granted herein are retained.\n\n\n2. RESTRICTIONS\n\nYou may not distribute the Software as a hosted or managed, and paid service,\nwhere the service grants users access to any substantial set of the features or\nfunctionality of the Software. If you wish to do so, You will need to be granted\nadditional rights from the Licensor which will be subject to a separate mutually\nagreed agreement.\n\nYou may not sublicense the Software under any other terms than those listed in\nthis License.\n\n\n3. OBLIGATIONS\n\nWhen You modify the Software, You agree to: - attach a notice stating the\nModifications of the Software You made; and - attach a notice stating that the\nModifications of the Software are released under this License Agreement.\n\nWhen You distribute the Software or Modifications of the Software, You agree to:\n- give any recipients of the Software a copy of this License Agreement;\n- retain all Explanatory Documentation; and if sharing the Modifications of the\nSoftware, add Explanatory Documentation documenting the changes made to create\nthe Modifications of the Software; -retain all copyright, patent, trademark and\nattribution notices.\n\n\n4. MISCELLANEOUS\n\nTermination. Licensor reserves the right to restrict Use of the Software in\nviolation of this License Agreement, upon which Your licenses will automatically\nterminate.\n\nContributions. Unless You explicitly state otherwise, any Contribution\nintentionally submitted for inclusion in the Software by You to the Licensor\nshall be under the terms and conditions of this License, without any additional\nterms or conditions. Notwithstanding the above, nothing herein shall supersede\nor modify the terms of any separate license agreement you may have executed with\nLicensor regarding such Contributions.\n\nTrademarks and related. Nothing in this License Agreement permits You (i) to\nmake Use of Licensors\u2019 trademarks, trade names, or logos, (ii) otherwise suggest\nendorsement by Licensor, or (iii) misrepresent the relationship between the\nparties; and any rights not expressly granted herein are reserved by the\nLicensors.\n\nOutput You generate. Licensor claims no rights in the Output. You agree not to\ncontravene any provision as stated in the License Agreement with your Use of the\nOutput.\n\nDisclaimer of Warranty. Except as expressly provided otherwise herein, and to\nthe fullest extent permitted by law, Licensor provides the Software (and each\nContributor provides its Contributions) AS IS, and Licensor disclaims all\nwarranties or guarantees of any kind, express or implied, whether arising under\nany law or from any usage in trade, or otherwise including but not limited to\nthe implied warranties of merchantability, non-infringement, quiet enjoyment,\nfitness for a particular purpose, or otherwise.\n\nYou are solely responsible for determining the appropriateness of the Software\nand Modifications of the Software for your purposes (including your use or\ndistribution of the Software and Modifications of the Software), and assume any\nrisks associated with Your exercise of permissions under this License Agreement.\n\nLimitation of Liability. In no event and under no legal theory, whether in tort\n(including negligence), contract, or otherwise, unless required by applicable\nlaw (such as deliberate and grossly negligent acts) or agreed to in writing,\nshall any Contributor be liable to You for damages, including any direct,\nindirect, special, incidental, or consequential damages of any character arising\nas a result of this License Agreement or out of the Use or inability to Use the\nSoftware (including but not limited to damages for loss of goodwill, work\nstoppage, computer failure or malfunction, model failure or malfunction, or\nany and all other commercial damages or losses), even if such Contributor has\nbeen advised of the possibility of such damages.\n\nAccepting Warranty or Additional Liability. While sharing the Software or\nModifications of the Software thereof, You may choose to offer and charge a fee\nfor, acceptance of support, warranty, indemnity, or other liability obligations\nand/or rights consistent with this\nLicense Agreement. However, in accepting such obligations, You may act only on\nYour own behalf and on Your sole responsibility, not on behalf of Licensor or\nany other Contributor, and you hereby agree to indemnify, defend, and hold\nLicensor and each other Contributor (and their successors or assigns) harmless\nfor any liability incurred by, or claims asserted against, such Licensor or\nContributor (and their successors or assigns) by reason of your accepting any\nsuch warranty or additional liability.\n\nSeverability. This License Agreement is a license of copyright and patent rights\nand an agreement in contract between You and the Licensor. If any provision of\nthis License Agreement is held to be invalid, illegal or unenforceable, the\nremaining provisions shall be unaffected thereby and remain valid as if such\nprovision had not been set forth herein.\n\n\n5. DEFINITIONS\n\n\u201cContribution\u201d refers to any work of authorship, including the original version\nof the Software and any Modifications of the Software that is intentionally\nsubmitted to Licensor for inclusion in the Software by the copyright owner or by\nan individual or entity authorized to submit on behalf of the copyright owner.\n\nFor the purposes of this definition, \u201csubmitted\u201d means any form of electronic,\nverbal, or written communication sent to the Licensor or its representatives,\nincluding but not limited to communication on electronic mailing lists, source\ncode control systems, and issue tracking systems that are managed by, or on\nbehalf of, the Licensor for the purpose of discussing and improving the\nSoftware, but excluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \u201cNot a Contribution.\u201d\n\n\u201cContributor\u201d refers to Licensor and any individual or entity on behalf of whom\na Contribution has been received by Licensor and subsequently incorporated\nwithin the Software.\n\n\u201cData\u201d refers to a collection of information extracted from the dataset used\nwith the Model, including to train, pretrain, or otherwise evaluate the Model.\nThe Data is not licensed under this License Agreement.\n\n\u201cExplanatory Documentation\u201d refers to any documentation or related information\nincluding but not limited to model cards or data cards dedicated to inform the\npublic about the characteristics of the Software. Explanatory documentation is\nnot licensed under this License.\n\n\"License Agreement\" refers to these terms and conditions.\n\n\u201cLicensor\u201d refers to the rights owners or entity authorized by the rights owners\nthat are granting the terms and conditions of this License Agreement.\n\n\u201cModel\u201d refers to machine-learning based assemblies (including checkpoints),\nconsisting of learnt weights and parameters (including optimizer states),\ncorresponding to a model architecture as embodied in Software source code.\nSource code is not licensed under this License Agreement.\n\n\u201cModifications of the Software\u201d refers to all changes to the Software, including\nwithout limitation derivative works of the Software.\n\n\u201cOutput\u201d refers to the results of operating the Software.\n\n\u201cShare\u201d refers to any transmission, reproduction, publication or other sharing\nof the Software or Modifications of the Software to a third party, including\nproviding the Softwaire as a hosted service made available by electronic or\nother remote means, including - but not limited to - API-based or web access.\n\n\u201cSoftware\u201d refers to the software and Model (or parts of either) that Licensor\nmakes available under this License Agreement.\n\n\u201cThird Parties\u201d refers to individuals or legal entities that are not under\ncommon control with Licensor or You.\n\n\u201cUse\u201d refers to anything You or your representatives do with the Software,\nincluding but not limited to generating any Output, fine tuning, updating,\nrunning, training, evaluating and/or reparametrizing the Model.\n\n\"You\" (or \"Your\")  refers to an individual or Legal Entity exercising\npermissions granted by this License Agreement and/or making Use of the Software\nfor whichever purpose and in any field of Use.\n</code></pre>"},{"location":"reference/config/","title":"Config","text":"<p>Configuration module of the Wordcab Transcribe.</p>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings","title":"<code>Settings</code>","text":"<p>Configuration settings for the Wordcab Transcribe API.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@dataclass\nclass Settings:\n    \"\"\"Configuration settings for the Wordcab Transcribe API.\"\"\"\n\n    # General configuration\n    project_name: str\n    version: str\n    description: str\n    api_prefix: str\n    debug: bool\n    # Models configuration\n    # Whisper\n    whisper_model: str\n    compute_type: str\n    extra_languages: Union[List[str], None]\n    extra_languages_model_paths: Union[Dict[str, str], None]\n    # Diarization\n    window_lengths: List[float]\n    shift_lengths: List[float]\n    multiscale_weights: List[float]\n    # ASR type configuration\n    asr_type: Literal[\"async\", \"live\", \"only_transcription\", \"only_diarization\"]\n    # Endpoint configuration\n    cortex_endpoint: bool\n    # API authentication configuration\n    username: str\n    password: str\n    openssl_key: str\n    openssl_algorithm: str\n    access_token_expire_minutes: int\n    # Cortex configuration\n    cortex_api_key: str\n    # Svix configuration\n    svix_api_key: str\n    svix_app_id: str\n    # Remote servers configuration\n    transcribe_server_urls: Union[List[str], None]\n    diarize_server_urls: Union[List[str], None]\n\n    @field_validator(\"project_name\")\n    def project_name_must_not_be_none(cls, value: str):  # noqa: B902, N805\n        \"\"\"Check that the project_name is not None.\"\"\"\n        if value is None:\n            raise ValueError(\n                \"`project_name` must not be None, please verify the `.env` file.\"\n            )\n\n        return value\n\n    @field_validator(\"version\")\n    def version_must_not_be_none(cls, value: str):  # noqa: B902, N805\n        \"\"\"Check that the version is not None.\"\"\"\n        if value is None:\n            raise ValueError(\n                \"`version` must not be None, please verify the `.env` file.\"\n            )\n\n        return value\n\n    @field_validator(\"description\")\n    def description_must_not_be_none(cls, value: str):  # noqa: B902, N805\n        \"\"\"Check that the description is not None.\"\"\"\n        if value is None:\n            raise ValueError(\n                \"`description` must not be None, please verify the `.env` file.\"\n            )\n\n        return value\n\n    @field_validator(\"api_prefix\")\n    def api_prefix_must_not_be_none(cls, value: str):  # noqa: B902, N805\n        \"\"\"Check that the api_prefix is not None.\"\"\"\n        if value is None:\n            raise ValueError(\n                \"`api_prefix` must not be None, please verify the `.env` file.\"\n            )\n\n        return value\n\n    @field_validator(\"compute_type\")\n    def compute_type_must_be_valid(cls, value: str):  # noqa: B902, N805\n        \"\"\"Check that the model precision is valid.\"\"\"\n        compute_type_values = [\n            \"int8\",\n            \"int8_float16\",\n            \"int8_bfloat16\",\n            \"int16\",\n            \"float16\",\n            \"bfloat16\",\n            \"float32\",\n        ]\n        if value not in compute_type_values:\n            raise ValueError(\n                f\"{value} is not a valid compute type. Choose one of\"\n                f\" {compute_type_values}.\"\n            )\n\n        return value\n\n    @field_validator(\"openssl_algorithm\")\n    def openssl_algorithm_must_be_valid(cls, value: str):  # noqa: B902, N805\n        \"\"\"Check that the OpenSSL algorithm is valid.\"\"\"\n        if value not in {\"HS256\", \"HS384\", \"HS512\"}:\n            raise ValueError(\n                \"openssl_algorithm must be a valid algorithm, please verify the `.env`\"\n                \" file.\"\n            )\n\n        return value\n\n    @field_validator(\"access_token_expire_minutes\")\n    def access_token_expire_minutes_must_be_valid(cls, value: int):  # noqa: B902, N805\n        \"\"\"Check that the access token expiration is valid. Only if debug is False.\"\"\"\n        if value &lt;= 0:\n            raise ValueError(\n                \"access_token_expire_minutes must be positive, please verify the `.env`\"\n                \" file.\"\n            )\n\n        return value\n\n    def __post_init__(self):\n        \"\"\"Post initialization checks.\"\"\"\n        if self.debug is False:\n            if self.username == \"admin\" or self.username is None:  # noqa: S105\n                logger.warning(\n                    f\"Username is set to `{self.username}`, which is not secure for\"\n                    \" production.\"\n                )\n            if self.password == \"admin\" or self.password is None:  # noqa: S105\n                logger.warning(\n                    f\"Password is set to `{self.password}`, which is not secure for\"\n                    \" production.\"\n                )\n            if (\n                self.openssl_key == \"0123456789abcdefghijklmnopqrstuvwyz\"  # noqa: S105\n                or self.openssl_key is None\n            ):\n                logger.warning(\n                    f\"OpenSSL key is set to `{self.openssl_key}`, which is the default\"\n                    \" encryption key. It's absolutely not secure for production.\"\n                    \" Please change it in the `.env` file. You can generate a new key\"\n                    \" with `openssl rand -hex 32`.\"\n                )\n\n        if (\n            len(self.window_lengths)\n            != len(self.shift_lengths)\n            != len(self.multiscale_weights)\n        ):\n            raise ValueError(\n                \"Length of window_lengths, shift_lengths and multiscale_weights must\"\n                f\" be the same.\\nFound: {len(self.window_lengths)},\"\n                f\" {len(self.shift_lengths)}, {len(self.multiscale_weights)}\"\n            )\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialization checks.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post initialization checks.\"\"\"\n    if self.debug is False:\n        if self.username == \"admin\" or self.username is None:  # noqa: S105\n            logger.warning(\n                f\"Username is set to `{self.username}`, which is not secure for\"\n                \" production.\"\n            )\n        if self.password == \"admin\" or self.password is None:  # noqa: S105\n            logger.warning(\n                f\"Password is set to `{self.password}`, which is not secure for\"\n                \" production.\"\n            )\n        if (\n            self.openssl_key == \"0123456789abcdefghijklmnopqrstuvwyz\"  # noqa: S105\n            or self.openssl_key is None\n        ):\n            logger.warning(\n                f\"OpenSSL key is set to `{self.openssl_key}`, which is the default\"\n                \" encryption key. It's absolutely not secure for production.\"\n                \" Please change it in the `.env` file. You can generate a new key\"\n                \" with `openssl rand -hex 32`.\"\n            )\n\n    if (\n        len(self.window_lengths)\n        != len(self.shift_lengths)\n        != len(self.multiscale_weights)\n    ):\n        raise ValueError(\n            \"Length of window_lengths, shift_lengths and multiscale_weights must\"\n            f\" be the same.\\nFound: {len(self.window_lengths)},\"\n            f\" {len(self.shift_lengths)}, {len(self.multiscale_weights)}\"\n        )\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.access_token_expire_minutes_must_be_valid","title":"<code>access_token_expire_minutes_must_be_valid(value)</code>","text":"<p>Check that the access token expiration is valid. Only if debug is False.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"access_token_expire_minutes\")\ndef access_token_expire_minutes_must_be_valid(cls, value: int):  # noqa: B902, N805\n    \"\"\"Check that the access token expiration is valid. Only if debug is False.\"\"\"\n    if value &lt;= 0:\n        raise ValueError(\n            \"access_token_expire_minutes must be positive, please verify the `.env`\"\n            \" file.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.api_prefix_must_not_be_none","title":"<code>api_prefix_must_not_be_none(value)</code>","text":"<p>Check that the api_prefix is not None.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"api_prefix\")\ndef api_prefix_must_not_be_none(cls, value: str):  # noqa: B902, N805\n    \"\"\"Check that the api_prefix is not None.\"\"\"\n    if value is None:\n        raise ValueError(\n            \"`api_prefix` must not be None, please verify the `.env` file.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.compute_type_must_be_valid","title":"<code>compute_type_must_be_valid(value)</code>","text":"<p>Check that the model precision is valid.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"compute_type\")\ndef compute_type_must_be_valid(cls, value: str):  # noqa: B902, N805\n    \"\"\"Check that the model precision is valid.\"\"\"\n    compute_type_values = [\n        \"int8\",\n        \"int8_float16\",\n        \"int8_bfloat16\",\n        \"int16\",\n        \"float16\",\n        \"bfloat16\",\n        \"float32\",\n    ]\n    if value not in compute_type_values:\n        raise ValueError(\n            f\"{value} is not a valid compute type. Choose one of\"\n            f\" {compute_type_values}.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.description_must_not_be_none","title":"<code>description_must_not_be_none(value)</code>","text":"<p>Check that the description is not None.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"description\")\ndef description_must_not_be_none(cls, value: str):  # noqa: B902, N805\n    \"\"\"Check that the description is not None.\"\"\"\n    if value is None:\n        raise ValueError(\n            \"`description` must not be None, please verify the `.env` file.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.openssl_algorithm_must_be_valid","title":"<code>openssl_algorithm_must_be_valid(value)</code>","text":"<p>Check that the OpenSSL algorithm is valid.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"openssl_algorithm\")\ndef openssl_algorithm_must_be_valid(cls, value: str):  # noqa: B902, N805\n    \"\"\"Check that the OpenSSL algorithm is valid.\"\"\"\n    if value not in {\"HS256\", \"HS384\", \"HS512\"}:\n        raise ValueError(\n            \"openssl_algorithm must be a valid algorithm, please verify the `.env`\"\n            \" file.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.project_name_must_not_be_none","title":"<code>project_name_must_not_be_none(value)</code>","text":"<p>Check that the project_name is not None.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"project_name\")\ndef project_name_must_not_be_none(cls, value: str):  # noqa: B902, N805\n    \"\"\"Check that the project_name is not None.\"\"\"\n    if value is None:\n        raise ValueError(\n            \"`project_name` must not be None, please verify the `.env` file.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/config/#src.wordcab_transcribe.config.Settings.version_must_not_be_none","title":"<code>version_must_not_be_none(value)</code>","text":"<p>Check that the version is not None.</p> Source code in <code>src/wordcab_transcribe/config.py</code> <pre><code>@field_validator(\"version\")\ndef version_must_not_be_none(cls, value: str):  # noqa: B902, N805\n    \"\"\"Check that the version is not None.\"\"\"\n    if value is None:\n        raise ValueError(\n            \"`version` must not be None, please verify the `.env` file.\"\n        )\n\n    return value\n</code></pre>"},{"location":"reference/schemas/","title":"Schemas","text":"<p>Models module of the Wordcab Transcribe.</p>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.AudioRequest","title":"<code>AudioRequest</code>","text":"<p>             Bases: <code>BaseRequest</code></p> <p>Request model for the ASR audio file and url endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class AudioRequest(BaseRequest):\n    \"\"\"Request model for the ASR audio file and url endpoint.\"\"\"\n\n    multi_channel: bool = False\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n                \"multi_channel\": False,\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.AudioRequest.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n            \"multi_channel\": False,\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.AudioResponse","title":"<code>AudioResponse</code>","text":"<p>             Bases: <code>BaseResponse</code></p> <p>Response model for the ASR audio file and url endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class AudioResponse(BaseResponse):\n    \"\"\"Response model for the ASR audio file and url endpoint.\"\"\"\n\n    multi_channel: bool\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"utterances\": [\n                    {\n                        \"text\": \"Hello World!\",\n                        \"start\": 0.345,\n                        \"end\": 1.234,\n                        \"speaker\": 0,\n                    },\n                    {\n                        \"text\": \"Wordcab is awesome\",\n                        \"start\": 1.234,\n                        \"end\": 2.678,\n                        \"speaker\": 1,\n                    },\n                ],\n                \"audio_duration\": 2.678,\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n                \"process_times\": {\n                    \"total\": 2.678,\n                    \"transcription\": 2.439,\n                    \"diarization\": None,\n                    \"post_processing\": 0.239,\n                },\n                \"multi_channel\": False,\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.AudioResponse.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"utterances\": [\n                {\n                    \"text\": \"Hello World!\",\n                    \"start\": 0.345,\n                    \"end\": 1.234,\n                    \"speaker\": 0,\n                },\n                {\n                    \"text\": \"Wordcab is awesome\",\n                    \"start\": 1.234,\n                    \"end\": 2.678,\n                    \"speaker\": 1,\n                },\n            ],\n            \"audio_duration\": 2.678,\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n            \"process_times\": {\n                \"total\": 2.678,\n                \"transcription\": 2.439,\n                \"diarization\": None,\n                \"post_processing\": 0.239,\n            },\n            \"multi_channel\": False,\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.BaseRequest","title":"<code>BaseRequest</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base request model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class BaseRequest(BaseModel):\n    \"\"\"Base request model for the API.\"\"\"\n\n    offset_start: Union[float, None] = None\n    offset_end: Union[float, None] = None\n    num_speakers: int = -1\n    diarization: bool = False\n    source_lang: str = \"en\"\n    timestamps: Timestamps = Timestamps.seconds\n    vocab: Union[List[str], None] = None\n    word_timestamps: bool = False\n    internal_vad: bool = False\n    repetition_penalty: float = 1.2\n    compression_ratio_threshold: float = 2.4\n    log_prob_threshold: float = -1.0\n    no_speech_threshold: float = 0.6\n    condition_on_previous_text: bool = True\n\n    @field_validator(\"vocab\")\n    def validate_each_vocab_value(\n        cls, value: Union[List[str], None]  # noqa: B902, N805\n    ) -&gt; List[str]:\n        \"\"\"Validate the value of each vocab field.\"\"\"\n        if value == []:\n            return None\n        elif value is not None and not all(isinstance(v, str) for v in value):\n            raise ValueError(\"`vocab` must be a list of strings.\")\n\n        return value\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.BaseRequest.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.BaseRequest.validate_each_vocab_value","title":"<code>validate_each_vocab_value(value)</code>","text":"<p>Validate the value of each vocab field.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>@field_validator(\"vocab\")\ndef validate_each_vocab_value(\n    cls, value: Union[List[str], None]  # noqa: B902, N805\n) -&gt; List[str]:\n    \"\"\"Validate the value of each vocab field.\"\"\"\n    if value == []:\n        return None\n    elif value is not None and not all(isinstance(v, str) for v in value):\n        raise ValueError(\"`vocab` must be a list of strings.\")\n\n    return value\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.BaseResponse","title":"<code>BaseResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base response model, not meant to be used directly.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class BaseResponse(BaseModel):\n    \"\"\"Base response model, not meant to be used directly.\"\"\"\n\n    utterances: List[Utterance]\n    audio_duration: float\n    offset_start: Union[float, None]\n    offset_end: Union[float, None]\n    num_speakers: int\n    diarization: bool\n    source_lang: str\n    timestamps: str\n    vocab: Union[List[str], None]\n    word_timestamps: bool\n    internal_vad: bool\n    repetition_penalty: float\n    compression_ratio_threshold: float\n    log_prob_threshold: float\n    no_speech_threshold: float\n    condition_on_previous_text: bool\n    process_times: ProcessTimes\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexError","title":"<code>CortexError</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Error model for the Cortex API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class CortexError(BaseModel):\n    \"\"\"Error model for the Cortex API.\"\"\"\n\n    message: str\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"message\": \"Error message here\",\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexError.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"message\": \"Error message here\",\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexPayload","title":"<code>CortexPayload</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request object for Cortex endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class CortexPayload(BaseModel):\n    \"\"\"Request object for Cortex endpoint.\"\"\"\n\n    url_type: Literal[\"audio_url\", \"youtube\"]\n    url: Optional[str] = None\n    api_key: Optional[str] = None\n    offset_start: Optional[float] = None\n    offset_end: Optional[float] = None\n    num_speakers: Optional[int] = -1\n    diarization: Optional[bool] = False\n    multi_channel: Optional[bool] = False\n    source_lang: Optional[str] = \"en\"\n    timestamps: Optional[Timestamps] = Timestamps.seconds\n    vocab: Union[List[str], None] = None\n    word_timestamps: Optional[bool] = False\n    internal_vad: Optional[bool] = False\n    repetition_penalty: Optional[float] = 1.2\n    compression_ratio_threshold: Optional[float] = 2.4\n    log_prob_threshold: Optional[float] = -1.0\n    no_speech_threshold: Optional[float] = 0.6\n    condition_on_previous_text: Optional[bool] = True\n    job_name: Optional[str] = None\n    ping: Optional[bool] = False\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"url_type\": \"youtube\",\n                \"url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n                \"api_key\": \"1234567890\",\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"multi_channel\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n                \"job_name\": \"job_abc123\",\n                \"ping\": False,\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexPayload.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"url_type\": \"youtube\",\n            \"url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n            \"api_key\": \"1234567890\",\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"multi_channel\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n            \"job_name\": \"job_abc123\",\n            \"ping\": False,\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexUrlResponse","title":"<code>CortexUrlResponse</code>","text":"<p>             Bases: <code>AudioResponse</code></p> <p>Response model for the audio_url type of the Cortex endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class CortexUrlResponse(AudioResponse):\n    \"\"\"Response model for the audio_url type of the Cortex endpoint.\"\"\"\n\n    job_name: str\n    request_id: Optional[str] = None\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"utterances\": [\n                    {\n                        \"speaker\": 0,\n                        \"start\": 0.0,\n                        \"end\": 1.0,\n                        \"text\": \"Hello World!\",\n                    },\n                    {\n                        \"speaker\": 0,\n                        \"start\": 1.0,\n                        \"end\": 2.0,\n                        \"text\": \"Wordcab is awesome\",\n                    },\n                ],\n                \"audio_duration\": 2.0,\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n                \"process_times\": {\n                    \"total\": 2.678,\n                    \"transcription\": 2.439,\n                    \"diarization\": None,\n                    \"post_processing\": 0.239,\n                },\n                \"multi_channel\": False,\n                \"job_name\": \"job_name\",\n                \"request_id\": \"request_id\",\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexUrlResponse.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"utterances\": [\n                {\n                    \"speaker\": 0,\n                    \"start\": 0.0,\n                    \"end\": 1.0,\n                    \"text\": \"Hello World!\",\n                },\n                {\n                    \"speaker\": 0,\n                    \"start\": 1.0,\n                    \"end\": 2.0,\n                    \"text\": \"Wordcab is awesome\",\n                },\n            ],\n            \"audio_duration\": 2.0,\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n            \"process_times\": {\n                \"total\": 2.678,\n                \"transcription\": 2.439,\n                \"diarization\": None,\n                \"post_processing\": 0.239,\n            },\n            \"multi_channel\": False,\n            \"job_name\": \"job_name\",\n            \"request_id\": \"request_id\",\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexYoutubeResponse","title":"<code>CortexYoutubeResponse</code>","text":"<p>             Bases: <code>YouTubeResponse</code></p> <p>Response model for the YouTube type of the Cortex endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class CortexYoutubeResponse(YouTubeResponse):\n    \"\"\"Response model for the YouTube type of the Cortex endpoint.\"\"\"\n\n    job_name: str\n    request_id: Optional[str] = None\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"utterances\": [\n                    {\n                        \"speaker\": 0,\n                        \"start\": 0.0,\n                        \"end\": 1.0,\n                        \"text\": \"Never gonna give you up!\",\n                    },\n                    {\n                        \"speaker\": 0,\n                        \"start\": 1.0,\n                        \"end\": 2.0,\n                        \"text\": \"Never gonna let you down!\",\n                    },\n                ],\n                \"audio_duration\": 2.0,\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n                \"process_times\": {\n                    \"total\": 2.678,\n                    \"transcription\": 2.439,\n                    \"diarization\": None,\n                    \"post_processing\": 0.239,\n                },\n                \"video_url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n                \"job_name\": \"job_name\",\n                \"request_id\": \"request_id\",\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.CortexYoutubeResponse.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"utterances\": [\n                {\n                    \"speaker\": 0,\n                    \"start\": 0.0,\n                    \"end\": 1.0,\n                    \"text\": \"Never gonna give you up!\",\n                },\n                {\n                    \"speaker\": 0,\n                    \"start\": 1.0,\n                    \"end\": 2.0,\n                    \"text\": \"Never gonna let you down!\",\n                },\n            ],\n            \"audio_duration\": 2.0,\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n            \"process_times\": {\n                \"total\": 2.678,\n                \"transcription\": 2.439,\n                \"diarization\": None,\n                \"post_processing\": 0.239,\n            },\n            \"video_url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n            \"job_name\": \"job_name\",\n            \"request_id\": \"request_id\",\n        }\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.DiarizationOutput","title":"<code>DiarizationOutput</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Diarization output model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class DiarizationOutput(BaseModel):\n    \"\"\"Diarization output model for the API.\"\"\"\n\n    segments: List[DiarizationSegment]\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.DiarizationRequest","title":"<code>DiarizationRequest</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request model for the diarize endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class DiarizationRequest(BaseModel):\n    \"\"\"Request model for the diarize endpoint.\"\"\"\n\n    audio: TensorShare\n    duration: float\n    num_speakers: int\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.DiarizationSegment","title":"<code>DiarizationSegment</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Diarization segment model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class DiarizationSegment(NamedTuple):\n    \"\"\"Diarization segment model for the API.\"\"\"\n\n    start: float\n    end: float\n    speaker: int\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.MultiChannelSegment","title":"<code>MultiChannelSegment</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Multi-channel segment model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class MultiChannelSegment(NamedTuple):\n    \"\"\"Multi-channel segment model for the API.\"\"\"\n\n    start: float\n    end: float\n    text: str\n    words: List[Word]\n    speaker: int\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.MultiChannelTranscriptionOutput","title":"<code>MultiChannelTranscriptionOutput</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Multi-channel transcription output model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class MultiChannelTranscriptionOutput(BaseModel):\n    \"\"\"Multi-channel transcription output model for the API.\"\"\"\n\n    segments: List[MultiChannelSegment]\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.PongResponse","title":"<code>PongResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response model for the ping endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class PongResponse(BaseModel):\n    \"\"\"Response model for the ping endpoint.\"\"\"\n\n    message: str\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"message\": \"pong\",\n            },\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.PongResponse.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"message\": \"pong\",\n        },\n    }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.ProcessTimes","title":"<code>ProcessTimes</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The execution times of the different processes.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class ProcessTimes(BaseModel):\n    \"\"\"The execution times of the different processes.\"\"\"\n\n    total: Union[float, None] = None\n    transcription: Union[float, None] = None\n    diarization: Union[float, None] = None\n    post_processing: Union[float, None] = None\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.Timestamps","title":"<code>Timestamps</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Timestamps enum for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Timestamps(str, Enum):\n    \"\"\"Timestamps enum for the API.\"\"\"\n\n    seconds = \"s\"\n    milliseconds = \"ms\"\n    hour_minute_second = \"hms\"\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.Token","title":"<code>Token</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Token model for authentication.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Token(BaseModel):\n    \"\"\"Token model for authentication.\"\"\"\n\n    access_token: str\n    token_type: str\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.TokenData","title":"<code>TokenData</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>TokenData model for authentication.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class TokenData(BaseModel):\n    \"\"\"TokenData model for authentication.\"\"\"\n\n    username: Optional[str] = None\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.TranscribeRequest","title":"<code>TranscribeRequest</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request model for the transcribe endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class TranscribeRequest(BaseModel):\n    \"\"\"Request model for the transcribe endpoint.\"\"\"\n\n    audio: Union[TensorShare, List[TensorShare]]\n    compression_ratio_threshold: float\n    condition_on_previous_text: bool\n    internal_vad: bool\n    log_prob_threshold: float\n    no_speech_threshold: float\n    repetition_penalty: float\n    source_lang: str\n    vocab: Union[List[str], None]\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.TranscriptionOutput","title":"<code>TranscriptionOutput</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Transcription output model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class TranscriptionOutput(BaseModel):\n    \"\"\"Transcription output model for the API.\"\"\"\n\n    segments: List[Segment]\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.UrlSchema","title":"<code>UrlSchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request model for the add_url endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class UrlSchema(BaseModel):\n    \"\"\"Request model for the add_url endpoint.\"\"\"\n\n    task: Literal[\"transcription\", \"diarization\"]\n    url: HttpUrl\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.Utterance","title":"<code>Utterance</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Utterance model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Utterance(BaseModel):\n    \"\"\"Utterance model for the API.\"\"\"\n\n    text: str\n    start: Union[float, str]\n    end: Union[float, str]\n    speaker: Union[int, None] = None\n    words: Union[List[Word], None] = None\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.Word","title":"<code>Word</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Word model for the API.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Word(BaseModel):\n    \"\"\"Word model for the API.\"\"\"\n\n    word: str\n    start: float\n    end: float\n    probability: float\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.YouTubeResponse","title":"<code>YouTubeResponse</code>","text":"<p>             Bases: <code>BaseResponse</code></p> <p>Response model for the ASR YouTube endpoint.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class YouTubeResponse(BaseResponse):\n    \"\"\"Response model for the ASR YouTube endpoint.\"\"\"\n\n    video_url: str\n\n    class Config:\n        \"\"\"Pydantic config class.\"\"\"\n\n        json_schema_extra = {\n            \"example\": {\n                \"utterances\": [\n                    {\n                        \"speaker\": 0,\n                        \"start\": 0.0,\n                        \"end\": 1.0,\n                        \"text\": \"Never gonna give you up!\",\n                    },\n                    {\n                        \"speaker\": 0,\n                        \"start\": 1.0,\n                        \"end\": 2.0,\n                        \"text\": \"Never gonna let you down!\",\n                    },\n                ],\n                \"audio_duration\": 2.0,\n                \"offset_start\": None,\n                \"offset_end\": None,\n                \"num_speakers\": -1,\n                \"diarization\": False,\n                \"source_lang\": \"en\",\n                \"timestamps\": \"s\",\n                \"vocab\": [\n                    \"custom company name\",\n                    \"custom product name\",\n                    \"custom co-worker name\",\n                ],\n                \"word_timestamps\": False,\n                \"internal_vad\": False,\n                \"repetition_penalty\": 1.2,\n                \"compression_ratio_threshold\": 2.4,\n                \"log_prob_threshold\": -1.0,\n                \"no_speech_threshold\": 0.6,\n                \"condition_on_previous_text\": True,\n                \"process_times\": {\n                    \"total\": 2.678,\n                    \"transcription\": 2.439,\n                    \"diarization\": None,\n                    \"post_processing\": 0.239,\n                },\n                \"video_url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n            }\n        }\n</code></pre>"},{"location":"reference/schemas/#src.wordcab_transcribe.models.YouTubeResponse.Config","title":"<code>Config</code>","text":"<p>Pydantic config class.</p> Source code in <code>src/wordcab_transcribe/models.py</code> <pre><code>class Config:\n    \"\"\"Pydantic config class.\"\"\"\n\n    json_schema_extra = {\n        \"example\": {\n            \"utterances\": [\n                {\n                    \"speaker\": 0,\n                    \"start\": 0.0,\n                    \"end\": 1.0,\n                    \"text\": \"Never gonna give you up!\",\n                },\n                {\n                    \"speaker\": 0,\n                    \"start\": 1.0,\n                    \"end\": 2.0,\n                    \"text\": \"Never gonna let you down!\",\n                },\n            ],\n            \"audio_duration\": 2.0,\n            \"offset_start\": None,\n            \"offset_end\": None,\n            \"num_speakers\": -1,\n            \"diarization\": False,\n            \"source_lang\": \"en\",\n            \"timestamps\": \"s\",\n            \"vocab\": [\n                \"custom company name\",\n                \"custom product name\",\n                \"custom co-worker name\",\n            ],\n            \"word_timestamps\": False,\n            \"internal_vad\": False,\n            \"repetition_penalty\": 1.2,\n            \"compression_ratio_threshold\": 2.4,\n            \"log_prob_threshold\": -1.0,\n            \"no_speech_threshold\": 0.6,\n            \"condition_on_previous_text\": True,\n            \"process_times\": {\n                \"total\": 2.678,\n                \"transcription\": 2.439,\n                \"diarization\": None,\n                \"post_processing\": 0.239,\n            },\n            \"video_url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n        }\n    }\n</code></pre>"},{"location":"reference/services/","title":"Services","text":"<p>ASR Service module that handle all AI interactions.</p> <p>GPU service class to handle gpu availability for models.</p> <p>Diarization Service for audio files.</p> <p>Post-Processing Service for audio files.</p> <p>Transcribe Service for audio files.</p> <p>Voice Activation Detection (VAD) Service for audio files.</p>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService","title":"<code>ASRAsyncService</code>","text":"<p>             Bases: <code>ASRService</code></p> <p>ASR Service module for async endpoints.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ASRAsyncService(ASRService):\n    \"\"\"ASR Service module for async endpoints.\"\"\"\n\n    def __init__(\n        self,\n        whisper_model: str,\n        compute_type: str,\n        window_lengths: List[float],\n        shift_lengths: List[float],\n        multiscale_weights: List[float],\n        extra_languages: Union[List[str], None],\n        extra_languages_model_paths: Union[List[str], None],\n        transcribe_server_urls: Union[List[str], None],\n        diarize_server_urls: Union[List[str], None],\n        debug_mode: bool,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ASRAsyncService class.\n\n        Args:\n            whisper_model (str):\n                The path to the whisper model.\n            compute_type (str):\n                The compute type to use for inference.\n            window_lengths (List[float]):\n                The window lengths to use for diarization.\n            shift_lengths (List[float]):\n                The shift lengths to use for diarization.\n            multiscale_weights (List[float]):\n                The multiscale weights to use for diarization.\n            extra_languages (Union[List[str], None]):\n                The list of extra languages to support.\n            extra_languages_model_paths (Union[List[str], None]):\n                The list of paths to the extra language models.\n            use_remote_servers (bool):\n                Whether to use remote servers for transcription and diarization.\n            transcribe_server_urls (Union[List[str], None]):\n                The list of URLs to the remote transcription servers.\n            diarize_server_urls (Union[List[str], None]):\n                The list of URLs to the remote diarization servers.\n            debug_mode (bool):\n                Whether to run in debug mode.\n        \"\"\"\n        super().__init__()\n\n        self.whisper_model: str = whisper_model\n        self.compute_type: str = compute_type\n        self.window_lengths: List[float] = window_lengths\n        self.shift_lengths: List[float] = shift_lengths\n        self.multiscale_weights: List[float] = multiscale_weights\n        self.extra_languages: Union[List[str], None] = extra_languages\n        self.extra_languages_model_paths: Union[List[str], None] = (\n            extra_languages_model_paths\n        )\n\n        self.local_services: LocalServiceRegistry = LocalServiceRegistry()\n        self.remote_services: RemoteServiceRegistry = RemoteServiceRegistry()\n        self.dual_channel_transcribe_options: dict = {\n            \"beam_size\": 5,\n            \"patience\": 1,\n            \"length_penalty\": 1,\n            \"suppress_blank\": False,\n            \"word_timestamps\": True,\n            \"temperature\": 0.0,\n        }\n\n        if transcribe_server_urls is not None:\n            logger.info(\n                \"You provided URLs for remote transcription server, no local model will\"\n                \" be used.\"\n            )\n            self.remote_services.transcription = RemoteServiceConfig(\n                use_remote=True,\n                url_handler=URLService(remote_urls=transcribe_server_urls),\n            )\n        else:\n            logger.info(\n                \"You did not provide URLs for remote transcription server, local model\"\n                \" will be used.\"\n            )\n            self.create_transcription_local_service()\n\n        if diarize_server_urls is not None:\n            logger.info(\n                \"You provided URLs for remote diarization server, no local model will\"\n                \" be used.\"\n            )\n            self.remote_services.diarization = RemoteServiceConfig(\n                use_remote=True,\n                url_handler=URLService(remote_urls=diarize_server_urls),\n            )\n        else:\n            logger.info(\n                \"You did not provide URLs for remote diarization server, local model\"\n                \" will be used.\"\n            )\n            self.create_diarization_local_service()\n\n        self.debug_mode = debug_mode\n\n    def create_transcription_local_service(self) -&gt; None:\n        \"\"\"Create a local transcription service.\"\"\"\n        self.local_services.transcription = TranscribeService(\n            model_path=self.whisper_model,\n            compute_type=self.compute_type,\n            device=self.device,\n            device_index=self.device_index,\n            extra_languages=self.extra_languages,\n            extra_languages_model_paths=self.extra_languages_model_paths,\n        )\n\n    def create_diarization_local_service(self) -&gt; None:\n        \"\"\"Create a local diarization service.\"\"\"\n        self.local_services.diarization = DiarizeService(\n            device=self.device,\n            device_index=self.device_index,\n            window_lengths=self.window_lengths,\n            shift_lengths=self.shift_lengths,\n            multiscale_weights=self.multiscale_weights,\n        )\n\n    def create_local_service(\n        self, task: Literal[\"transcription\", \"diarization\"]\n    ) -&gt; None:\n        \"\"\"Create a local service.\"\"\"\n        if task == \"transcription\":\n            self.create_transcription_local_service()\n        elif task == \"diarization\":\n            self.create_diarization_local_service()\n        else:\n            raise NotImplementedError(\"No task specified.\")\n\n    async def inference_warmup(self) -&gt; None:\n        \"\"\"Warmup the GPU by loading the models.\"\"\"\n        sample_path = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n\n        for gpu_index in self.gpu_handler.device_index:\n            logger.info(f\"Warmup GPU {gpu_index}.\")\n            await self.process_input(\n                filepath=str(sample_path),\n                offset_start=None,\n                offset_end=None,\n                num_speakers=1,\n                diarization=True,\n                multi_channel=False,\n                source_lang=\"en\",\n                timestamps_format=\"s\",\n                vocab=None,\n                word_timestamps=False,\n                internal_vad=False,\n                repetition_penalty=1.0,\n                compression_ratio_threshold=2.4,\n                log_prob_threshold=-1.0,\n                no_speech_threshold=0.6,\n                condition_on_previous_text=True,\n            )\n\n    async def process_input(  # noqa: C901\n        self,\n        filepath: Union[str, List[str]],\n        offset_start: Union[float, None],\n        offset_end: Union[float, None],\n        num_speakers: int,\n        diarization: bool,\n        multi_channel: bool,\n        source_lang: str,\n        timestamps_format: str,\n        vocab: Union[List[str], None],\n        word_timestamps: bool,\n        internal_vad: bool,\n        repetition_penalty: float,\n        compression_ratio_threshold: float,\n        log_prob_threshold: float,\n        no_speech_threshold: float,\n        condition_on_previous_text: bool,\n    ) -&gt; Union[Tuple[List[dict], ProcessTimes, float], Exception]:\n        \"\"\"Process the input request and return the results.\n\n        This method will create a task and add it to the appropriate queues.\n        All tasks are added to the transcription queue, but will be added to the\n        diarization queues only if the user requested it.\n        Each step will be processed asynchronously and the results will be returned\n        and stored in separated keys in the task dictionary.\n\n        Args:\n            filepath (Union[str, List[str]]):\n                Path to the audio file or list of paths to the audio files to process.\n            offset_start (Union[float, None]):\n                The start time of the audio file to process.\n            offset_end (Union[float, None]):\n                The end time of the audio file to process.\n            num_speakers (int):\n                The number of oracle speakers.\n            diarization (bool):\n                Whether to do diarization or not.\n            multi_channel (bool):\n                Whether to do multi-channel diarization or not.\n            source_lang (str):\n                Source language of the audio file.\n            timestamps_format (str):\n                Timestamps format to use.\n            vocab (Union[List[str], None]):\n                List of words to use for the vocabulary.\n            word_timestamps (bool):\n                Whether to return word timestamps or not.\n            internal_vad (bool):\n                Whether to use faster-whisper's VAD or not.\n            repetition_penalty (float):\n                The repetition penalty to use for the beam search.\n            compression_ratio_threshold (float):\n                If the gzip compression ratio is above this value, treat as failed.\n            log_prob_threshold (float):\n                If the average log probability over sampled tokens is below this value, treat as failed.\n            no_speech_threshold (float):\n                If the no_speech probability is higher than this value AND the average log probability\n                over sampled tokens is below `log_prob_threshold`, consider the segment as silent.\n            condition_on_previous_text (bool):\n                If True, the previous output of the model is provided as a prompt for the next window;\n                disabling may make the text inconsistent across windows, but the model becomes less prone\n                to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n\n        Returns:\n            Union[Tuple[List[dict], ProcessTimes, float], Exception]:\n                The results of the ASR pipeline or an exception if something went wrong.\n                Results are returned as a tuple of the following:\n                    * List[dict]: The final results of the ASR pipeline.\n                    * ProcessTimes: The process times of each step of the ASR pipeline.\n                    * float: The audio duration\n        \"\"\"\n        if isinstance(filepath, list):\n            audio, durations = [], []\n            for path in filepath:\n                _audio, _duration = read_audio(\n                    path, offset_start=offset_start, offset_end=offset_end\n                )\n\n                audio.append(_audio)\n                durations.append(_duration)\n\n            duration = sum(durations) / len(durations)\n\n        else:\n            audio, duration = read_audio(\n                filepath, offset_start=offset_start, offset_end=offset_end\n            )\n\n        gpu_index = None\n        if self.remote_services.transcription.use_remote is True:\n            _url = await self.remote_services.transcription.next_url()\n            transcription_execution = RemoteExecution(url=_url)\n        else:\n            gpu_index = await self.gpu_handler.get_device()\n            transcription_execution = LocalExecution(index=gpu_index)\n\n        if diarization and multi_channel is False:\n            if self.remote_services.diarization.use_remote is True:\n                _url = await self.remote_services.diarization.next_url()\n                diarization_execution = RemoteExecution(url=_url)\n            else:\n                if gpu_index is None:\n                    gpu_index = await self.gpu_handler.get_device()\n\n                diarization_execution = LocalExecution(index=gpu_index)\n        else:\n            diarization_execution = None\n\n        task = ASRTask(\n            audio=audio,\n            diarization=DiarizationTask(\n                execution=diarization_execution, num_speakers=num_speakers\n            ),\n            duration=duration,\n            multi_channel=multi_channel,\n            offset_start=offset_start,\n            post_processing=PostProcessingTask(),\n            process_times=ProcessTimes(),\n            timestamps_format=timestamps_format,\n            transcription=TranscriptionTask(\n                execution=transcription_execution,\n                options=TranscriptionOptions(\n                    compression_ratio_threshold=compression_ratio_threshold,\n                    condition_on_previous_text=condition_on_previous_text,\n                    internal_vad=internal_vad,\n                    log_prob_threshold=log_prob_threshold,\n                    no_speech_threshold=no_speech_threshold,\n                    repetition_penalty=repetition_penalty,\n                    source_lang=source_lang,\n                    vocab=vocab,\n                ),\n            ),\n            word_timestamps=word_timestamps,\n        )\n\n        try:\n            start_process_time = time.time()\n\n            transcription_task = self.process_transcription(task, self.debug_mode)\n            diarization_task = self.process_diarization(task, self.debug_mode)\n\n            await asyncio.gather(transcription_task, diarization_task)\n\n            if isinstance(task.diarization.result, ProcessException):\n                return task.diarization.result\n\n            if (\n                diarization\n                and task.diarization.result is None\n                and multi_channel is False\n            ):\n                # Empty audio early return\n                return early_return(duration=duration)\n\n            if isinstance(task.transcription.result, ProcessException):\n                return task.transcription.result\n\n            await asyncio.get_event_loop().run_in_executor(\n                None,\n                self.process_post_processing,\n                task,\n            )\n\n            if isinstance(task.post_processing.result, ProcessException):\n                return task.post_processing.result\n\n            task.process_times.total = time.time() - start_process_time\n\n            return task.post_processing.result, task.process_times, duration\n\n        except Exception as e:\n            return e\n\n        finally:\n            del task\n\n            if gpu_index is not None:\n                self.gpu_handler.release_device(gpu_index)\n\n    async def process_transcription(self, task: ASRTask, debug_mode: bool) -&gt; None:\n        \"\"\"\n        Process a task of transcription and update the task with the result.\n\n        Args:\n            task (ASRTask): The task and its parameters.\n            debug_mode (bool): Whether to run in debug mode or not.\n\n        Returns:\n            None: The task is updated with the result.\n        \"\"\"\n        try:\n            if isinstance(task.transcription.execution, LocalExecution):\n                out = await time_and_tell_async(\n                    lambda: self.local_services.transcription(\n                        task.audio,\n                        model_index=task.transcription.execution.index,\n                        suppress_blank=False,\n                        word_timestamps=True,\n                        **task.transcription.options.model_dump(),\n                    ),\n                    func_name=\"transcription\",\n                    debug_mode=debug_mode,\n                )\n                result, process_time = out\n\n            elif isinstance(task.transcription.execution, RemoteExecution):\n                if isinstance(task.audio, list):\n                    ts = [\n                        TensorShare.from_dict({\"audio\": a}, backend=Backend.TORCH)\n                        for a in task.audio\n                    ]\n                else:\n                    ts = TensorShare.from_dict(\n                        {\"audio\": task.audio}, backend=Backend.TORCH\n                    )\n\n                data = TranscribeRequest(\n                    audio=ts,\n                    **task.transcription.options.model_dump(),\n                )\n                out = await time_and_tell_async(\n                    self.remote_transcription(\n                        url=task.transcription.execution.url,\n                        data=data,\n                    ),\n                    func_name=\"transcription\",\n                    debug_mode=debug_mode,\n                )\n                result, process_time = out\n\n            else:\n                raise NotImplementedError(\"No execution method specified.\")\n\n        except Exception as e:\n            result = ProcessException(\n                source=ExceptionSource.transcription,\n                message=f\"Error in transcription: {e}\\n{traceback.format_exc()}\",\n            )\n            process_time = None\n\n        finally:\n            task.process_times.transcription = process_time\n            task.transcription.result = result\n\n        return None\n\n    async def process_diarization(self, task: ASRTask, debug_mode: bool) -&gt; None:\n        \"\"\"\n        Process a task of diarization.\n\n        Args:\n            task (ASRTask): The task and its parameters.\n            debug_mode (bool): Whether to run in debug mode or not.\n\n        Returns:\n            None: The task is updated with the result.\n        \"\"\"\n        try:\n            if isinstance(task.diarization.execution, LocalExecution):\n                out = await time_and_tell_async(\n                    lambda: self.local_services.diarization(\n                        waveform=task.audio,\n                        audio_duration=task.duration,\n                        oracle_num_speakers=task.diarization.num_speakers,\n                        model_index=task.diarization.execution.index,\n                        vad_service=self.local_services.vad,\n                    ),\n                    func_name=\"diarization\",\n                    debug_mode=debug_mode,\n                )\n                result, process_time = out\n\n            elif isinstance(task.diarization.execution, RemoteExecution):\n                ts = TensorShare.from_dict({\"audio\": task.audio}, backend=Backend.TORCH)\n\n                data = DiarizationRequest(\n                    audio=ts,\n                    duration=task.duration,\n                    num_speakers=task.diarization.num_speakers,\n                )\n                out = await time_and_tell_async(\n                    self.remote_diarization(\n                        url=task.diarization.execution.url,\n                        data=data,\n                    ),\n                    func_name=\"diarization\",\n                    debug_mode=debug_mode,\n                )\n                result, process_time = out\n\n            elif task.diarization.execution is None:\n                result = None\n                process_time = None\n\n            else:\n                raise NotImplementedError(\"No execution method specified.\")\n\n        except Exception as e:\n            result = ProcessException(\n                source=ExceptionSource.diarization,\n                message=f\"Error in diarization: {e}\\n{traceback.format_exc()}\",\n            )\n            process_time = None\n\n        finally:\n            task.process_times.diarization = process_time\n            task.diarization.result = result\n\n        return None\n\n    def process_post_processing(self, task: ASRTask) -&gt; None:\n        \"\"\"\n        Process a task of post-processing.\n\n        Args:\n            task (ASRTask): The task and its parameters.\n\n        Returns:\n            None: The task is updated with the result.\n        \"\"\"\n        try:\n            total_post_process_time = 0\n\n            if task.multi_channel:\n                utterances, process_time = time_and_tell(\n                    self.local_services.post_processing.multi_channel_speaker_mapping(\n                        task.transcription.result\n                    ),\n                    func_name=\"multi_channel_speaker_mapping\",\n                    debug_mode=self.debug_mode,\n                )\n                total_post_process_time += process_time\n\n            else:\n                formatted_segments, process_time = time_and_tell(\n                    format_segments(\n                        transcription_output=task.transcription.result,\n                    ),\n                    func_name=\"format_segments\",\n                    debug_mode=self.debug_mode,\n                )\n                total_post_process_time += process_time\n\n                if task.diarization.execution is not None:\n                    utterances, process_time = time_and_tell(\n                        self.local_services.post_processing.single_channel_speaker_mapping(\n                            transcript_segments=formatted_segments,\n                            speaker_timestamps=task.diarization.result,\n                            word_timestamps=task.word_timestamps,\n                        ),\n                        func_name=\"single_channel_speaker_mapping\",\n                        debug_mode=self.debug_mode,\n                    )\n                    total_post_process_time += process_time\n                else:\n                    utterances = formatted_segments\n\n            final_utterances, process_time = time_and_tell(\n                self.local_services.post_processing.final_processing_before_returning(\n                    utterances=utterances,\n                    offset_start=task.offset_start,\n                    timestamps_format=task.timestamps_format,\n                    word_timestamps=task.word_timestamps,\n                ),\n                func_name=\"final_processing_before_returning\",\n                debug_mode=self.debug_mode,\n            )\n            total_post_process_time += process_time\n\n        except Exception as e:\n            final_utterances = ProcessException(\n                source=ExceptionSource.post_processing,\n                message=f\"Error in post-processing: {e}\\n{traceback.format_exc()}\",\n            )\n            total_post_process_time = None\n\n        finally:\n            task.process_times.post_processing = total_post_process_time\n            task.post_processing.result = final_utterances\n\n        return None\n\n    async def remote_transcription(\n        self,\n        url: str,\n        data: TranscribeRequest,\n    ) -&gt; TranscriptionOutput:\n        \"\"\"Remote transcription method.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url=f\"{url}/api/v1/transcribe\",\n                data=data.model_dump_json(),\n                headers={\"Content-Type\": \"application/json\"},\n            ) as response:\n                if response.status != 200:\n                    raise Exception(response.status)\n                else:\n                    return TranscriptionOutput(**await response.json())\n\n    async def remote_diarization(\n        self,\n        url: str,\n        data: DiarizationRequest,\n    ) -&gt; DiarizationOutput:\n        \"\"\"Remote diarization method.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url=f\"{url}/api/v1/diarize\",\n                data=data.model_dump_json(),\n                headers={\"Content-Type\": \"application/json\"},\n            ) as response:\n                if response.status != 200:\n                    r = await response.json()\n                    raise Exception(r[\"detail\"])\n                else:\n                    return DiarizationOutput(**await response.json())\n\n    async def get_url(\n        self, task: Literal[\"transcription\", \"diarization\"]\n    ) -&gt; Union[List[str], ProcessException]:\n        \"\"\"Get the list of remote URLs.\"\"\"\n        logger.info(self.remote_services.transcription)\n        logger.info(self.remote_services.diarization)\n        try:\n            selected_task = getattr(self.remote_services, task)\n            logger.info(selected_task)\n            # Case 1: We are not using remote task\n            if selected_task.use_remote is False:\n                return ProcessException(\n                    source=ExceptionSource.get_url,\n                    message=f\"You are not using remote {task}.\",\n                )\n            # Case 2: We are using remote task\n            else:\n                return selected_task.get_urls()\n\n        except Exception as e:\n            return ProcessException(\n                source=ExceptionSource.get_url,\n                message=f\"Error in getting URL: {e}\\n{traceback.format_exc()}\",\n            )\n\n    async def add_url(self, data: UrlSchema) -&gt; Union[UrlSchema, ProcessException]:\n        \"\"\"Add a remote URL to the list of URLs.\"\"\"\n        try:\n            selected_task = getattr(self.remote_services, data.task)\n            # Case 1: We are not using remote task yet\n            if selected_task.use_remote is False:\n                setattr(\n                    self.remote_services,\n                    data.task,\n                    RemoteServiceConfig(\n                        use_remote=True,\n                        url_handler=URLService(remote_urls=[str(data.url)]),\n                    ),\n                )\n                setattr(self.local_services, data.task, None)\n            # Case 2: We are already using remote task\n            else:\n                await selected_task.add_url(str(data.url))\n\n        except Exception as e:\n            return ProcessException(\n                source=ExceptionSource.add_url,\n                message=f\"Error in adding URL: {e}\\n{traceback.format_exc()}\",\n            )\n\n        return data\n\n    async def remove_url(self, data: UrlSchema) -&gt; Union[UrlSchema, ProcessException]:\n        \"\"\"Remove a remote URL from the list of URLs.\"\"\"\n        try:\n            selected_task = getattr(self.remote_services, data.task)\n            # Case 1: We are not using remote task\n            if selected_task.use_remote is False:\n                raise ValueError(f\"You are not using remote {data.task}.\")\n            # Case 2: We are using remote task\n            else:\n                await selected_task.remove_url(str(data.url))\n                if selected_task.get_queue_size() == 0:\n                    # No more remote URLs, switch to local service\n                    self.create_local_service(task=data.task)\n                    setattr(self.remote_services, data.task, RemoteServiceConfig())\n\n            return data\n\n        except Exception as e:\n            return ProcessException(\n                source=ExceptionSource.remove_url,\n                message=f\"Error in removing URL: {e}\\n{traceback.format_exc()}\",\n            )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.__init__","title":"<code>__init__(whisper_model, compute_type, window_lengths, shift_lengths, multiscale_weights, extra_languages, extra_languages_model_paths, transcribe_server_urls, diarize_server_urls, debug_mode)</code>","text":"<p>Initialize the ASRAsyncService class.</p> <p>Parameters:</p> Name Type Description Default <code>whisper_model</code> <code>str</code> <p>The path to the whisper model.</p> required <code>compute_type</code> <code>str</code> <p>The compute type to use for inference.</p> required <code>window_lengths</code> <code>List[float]</code> <p>The window lengths to use for diarization.</p> required <code>shift_lengths</code> <code>List[float]</code> <p>The shift lengths to use for diarization.</p> required <code>multiscale_weights</code> <code>List[float]</code> <p>The multiscale weights to use for diarization.</p> required <code>extra_languages</code> <code>Union[List[str], None]</code> <p>The list of extra languages to support.</p> required <code>extra_languages_model_paths</code> <code>Union[List[str], None]</code> <p>The list of paths to the extra language models.</p> required <code>use_remote_servers</code> <code>bool</code> <p>Whether to use remote servers for transcription and diarization.</p> required <code>transcribe_server_urls</code> <code>Union[List[str], None]</code> <p>The list of URLs to the remote transcription servers.</p> required <code>diarize_server_urls</code> <code>Union[List[str], None]</code> <p>The list of URLs to the remote diarization servers.</p> required <code>debug_mode</code> <code>bool</code> <p>Whether to run in debug mode.</p> required Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def __init__(\n    self,\n    whisper_model: str,\n    compute_type: str,\n    window_lengths: List[float],\n    shift_lengths: List[float],\n    multiscale_weights: List[float],\n    extra_languages: Union[List[str], None],\n    extra_languages_model_paths: Union[List[str], None],\n    transcribe_server_urls: Union[List[str], None],\n    diarize_server_urls: Union[List[str], None],\n    debug_mode: bool,\n) -&gt; None:\n    \"\"\"\n    Initialize the ASRAsyncService class.\n\n    Args:\n        whisper_model (str):\n            The path to the whisper model.\n        compute_type (str):\n            The compute type to use for inference.\n        window_lengths (List[float]):\n            The window lengths to use for diarization.\n        shift_lengths (List[float]):\n            The shift lengths to use for diarization.\n        multiscale_weights (List[float]):\n            The multiscale weights to use for diarization.\n        extra_languages (Union[List[str], None]):\n            The list of extra languages to support.\n        extra_languages_model_paths (Union[List[str], None]):\n            The list of paths to the extra language models.\n        use_remote_servers (bool):\n            Whether to use remote servers for transcription and diarization.\n        transcribe_server_urls (Union[List[str], None]):\n            The list of URLs to the remote transcription servers.\n        diarize_server_urls (Union[List[str], None]):\n            The list of URLs to the remote diarization servers.\n        debug_mode (bool):\n            Whether to run in debug mode.\n    \"\"\"\n    super().__init__()\n\n    self.whisper_model: str = whisper_model\n    self.compute_type: str = compute_type\n    self.window_lengths: List[float] = window_lengths\n    self.shift_lengths: List[float] = shift_lengths\n    self.multiscale_weights: List[float] = multiscale_weights\n    self.extra_languages: Union[List[str], None] = extra_languages\n    self.extra_languages_model_paths: Union[List[str], None] = (\n        extra_languages_model_paths\n    )\n\n    self.local_services: LocalServiceRegistry = LocalServiceRegistry()\n    self.remote_services: RemoteServiceRegistry = RemoteServiceRegistry()\n    self.dual_channel_transcribe_options: dict = {\n        \"beam_size\": 5,\n        \"patience\": 1,\n        \"length_penalty\": 1,\n        \"suppress_blank\": False,\n        \"word_timestamps\": True,\n        \"temperature\": 0.0,\n    }\n\n    if transcribe_server_urls is not None:\n        logger.info(\n            \"You provided URLs for remote transcription server, no local model will\"\n            \" be used.\"\n        )\n        self.remote_services.transcription = RemoteServiceConfig(\n            use_remote=True,\n            url_handler=URLService(remote_urls=transcribe_server_urls),\n        )\n    else:\n        logger.info(\n            \"You did not provide URLs for remote transcription server, local model\"\n            \" will be used.\"\n        )\n        self.create_transcription_local_service()\n\n    if diarize_server_urls is not None:\n        logger.info(\n            \"You provided URLs for remote diarization server, no local model will\"\n            \" be used.\"\n        )\n        self.remote_services.diarization = RemoteServiceConfig(\n            use_remote=True,\n            url_handler=URLService(remote_urls=diarize_server_urls),\n        )\n    else:\n        logger.info(\n            \"You did not provide URLs for remote diarization server, local model\"\n            \" will be used.\"\n        )\n        self.create_diarization_local_service()\n\n    self.debug_mode = debug_mode\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.add_url","title":"<code>add_url(data)</code>  <code>async</code>","text":"<p>Add a remote URL to the list of URLs.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def add_url(self, data: UrlSchema) -&gt; Union[UrlSchema, ProcessException]:\n    \"\"\"Add a remote URL to the list of URLs.\"\"\"\n    try:\n        selected_task = getattr(self.remote_services, data.task)\n        # Case 1: We are not using remote task yet\n        if selected_task.use_remote is False:\n            setattr(\n                self.remote_services,\n                data.task,\n                RemoteServiceConfig(\n                    use_remote=True,\n                    url_handler=URLService(remote_urls=[str(data.url)]),\n                ),\n            )\n            setattr(self.local_services, data.task, None)\n        # Case 2: We are already using remote task\n        else:\n            await selected_task.add_url(str(data.url))\n\n    except Exception as e:\n        return ProcessException(\n            source=ExceptionSource.add_url,\n            message=f\"Error in adding URL: {e}\\n{traceback.format_exc()}\",\n        )\n\n    return data\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.create_diarization_local_service","title":"<code>create_diarization_local_service()</code>","text":"<p>Create a local diarization service.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def create_diarization_local_service(self) -&gt; None:\n    \"\"\"Create a local diarization service.\"\"\"\n    self.local_services.diarization = DiarizeService(\n        device=self.device,\n        device_index=self.device_index,\n        window_lengths=self.window_lengths,\n        shift_lengths=self.shift_lengths,\n        multiscale_weights=self.multiscale_weights,\n    )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.create_local_service","title":"<code>create_local_service(task)</code>","text":"<p>Create a local service.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def create_local_service(\n    self, task: Literal[\"transcription\", \"diarization\"]\n) -&gt; None:\n    \"\"\"Create a local service.\"\"\"\n    if task == \"transcription\":\n        self.create_transcription_local_service()\n    elif task == \"diarization\":\n        self.create_diarization_local_service()\n    else:\n        raise NotImplementedError(\"No task specified.\")\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.create_transcription_local_service","title":"<code>create_transcription_local_service()</code>","text":"<p>Create a local transcription service.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def create_transcription_local_service(self) -&gt; None:\n    \"\"\"Create a local transcription service.\"\"\"\n    self.local_services.transcription = TranscribeService(\n        model_path=self.whisper_model,\n        compute_type=self.compute_type,\n        device=self.device,\n        device_index=self.device_index,\n        extra_languages=self.extra_languages,\n        extra_languages_model_paths=self.extra_languages_model_paths,\n    )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.get_url","title":"<code>get_url(task)</code>  <code>async</code>","text":"<p>Get the list of remote URLs.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def get_url(\n    self, task: Literal[\"transcription\", \"diarization\"]\n) -&gt; Union[List[str], ProcessException]:\n    \"\"\"Get the list of remote URLs.\"\"\"\n    logger.info(self.remote_services.transcription)\n    logger.info(self.remote_services.diarization)\n    try:\n        selected_task = getattr(self.remote_services, task)\n        logger.info(selected_task)\n        # Case 1: We are not using remote task\n        if selected_task.use_remote is False:\n            return ProcessException(\n                source=ExceptionSource.get_url,\n                message=f\"You are not using remote {task}.\",\n            )\n        # Case 2: We are using remote task\n        else:\n            return selected_task.get_urls()\n\n    except Exception as e:\n        return ProcessException(\n            source=ExceptionSource.get_url,\n            message=f\"Error in getting URL: {e}\\n{traceback.format_exc()}\",\n        )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.inference_warmup","title":"<code>inference_warmup()</code>  <code>async</code>","text":"<p>Warmup the GPU by loading the models.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def inference_warmup(self) -&gt; None:\n    \"\"\"Warmup the GPU by loading the models.\"\"\"\n    sample_path = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n\n    for gpu_index in self.gpu_handler.device_index:\n        logger.info(f\"Warmup GPU {gpu_index}.\")\n        await self.process_input(\n            filepath=str(sample_path),\n            offset_start=None,\n            offset_end=None,\n            num_speakers=1,\n            diarization=True,\n            multi_channel=False,\n            source_lang=\"en\",\n            timestamps_format=\"s\",\n            vocab=None,\n            word_timestamps=False,\n            internal_vad=False,\n            repetition_penalty=1.0,\n            compression_ratio_threshold=2.4,\n            log_prob_threshold=-1.0,\n            no_speech_threshold=0.6,\n            condition_on_previous_text=True,\n        )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.process_diarization","title":"<code>process_diarization(task, debug_mode)</code>  <code>async</code>","text":"<p>Process a task of diarization.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>ASRTask</code> <p>The task and its parameters.</p> required <code>debug_mode</code> <code>bool</code> <p>Whether to run in debug mode or not.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The task is updated with the result.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def process_diarization(self, task: ASRTask, debug_mode: bool) -&gt; None:\n    \"\"\"\n    Process a task of diarization.\n\n    Args:\n        task (ASRTask): The task and its parameters.\n        debug_mode (bool): Whether to run in debug mode or not.\n\n    Returns:\n        None: The task is updated with the result.\n    \"\"\"\n    try:\n        if isinstance(task.diarization.execution, LocalExecution):\n            out = await time_and_tell_async(\n                lambda: self.local_services.diarization(\n                    waveform=task.audio,\n                    audio_duration=task.duration,\n                    oracle_num_speakers=task.diarization.num_speakers,\n                    model_index=task.diarization.execution.index,\n                    vad_service=self.local_services.vad,\n                ),\n                func_name=\"diarization\",\n                debug_mode=debug_mode,\n            )\n            result, process_time = out\n\n        elif isinstance(task.diarization.execution, RemoteExecution):\n            ts = TensorShare.from_dict({\"audio\": task.audio}, backend=Backend.TORCH)\n\n            data = DiarizationRequest(\n                audio=ts,\n                duration=task.duration,\n                num_speakers=task.diarization.num_speakers,\n            )\n            out = await time_and_tell_async(\n                self.remote_diarization(\n                    url=task.diarization.execution.url,\n                    data=data,\n                ),\n                func_name=\"diarization\",\n                debug_mode=debug_mode,\n            )\n            result, process_time = out\n\n        elif task.diarization.execution is None:\n            result = None\n            process_time = None\n\n        else:\n            raise NotImplementedError(\"No execution method specified.\")\n\n    except Exception as e:\n        result = ProcessException(\n            source=ExceptionSource.diarization,\n            message=f\"Error in diarization: {e}\\n{traceback.format_exc()}\",\n        )\n        process_time = None\n\n    finally:\n        task.process_times.diarization = process_time\n        task.diarization.result = result\n\n    return None\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.process_input","title":"<code>process_input(filepath, offset_start, offset_end, num_speakers, diarization, multi_channel, source_lang, timestamps_format, vocab, word_timestamps, internal_vad, repetition_penalty, compression_ratio_threshold, log_prob_threshold, no_speech_threshold, condition_on_previous_text)</code>  <code>async</code>","text":"<p>Process the input request and return the results.</p> <p>This method will create a task and add it to the appropriate queues. All tasks are added to the transcription queue, but will be added to the diarization queues only if the user requested it. Each step will be processed asynchronously and the results will be returned and stored in separated keys in the task dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Union[str, List[str]]</code> <p>Path to the audio file or list of paths to the audio files to process.</p> required <code>offset_start</code> <code>Union[float, None]</code> <p>The start time of the audio file to process.</p> required <code>offset_end</code> <code>Union[float, None]</code> <p>The end time of the audio file to process.</p> required <code>num_speakers</code> <code>int</code> <p>The number of oracle speakers.</p> required <code>diarization</code> <code>bool</code> <p>Whether to do diarization or not.</p> required <code>multi_channel</code> <code>bool</code> <p>Whether to do multi-channel diarization or not.</p> required <code>source_lang</code> <code>str</code> <p>Source language of the audio file.</p> required <code>timestamps_format</code> <code>str</code> <p>Timestamps format to use.</p> required <code>vocab</code> <code>Union[List[str], None]</code> <p>List of words to use for the vocabulary.</p> required <code>word_timestamps</code> <code>bool</code> <p>Whether to return word timestamps or not.</p> required <code>internal_vad</code> <code>bool</code> <p>Whether to use faster-whisper's VAD or not.</p> required <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty to use for the beam search.</p> required <code>compression_ratio_threshold</code> <code>float</code> <p>If the gzip compression ratio is above this value, treat as failed.</p> required <code>log_prob_threshold</code> <code>float</code> <p>If the average log probability over sampled tokens is below this value, treat as failed.</p> required <code>no_speech_threshold</code> <code>float</code> <p>If the no_speech probability is higher than this value AND the average log probability over sampled tokens is below <code>log_prob_threshold</code>, consider the segment as silent.</p> required <code>condition_on_previous_text</code> <code>bool</code> <p>If True, the previous output of the model is provided as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.</p> required <p>Returns:</p> Type Description <code>Union[Tuple[List[dict], ProcessTimes, float], Exception]</code> <p>Union[Tuple[List[dict], ProcessTimes, float], Exception]: The results of the ASR pipeline or an exception if something went wrong. Results are returned as a tuple of the following:     * List[dict]: The final results of the ASR pipeline.     * ProcessTimes: The process times of each step of the ASR pipeline.     * float: The audio duration</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def process_input(  # noqa: C901\n    self,\n    filepath: Union[str, List[str]],\n    offset_start: Union[float, None],\n    offset_end: Union[float, None],\n    num_speakers: int,\n    diarization: bool,\n    multi_channel: bool,\n    source_lang: str,\n    timestamps_format: str,\n    vocab: Union[List[str], None],\n    word_timestamps: bool,\n    internal_vad: bool,\n    repetition_penalty: float,\n    compression_ratio_threshold: float,\n    log_prob_threshold: float,\n    no_speech_threshold: float,\n    condition_on_previous_text: bool,\n) -&gt; Union[Tuple[List[dict], ProcessTimes, float], Exception]:\n    \"\"\"Process the input request and return the results.\n\n    This method will create a task and add it to the appropriate queues.\n    All tasks are added to the transcription queue, but will be added to the\n    diarization queues only if the user requested it.\n    Each step will be processed asynchronously and the results will be returned\n    and stored in separated keys in the task dictionary.\n\n    Args:\n        filepath (Union[str, List[str]]):\n            Path to the audio file or list of paths to the audio files to process.\n        offset_start (Union[float, None]):\n            The start time of the audio file to process.\n        offset_end (Union[float, None]):\n            The end time of the audio file to process.\n        num_speakers (int):\n            The number of oracle speakers.\n        diarization (bool):\n            Whether to do diarization or not.\n        multi_channel (bool):\n            Whether to do multi-channel diarization or not.\n        source_lang (str):\n            Source language of the audio file.\n        timestamps_format (str):\n            Timestamps format to use.\n        vocab (Union[List[str], None]):\n            List of words to use for the vocabulary.\n        word_timestamps (bool):\n            Whether to return word timestamps or not.\n        internal_vad (bool):\n            Whether to use faster-whisper's VAD or not.\n        repetition_penalty (float):\n            The repetition penalty to use for the beam search.\n        compression_ratio_threshold (float):\n            If the gzip compression ratio is above this value, treat as failed.\n        log_prob_threshold (float):\n            If the average log probability over sampled tokens is below this value, treat as failed.\n        no_speech_threshold (float):\n            If the no_speech probability is higher than this value AND the average log probability\n            over sampled tokens is below `log_prob_threshold`, consider the segment as silent.\n        condition_on_previous_text (bool):\n            If True, the previous output of the model is provided as a prompt for the next window;\n            disabling may make the text inconsistent across windows, but the model becomes less prone\n            to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n\n    Returns:\n        Union[Tuple[List[dict], ProcessTimes, float], Exception]:\n            The results of the ASR pipeline or an exception if something went wrong.\n            Results are returned as a tuple of the following:\n                * List[dict]: The final results of the ASR pipeline.\n                * ProcessTimes: The process times of each step of the ASR pipeline.\n                * float: The audio duration\n    \"\"\"\n    if isinstance(filepath, list):\n        audio, durations = [], []\n        for path in filepath:\n            _audio, _duration = read_audio(\n                path, offset_start=offset_start, offset_end=offset_end\n            )\n\n            audio.append(_audio)\n            durations.append(_duration)\n\n        duration = sum(durations) / len(durations)\n\n    else:\n        audio, duration = read_audio(\n            filepath, offset_start=offset_start, offset_end=offset_end\n        )\n\n    gpu_index = None\n    if self.remote_services.transcription.use_remote is True:\n        _url = await self.remote_services.transcription.next_url()\n        transcription_execution = RemoteExecution(url=_url)\n    else:\n        gpu_index = await self.gpu_handler.get_device()\n        transcription_execution = LocalExecution(index=gpu_index)\n\n    if diarization and multi_channel is False:\n        if self.remote_services.diarization.use_remote is True:\n            _url = await self.remote_services.diarization.next_url()\n            diarization_execution = RemoteExecution(url=_url)\n        else:\n            if gpu_index is None:\n                gpu_index = await self.gpu_handler.get_device()\n\n            diarization_execution = LocalExecution(index=gpu_index)\n    else:\n        diarization_execution = None\n\n    task = ASRTask(\n        audio=audio,\n        diarization=DiarizationTask(\n            execution=diarization_execution, num_speakers=num_speakers\n        ),\n        duration=duration,\n        multi_channel=multi_channel,\n        offset_start=offset_start,\n        post_processing=PostProcessingTask(),\n        process_times=ProcessTimes(),\n        timestamps_format=timestamps_format,\n        transcription=TranscriptionTask(\n            execution=transcription_execution,\n            options=TranscriptionOptions(\n                compression_ratio_threshold=compression_ratio_threshold,\n                condition_on_previous_text=condition_on_previous_text,\n                internal_vad=internal_vad,\n                log_prob_threshold=log_prob_threshold,\n                no_speech_threshold=no_speech_threshold,\n                repetition_penalty=repetition_penalty,\n                source_lang=source_lang,\n                vocab=vocab,\n            ),\n        ),\n        word_timestamps=word_timestamps,\n    )\n\n    try:\n        start_process_time = time.time()\n\n        transcription_task = self.process_transcription(task, self.debug_mode)\n        diarization_task = self.process_diarization(task, self.debug_mode)\n\n        await asyncio.gather(transcription_task, diarization_task)\n\n        if isinstance(task.diarization.result, ProcessException):\n            return task.diarization.result\n\n        if (\n            diarization\n            and task.diarization.result is None\n            and multi_channel is False\n        ):\n            # Empty audio early return\n            return early_return(duration=duration)\n\n        if isinstance(task.transcription.result, ProcessException):\n            return task.transcription.result\n\n        await asyncio.get_event_loop().run_in_executor(\n            None,\n            self.process_post_processing,\n            task,\n        )\n\n        if isinstance(task.post_processing.result, ProcessException):\n            return task.post_processing.result\n\n        task.process_times.total = time.time() - start_process_time\n\n        return task.post_processing.result, task.process_times, duration\n\n    except Exception as e:\n        return e\n\n    finally:\n        del task\n\n        if gpu_index is not None:\n            self.gpu_handler.release_device(gpu_index)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.process_post_processing","title":"<code>process_post_processing(task)</code>","text":"<p>Process a task of post-processing.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>ASRTask</code> <p>The task and its parameters.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The task is updated with the result.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def process_post_processing(self, task: ASRTask) -&gt; None:\n    \"\"\"\n    Process a task of post-processing.\n\n    Args:\n        task (ASRTask): The task and its parameters.\n\n    Returns:\n        None: The task is updated with the result.\n    \"\"\"\n    try:\n        total_post_process_time = 0\n\n        if task.multi_channel:\n            utterances, process_time = time_and_tell(\n                self.local_services.post_processing.multi_channel_speaker_mapping(\n                    task.transcription.result\n                ),\n                func_name=\"multi_channel_speaker_mapping\",\n                debug_mode=self.debug_mode,\n            )\n            total_post_process_time += process_time\n\n        else:\n            formatted_segments, process_time = time_and_tell(\n                format_segments(\n                    transcription_output=task.transcription.result,\n                ),\n                func_name=\"format_segments\",\n                debug_mode=self.debug_mode,\n            )\n            total_post_process_time += process_time\n\n            if task.diarization.execution is not None:\n                utterances, process_time = time_and_tell(\n                    self.local_services.post_processing.single_channel_speaker_mapping(\n                        transcript_segments=formatted_segments,\n                        speaker_timestamps=task.diarization.result,\n                        word_timestamps=task.word_timestamps,\n                    ),\n                    func_name=\"single_channel_speaker_mapping\",\n                    debug_mode=self.debug_mode,\n                )\n                total_post_process_time += process_time\n            else:\n                utterances = formatted_segments\n\n        final_utterances, process_time = time_and_tell(\n            self.local_services.post_processing.final_processing_before_returning(\n                utterances=utterances,\n                offset_start=task.offset_start,\n                timestamps_format=task.timestamps_format,\n                word_timestamps=task.word_timestamps,\n            ),\n            func_name=\"final_processing_before_returning\",\n            debug_mode=self.debug_mode,\n        )\n        total_post_process_time += process_time\n\n    except Exception as e:\n        final_utterances = ProcessException(\n            source=ExceptionSource.post_processing,\n            message=f\"Error in post-processing: {e}\\n{traceback.format_exc()}\",\n        )\n        total_post_process_time = None\n\n    finally:\n        task.process_times.post_processing = total_post_process_time\n        task.post_processing.result = final_utterances\n\n    return None\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.process_transcription","title":"<code>process_transcription(task, debug_mode)</code>  <code>async</code>","text":"<p>Process a task of transcription and update the task with the result.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>ASRTask</code> <p>The task and its parameters.</p> required <code>debug_mode</code> <code>bool</code> <p>Whether to run in debug mode or not.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The task is updated with the result.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def process_transcription(self, task: ASRTask, debug_mode: bool) -&gt; None:\n    \"\"\"\n    Process a task of transcription and update the task with the result.\n\n    Args:\n        task (ASRTask): The task and its parameters.\n        debug_mode (bool): Whether to run in debug mode or not.\n\n    Returns:\n        None: The task is updated with the result.\n    \"\"\"\n    try:\n        if isinstance(task.transcription.execution, LocalExecution):\n            out = await time_and_tell_async(\n                lambda: self.local_services.transcription(\n                    task.audio,\n                    model_index=task.transcription.execution.index,\n                    suppress_blank=False,\n                    word_timestamps=True,\n                    **task.transcription.options.model_dump(),\n                ),\n                func_name=\"transcription\",\n                debug_mode=debug_mode,\n            )\n            result, process_time = out\n\n        elif isinstance(task.transcription.execution, RemoteExecution):\n            if isinstance(task.audio, list):\n                ts = [\n                    TensorShare.from_dict({\"audio\": a}, backend=Backend.TORCH)\n                    for a in task.audio\n                ]\n            else:\n                ts = TensorShare.from_dict(\n                    {\"audio\": task.audio}, backend=Backend.TORCH\n                )\n\n            data = TranscribeRequest(\n                audio=ts,\n                **task.transcription.options.model_dump(),\n            )\n            out = await time_and_tell_async(\n                self.remote_transcription(\n                    url=task.transcription.execution.url,\n                    data=data,\n                ),\n                func_name=\"transcription\",\n                debug_mode=debug_mode,\n            )\n            result, process_time = out\n\n        else:\n            raise NotImplementedError(\"No execution method specified.\")\n\n    except Exception as e:\n        result = ProcessException(\n            source=ExceptionSource.transcription,\n            message=f\"Error in transcription: {e}\\n{traceback.format_exc()}\",\n        )\n        process_time = None\n\n    finally:\n        task.process_times.transcription = process_time\n        task.transcription.result = result\n\n    return None\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.remote_diarization","title":"<code>remote_diarization(url, data)</code>  <code>async</code>","text":"<p>Remote diarization method.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def remote_diarization(\n    self,\n    url: str,\n    data: DiarizationRequest,\n) -&gt; DiarizationOutput:\n    \"\"\"Remote diarization method.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            url=f\"{url}/api/v1/diarize\",\n            data=data.model_dump_json(),\n            headers={\"Content-Type\": \"application/json\"},\n        ) as response:\n            if response.status != 200:\n                r = await response.json()\n                raise Exception(r[\"detail\"])\n            else:\n                return DiarizationOutput(**await response.json())\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.remote_transcription","title":"<code>remote_transcription(url, data)</code>  <code>async</code>","text":"<p>Remote transcription method.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def remote_transcription(\n    self,\n    url: str,\n    data: TranscribeRequest,\n) -&gt; TranscriptionOutput:\n    \"\"\"Remote transcription method.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            url=f\"{url}/api/v1/transcribe\",\n            data=data.model_dump_json(),\n            headers={\"Content-Type\": \"application/json\"},\n        ) as response:\n            if response.status != 200:\n                raise Exception(response.status)\n            else:\n                return TranscriptionOutput(**await response.json())\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRAsyncService.remove_url","title":"<code>remove_url(data)</code>  <code>async</code>","text":"<p>Remove a remote URL from the list of URLs.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def remove_url(self, data: UrlSchema) -&gt; Union[UrlSchema, ProcessException]:\n    \"\"\"Remove a remote URL from the list of URLs.\"\"\"\n    try:\n        selected_task = getattr(self.remote_services, data.task)\n        # Case 1: We are not using remote task\n        if selected_task.use_remote is False:\n            raise ValueError(f\"You are not using remote {data.task}.\")\n        # Case 2: We are using remote task\n        else:\n            await selected_task.remove_url(str(data.url))\n            if selected_task.get_queue_size() == 0:\n                # No more remote URLs, switch to local service\n                self.create_local_service(task=data.task)\n                setattr(self.remote_services, data.task, RemoteServiceConfig())\n\n        return data\n\n    except Exception as e:\n        return ProcessException(\n            source=ExceptionSource.remove_url,\n            message=f\"Error in removing URL: {e}\\n{traceback.format_exc()}\",\n        )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRDiarizationOnly","title":"<code>ASRDiarizationOnly</code>","text":"<p>             Bases: <code>ASRService</code></p> <p>ASR Service module for diarization-only endpoint.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ASRDiarizationOnly(ASRService):\n    \"\"\"ASR Service module for diarization-only endpoint.\"\"\"\n\n    def __init__(\n        self,\n        window_lengths: List[int],\n        shift_lengths: List[int],\n        multiscale_weights: List[float],\n        debug_mode: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the ASRDiarizationOnly class.\"\"\"\n        super().__init__()\n\n        self.diarization_service = DiarizeService(\n            device=self.device,\n            device_index=self.device_index,\n            window_lengths=window_lengths,\n            shift_lengths=shift_lengths,\n            multiscale_weights=multiscale_weights,\n        )\n        self.vad_service = VadService()\n        self.debug_mode = debug_mode\n\n    async def inference_warmup(self) -&gt; None:\n        \"\"\"Warmup the GPU by doing one inference.\"\"\"\n        sample_audio = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n\n        audio, duration = read_audio(str(sample_audio))\n        ts = TensorShare.from_dict({\"audio\": audio}, backend=Backend.TORCH)\n\n        data = DiarizationRequest(\n            audio=ts,\n            duration=duration,\n            num_speakers=1,\n        )\n\n        for gpu_index in self.gpu_handler.device_index:\n            logger.info(f\"Warmup GPU {gpu_index}.\")\n            await self.process_input(data=data)\n\n    async def process_input(self, data: DiarizationRequest) -&gt; DiarizationOutput:\n        \"\"\"\n        Process the input data and return the results as a list of segments.\n\n        Args:\n            data (DiarizationRequest):\n                The input data to process.\n\n        Returns:\n            DiarizationOutput:\n                The results of the ASR pipeline.\n        \"\"\"\n        gpu_index = await self.gpu_handler.get_device()\n\n        try:\n            result = self.diarization_service(\n                waveform=data.audio,\n                audio_duration=data.duration,\n                oracle_num_speakers=data.num_speakers,\n                model_index=gpu_index,\n                vad_service=self.vad_service,\n            )\n\n        except Exception as e:\n            result = ProcessException(\n                source=ExceptionSource.diarization,\n                message=f\"Error in diarization: {e}\\n{traceback.format_exc()}\",\n            )\n\n        finally:\n            self.gpu_handler.release_device(gpu_index)\n\n        return result\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRDiarizationOnly.__init__","title":"<code>__init__(window_lengths, shift_lengths, multiscale_weights, debug_mode)</code>","text":"<p>Initialize the ASRDiarizationOnly class.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def __init__(\n    self,\n    window_lengths: List[int],\n    shift_lengths: List[int],\n    multiscale_weights: List[float],\n    debug_mode: bool,\n) -&gt; None:\n    \"\"\"Initialize the ASRDiarizationOnly class.\"\"\"\n    super().__init__()\n\n    self.diarization_service = DiarizeService(\n        device=self.device,\n        device_index=self.device_index,\n        window_lengths=window_lengths,\n        shift_lengths=shift_lengths,\n        multiscale_weights=multiscale_weights,\n    )\n    self.vad_service = VadService()\n    self.debug_mode = debug_mode\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRDiarizationOnly.inference_warmup","title":"<code>inference_warmup()</code>  <code>async</code>","text":"<p>Warmup the GPU by doing one inference.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def inference_warmup(self) -&gt; None:\n    \"\"\"Warmup the GPU by doing one inference.\"\"\"\n    sample_audio = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n\n    audio, duration = read_audio(str(sample_audio))\n    ts = TensorShare.from_dict({\"audio\": audio}, backend=Backend.TORCH)\n\n    data = DiarizationRequest(\n        audio=ts,\n        duration=duration,\n        num_speakers=1,\n    )\n\n    for gpu_index in self.gpu_handler.device_index:\n        logger.info(f\"Warmup GPU {gpu_index}.\")\n        await self.process_input(data=data)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRDiarizationOnly.process_input","title":"<code>process_input(data)</code>  <code>async</code>","text":"<p>Process the input data and return the results as a list of segments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DiarizationRequest</code> <p>The input data to process.</p> required <p>Returns:</p> Name Type Description <code>DiarizationOutput</code> <code>DiarizationOutput</code> <p>The results of the ASR pipeline.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def process_input(self, data: DiarizationRequest) -&gt; DiarizationOutput:\n    \"\"\"\n    Process the input data and return the results as a list of segments.\n\n    Args:\n        data (DiarizationRequest):\n            The input data to process.\n\n    Returns:\n        DiarizationOutput:\n            The results of the ASR pipeline.\n    \"\"\"\n    gpu_index = await self.gpu_handler.get_device()\n\n    try:\n        result = self.diarization_service(\n            waveform=data.audio,\n            audio_duration=data.duration,\n            oracle_num_speakers=data.num_speakers,\n            model_index=gpu_index,\n            vad_service=self.vad_service,\n        )\n\n    except Exception as e:\n        result = ProcessException(\n            source=ExceptionSource.diarization,\n            message=f\"Error in diarization: {e}\\n{traceback.format_exc()}\",\n        )\n\n    finally:\n        self.gpu_handler.release_device(gpu_index)\n\n    return result\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRLiveService","title":"<code>ASRLiveService</code>","text":"<p>             Bases: <code>ASRService</code></p> <p>ASR Service module for live endpoints.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ASRLiveService(ASRService):\n    \"\"\"ASR Service module for live endpoints.\"\"\"\n\n    def __init__(self, whisper_model: str, compute_type: str, debug_mode: bool) -&gt; None:\n        \"\"\"Initialize the ASRLiveService class.\"\"\"\n        super().__init__()\n\n        self.transcription_service = TranscribeService(\n            model_path=whisper_model,\n            compute_type=compute_type,\n            device=self.device,\n            device_index=self.device_index,\n        )\n        self.debug_mode = debug_mode\n\n    async def inference_warmup(self) -&gt; None:\n        \"\"\"Warmup the GPU by loading the models.\"\"\"\n        sample_audio = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n        with open(sample_audio, \"rb\") as audio_file:\n            async for _ in self.process_input(\n                data=audio_file.read(),\n                source_lang=\"en\",\n            ):\n                pass\n\n    async def process_input(self, data: bytes, source_lang: str) -&gt; Iterable[dict]:\n        \"\"\"\n        Process the input data and return the results as a tuple of text and duration.\n\n        Args:\n            data (bytes):\n                The raw audio bytes to process.\n            source_lang (str):\n                The source language of the audio data.\n\n        Yields:\n            Iterable[dict]: The results of the ASR pipeline.\n        \"\"\"\n        gpu_index = await self.gpu_handler.get_device()\n\n        try:\n            waveform, _ = read_audio(data)\n\n            async for result in self.transcription_service.async_live_transcribe(\n                audio=waveform, source_lang=source_lang, model_index=gpu_index\n            ):\n                yield result\n\n        except Exception as e:\n            logger.error(\n                f\"Error in transcription gpu {gpu_index}: {e}\\n{traceback.format_exc()}\"\n            )\n\n        finally:\n            self.gpu_handler.release_device(gpu_index)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRLiveService.__init__","title":"<code>__init__(whisper_model, compute_type, debug_mode)</code>","text":"<p>Initialize the ASRLiveService class.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def __init__(self, whisper_model: str, compute_type: str, debug_mode: bool) -&gt; None:\n    \"\"\"Initialize the ASRLiveService class.\"\"\"\n    super().__init__()\n\n    self.transcription_service = TranscribeService(\n        model_path=whisper_model,\n        compute_type=compute_type,\n        device=self.device,\n        device_index=self.device_index,\n    )\n    self.debug_mode = debug_mode\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRLiveService.inference_warmup","title":"<code>inference_warmup()</code>  <code>async</code>","text":"<p>Warmup the GPU by loading the models.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def inference_warmup(self) -&gt; None:\n    \"\"\"Warmup the GPU by loading the models.\"\"\"\n    sample_audio = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n    with open(sample_audio, \"rb\") as audio_file:\n        async for _ in self.process_input(\n            data=audio_file.read(),\n            source_lang=\"en\",\n        ):\n            pass\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRLiveService.process_input","title":"<code>process_input(data, source_lang)</code>  <code>async</code>","text":"<p>Process the input data and return the results as a tuple of text and duration.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw audio bytes to process.</p> required <code>source_lang</code> <code>str</code> <p>The source language of the audio data.</p> required <p>Yields:</p> Type Description <code>Iterable[dict]</code> <p>Iterable[dict]: The results of the ASR pipeline.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def process_input(self, data: bytes, source_lang: str) -&gt; Iterable[dict]:\n    \"\"\"\n    Process the input data and return the results as a tuple of text and duration.\n\n    Args:\n        data (bytes):\n            The raw audio bytes to process.\n        source_lang (str):\n            The source language of the audio data.\n\n    Yields:\n        Iterable[dict]: The results of the ASR pipeline.\n    \"\"\"\n    gpu_index = await self.gpu_handler.get_device()\n\n    try:\n        waveform, _ = read_audio(data)\n\n        async for result in self.transcription_service.async_live_transcribe(\n            audio=waveform, source_lang=source_lang, model_index=gpu_index\n        ):\n            yield result\n\n    except Exception as e:\n        logger.error(\n            f\"Error in transcription gpu {gpu_index}: {e}\\n{traceback.format_exc()}\"\n        )\n\n    finally:\n        self.gpu_handler.release_device(gpu_index)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRService","title":"<code>ASRService</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base ASR Service module that handle all AI interactions and batch processing.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ASRService(ABC):\n    \"\"\"Base ASR Service module that handle all AI interactions and batch processing.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the ASR Service.\n\n        This class is not meant to be instantiated. Use the subclasses instead.\n        \"\"\"\n        self.device = (\n            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )  # Do we have a GPU? If so, use it!\n        self.num_gpus = torch.cuda.device_count() if self.device == \"cuda\" else 0\n        logger.info(f\"NVIDIA GPUs available: {self.num_gpus}\")\n\n        if self.num_gpus &gt; 1 and self.device == \"cuda\":\n            self.device_index = list(range(self.num_gpus))\n        else:\n            self.device_index = [0]\n\n        self.gpu_handler = GPUService(\n            device=self.device, device_index=self.device_index\n        )\n\n    @abstractmethod\n    async def process_input(self) -&gt; None:\n        \"\"\"Process the input request by creating a task and adding it to the appropriate queues.\"\"\"\n        raise NotImplementedError(\"This method should be implemented in subclasses.\")\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRService.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the ASR Service.</p> <p>This class is not meant to be instantiated. Use the subclasses instead.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ASR Service.\n\n    This class is not meant to be instantiated. Use the subclasses instead.\n    \"\"\"\n    self.device = (\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )  # Do we have a GPU? If so, use it!\n    self.num_gpus = torch.cuda.device_count() if self.device == \"cuda\" else 0\n    logger.info(f\"NVIDIA GPUs available: {self.num_gpus}\")\n\n    if self.num_gpus &gt; 1 and self.device == \"cuda\":\n        self.device_index = list(range(self.num_gpus))\n    else:\n        self.device_index = [0]\n\n    self.gpu_handler = GPUService(\n        device=self.device, device_index=self.device_index\n    )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRService.process_input","title":"<code>process_input()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process the input request by creating a task and adding it to the appropriate queues.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>@abstractmethod\nasync def process_input(self) -&gt; None:\n    \"\"\"Process the input request by creating a task and adding it to the appropriate queues.\"\"\"\n    raise NotImplementedError(\"This method should be implemented in subclasses.\")\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRTask","title":"<code>ASRTask</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>ASR Task model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ASRTask(BaseModel):\n    \"\"\"ASR Task model.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    audio: Union[torch.Tensor, List[torch.Tensor]]\n    diarization: \"DiarizationTask\"\n    duration: float\n    multi_channel: bool\n    offset_start: Union[float, None]\n    post_processing: \"PostProcessingTask\"\n    process_times: ProcessTimes\n    timestamps_format: Timestamps\n    transcription: \"TranscriptionTask\"\n    word_timestamps: bool\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRTranscriptionOnly","title":"<code>ASRTranscriptionOnly</code>","text":"<p>             Bases: <code>ASRService</code></p> <p>ASR Service module for transcription-only endpoint.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ASRTranscriptionOnly(ASRService):\n    \"\"\"ASR Service module for transcription-only endpoint.\"\"\"\n\n    def __init__(\n        self,\n        whisper_model: str,\n        compute_type: str,\n        extra_languages: Union[List[str], None],\n        extra_languages_model_paths: Union[List[str], None],\n        debug_mode: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the ASRTranscriptionOnly class.\"\"\"\n        super().__init__()\n\n        self.transcription_service = TranscribeService(\n            model_path=whisper_model,\n            compute_type=compute_type,\n            device=self.device,\n            device_index=self.device_index,\n            extra_languages=extra_languages,\n            extra_languages_model_paths=extra_languages_model_paths,\n        )\n        self.debug_mode = debug_mode\n\n    async def inference_warmup(self) -&gt; None:\n        \"\"\"Warmup the GPU by doing one inference.\"\"\"\n        sample_audio = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n\n        audio, _ = read_audio(str(sample_audio))\n        ts = TensorShare.from_dict({\"audio\": audio}, backend=Backend.TORCH)\n\n        data = TranscribeRequest(\n            audio=ts,\n            source_lang=\"en\",\n            compression_ratio_threshold=2.4,\n            condition_on_previous_text=True,\n            internal_vad=False,\n            log_prob_threshold=-1.0,\n            no_speech_threshold=0.6,\n            repetition_penalty=1.0,\n            vocab=None,\n        )\n\n        for gpu_index in self.gpu_handler.device_index:\n            logger.info(f\"Warmup GPU {gpu_index}.\")\n            await self.process_input(data=data)\n\n    async def process_input(\n        self, data: TranscribeRequest\n    ) -&gt; Union[TranscriptionOutput, List[TranscriptionOutput]]:\n        \"\"\"\n        Process the input data and return the results as a list of segments.\n\n        Args:\n            data (TranscribeRequest):\n                The input data to process.\n\n        Returns:\n            Union[TranscriptionOutput, List[TranscriptionOutput]]:\n                The results of the ASR pipeline.\n        \"\"\"\n        gpu_index = await self.gpu_handler.get_device()\n\n        try:\n            result = self.transcription_service(\n                audio=data.audio,\n                source_lang=data.source_lang,\n                model_index=gpu_index,\n                suppress_blank=False,\n                word_timestamps=True,\n                compression_ratio_threshold=data.compression_ratio_threshold,\n                condition_on_previous_text=data.condition_on_previous_text,\n                internal_vad=data.internal_vad,\n                log_prob_threshold=data.log_prob_threshold,\n                repetition_penalty=data.repetition_penalty,\n                no_speech_threshold=data.no_speech_threshold,\n                vocab=data.vocab,\n            )\n\n        except Exception as e:\n            result = ProcessException(\n                source=ExceptionSource.transcription,\n                message=f\"Error in transcription: {e}\\n{traceback.format_exc()}\",\n            )\n\n        finally:\n            self.gpu_handler.release_device(gpu_index)\n\n        return result\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRTranscriptionOnly.__init__","title":"<code>__init__(whisper_model, compute_type, extra_languages, extra_languages_model_paths, debug_mode)</code>","text":"<p>Initialize the ASRTranscriptionOnly class.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def __init__(\n    self,\n    whisper_model: str,\n    compute_type: str,\n    extra_languages: Union[List[str], None],\n    extra_languages_model_paths: Union[List[str], None],\n    debug_mode: bool,\n) -&gt; None:\n    \"\"\"Initialize the ASRTranscriptionOnly class.\"\"\"\n    super().__init__()\n\n    self.transcription_service = TranscribeService(\n        model_path=whisper_model,\n        compute_type=compute_type,\n        device=self.device,\n        device_index=self.device_index,\n        extra_languages=extra_languages,\n        extra_languages_model_paths=extra_languages_model_paths,\n    )\n    self.debug_mode = debug_mode\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRTranscriptionOnly.inference_warmup","title":"<code>inference_warmup()</code>  <code>async</code>","text":"<p>Warmup the GPU by doing one inference.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def inference_warmup(self) -&gt; None:\n    \"\"\"Warmup the GPU by doing one inference.\"\"\"\n    sample_audio = Path(__file__).parent.parent / \"assets/warmup_sample.wav\"\n\n    audio, _ = read_audio(str(sample_audio))\n    ts = TensorShare.from_dict({\"audio\": audio}, backend=Backend.TORCH)\n\n    data = TranscribeRequest(\n        audio=ts,\n        source_lang=\"en\",\n        compression_ratio_threshold=2.4,\n        condition_on_previous_text=True,\n        internal_vad=False,\n        log_prob_threshold=-1.0,\n        no_speech_threshold=0.6,\n        repetition_penalty=1.0,\n        vocab=None,\n    )\n\n    for gpu_index in self.gpu_handler.device_index:\n        logger.info(f\"Warmup GPU {gpu_index}.\")\n        await self.process_input(data=data)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ASRTranscriptionOnly.process_input","title":"<code>process_input(data)</code>  <code>async</code>","text":"<p>Process the input data and return the results as a list of segments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TranscribeRequest</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>Union[TranscriptionOutput, List[TranscriptionOutput]]</code> <p>Union[TranscriptionOutput, List[TranscriptionOutput]]: The results of the ASR pipeline.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def process_input(\n    self, data: TranscribeRequest\n) -&gt; Union[TranscriptionOutput, List[TranscriptionOutput]]:\n    \"\"\"\n    Process the input data and return the results as a list of segments.\n\n    Args:\n        data (TranscribeRequest):\n            The input data to process.\n\n    Returns:\n        Union[TranscriptionOutput, List[TranscriptionOutput]]:\n            The results of the ASR pipeline.\n    \"\"\"\n    gpu_index = await self.gpu_handler.get_device()\n\n    try:\n        result = self.transcription_service(\n            audio=data.audio,\n            source_lang=data.source_lang,\n            model_index=gpu_index,\n            suppress_blank=False,\n            word_timestamps=True,\n            compression_ratio_threshold=data.compression_ratio_threshold,\n            condition_on_previous_text=data.condition_on_previous_text,\n            internal_vad=data.internal_vad,\n            log_prob_threshold=data.log_prob_threshold,\n            repetition_penalty=data.repetition_penalty,\n            no_speech_threshold=data.no_speech_threshold,\n            vocab=data.vocab,\n        )\n\n    except Exception as e:\n        result = ProcessException(\n            source=ExceptionSource.transcription,\n            message=f\"Error in transcription: {e}\\n{traceback.format_exc()}\",\n        )\n\n    finally:\n        self.gpu_handler.release_device(gpu_index)\n\n    return result\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.DiarizationTask","title":"<code>DiarizationTask</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Diarization Task model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class DiarizationTask(BaseModel):\n    \"\"\"Diarization Task model.\"\"\"\n\n    execution: Union[LocalExecution, RemoteExecution, None]\n    num_speakers: int\n    result: Union[ProcessException, DiarizationOutput, None] = None\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ExceptionSource","title":"<code>ExceptionSource</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Exception source enum.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ExceptionSource(str, Enum):\n    \"\"\"Exception source enum.\"\"\"\n\n    add_url = \"add_url\"\n    diarization = \"diarization\"\n    get_url = \"get_url\"\n    post_processing = \"post_processing\"\n    remove_url = \"remove_url\"\n    transcription = \"transcription\"\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.LocalExecution","title":"<code>LocalExecution</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Local execution model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class LocalExecution(BaseModel):\n    \"\"\"Local execution model.\"\"\"\n\n    index: Union[int, None]\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.LocalServiceRegistry","title":"<code>LocalServiceRegistry</code>  <code>dataclass</code>","text":"<p>Registry for local services.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>@dataclass\nclass LocalServiceRegistry:\n    \"\"\"Registry for local services.\"\"\"\n\n    diarization: Union[DiarizeService, None] = None\n    post_processing: PostProcessingService = PostProcessingService()\n    transcription: Union[TranscribeService, None] = None\n    vad: VadService = VadService()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.PostProcessingTask","title":"<code>PostProcessingTask</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Post Processing Task model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class PostProcessingTask(BaseModel):\n    \"\"\"Post Processing Task model.\"\"\"\n\n    result: Union[ProcessException, List[Utterance], None] = None\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.ProcessException","title":"<code>ProcessException</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Process exception model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class ProcessException(BaseModel):\n    \"\"\"Process exception model.\"\"\"\n\n    source: ExceptionSource\n    message: str\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteExecution","title":"<code>RemoteExecution</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Remote execution model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class RemoteExecution(BaseModel):\n    \"\"\"Remote execution model.\"\"\"\n\n    url: str\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceConfig","title":"<code>RemoteServiceConfig</code>  <code>dataclass</code>","text":"<p>Remote service config.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>@dataclass\nclass RemoteServiceConfig:\n    \"\"\"Remote service config.\"\"\"\n\n    url_handler: Union[URLService, None] = None\n    use_remote: bool = False\n\n    def get_urls(self) -&gt; List[str]:\n        \"\"\"Get the list of URLs.\"\"\"\n        return self.url_handler.get_urls()\n\n    def get_queue_size(self) -&gt; int:\n        \"\"\"Get the queue size.\"\"\"\n        return self.url_handler.get_queue_size()\n\n    async def add_url(self, url: str) -&gt; None:\n        \"\"\"Add a URL to the list of URLs.\"\"\"\n        await self.url_handler.add_url(url)\n\n    async def next_url(self) -&gt; str:\n        \"\"\"Get the next URL.\"\"\"\n        return await self.url_handler.next_url()\n\n    async def remove_url(self, url: str) -&gt; None:\n        \"\"\"Remove a URL from the list of URLs.\"\"\"\n        await self.url_handler.remove_url(url)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceConfig.add_url","title":"<code>add_url(url)</code>  <code>async</code>","text":"<p>Add a URL to the list of URLs.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def add_url(self, url: str) -&gt; None:\n    \"\"\"Add a URL to the list of URLs.\"\"\"\n    await self.url_handler.add_url(url)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceConfig.get_queue_size","title":"<code>get_queue_size()</code>","text":"<p>Get the queue size.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def get_queue_size(self) -&gt; int:\n    \"\"\"Get the queue size.\"\"\"\n    return self.url_handler.get_queue_size()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceConfig.get_urls","title":"<code>get_urls()</code>","text":"<p>Get the list of URLs.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>def get_urls(self) -&gt; List[str]:\n    \"\"\"Get the list of URLs.\"\"\"\n    return self.url_handler.get_urls()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceConfig.next_url","title":"<code>next_url()</code>  <code>async</code>","text":"<p>Get the next URL.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def next_url(self) -&gt; str:\n    \"\"\"Get the next URL.\"\"\"\n    return await self.url_handler.next_url()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceConfig.remove_url","title":"<code>remove_url(url)</code>  <code>async</code>","text":"<p>Remove a URL from the list of URLs.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>async def remove_url(self, url: str) -&gt; None:\n    \"\"\"Remove a URL from the list of URLs.\"\"\"\n    await self.url_handler.remove_url(url)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.RemoteServiceRegistry","title":"<code>RemoteServiceRegistry</code>  <code>dataclass</code>","text":"<p>Registry for remote services.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>@dataclass\nclass RemoteServiceRegistry:\n    \"\"\"Registry for remote services.\"\"\"\n\n    diarization: RemoteServiceConfig = RemoteServiceConfig()\n    transcription: RemoteServiceConfig = RemoteServiceConfig()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.TranscriptionOptions","title":"<code>TranscriptionOptions</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Transcription options model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class TranscriptionOptions(BaseModel):\n    \"\"\"Transcription options model.\"\"\"\n\n    compression_ratio_threshold: float\n    condition_on_previous_text: bool\n    internal_vad: bool\n    log_prob_threshold: float\n    no_speech_threshold: float\n    repetition_penalty: float\n    source_lang: str\n    vocab: Union[List[str], None]\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.asr_service.TranscriptionTask","title":"<code>TranscriptionTask</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Transcription Task model.</p> Source code in <code>src/wordcab_transcribe/services/asr_service.py</code> <pre><code>class TranscriptionTask(BaseModel):\n    \"\"\"Transcription Task model.\"\"\"\n\n    execution: Union[LocalExecution, RemoteExecution]\n    options: TranscriptionOptions\n    result: Union[\n        ProcessException, TranscriptionOutput, List[TranscriptionOutput], None\n    ] = None\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.GPUService","title":"<code>GPUService</code>","text":"<p>GPU service class to handle gpu availability for models.</p> Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>class GPUService:\n    \"\"\"GPU service class to handle gpu availability for models.\"\"\"\n\n    def __init__(self, device: str, device_index: List[int]) -&gt; None:\n        \"\"\"\n        Initialize the GPU service.\n\n        Args:\n            device (str): Device to use for inference. Can be \"cpu\" or \"cuda\".\n            device_index (List[int]): Index of the device to use for inference.\n        \"\"\"\n        self.device: str = device\n        self.device_index: List[int] = device_index\n\n        self.queue = asyncio.Queue(maxsize=len(self.device_index))\n        for idx in self.device_index:\n            self.queue.put_nowait(idx)\n\n    async def get_device(self) -&gt; int:\n        \"\"\"\n        Get the next available device.\n\n        Returns:\n            int: Index of the next available device.\n        \"\"\"\n        while True:\n            try:\n                device_index = self.queue.get_nowait()\n                return device_index\n            except asyncio.QueueEmpty:\n                await asyncio.sleep(1.0)\n\n    def release_device(self, device_index: int) -&gt; None:\n        \"\"\"\n        Return a device to the available devices list.\n\n        Args:\n            device_index (int): Index of the device to add to the available devices list.\n        \"\"\"\n        if not any(item == device_index for item in self.queue._queue):\n            self.queue.put_nowait(device_index)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.GPUService.__init__","title":"<code>__init__(device, device_index)</code>","text":"<p>Initialize the GPU service.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device to use for inference. Can be \"cpu\" or \"cuda\".</p> required <code>device_index</code> <code>List[int]</code> <p>Index of the device to use for inference.</p> required Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>def __init__(self, device: str, device_index: List[int]) -&gt; None:\n    \"\"\"\n    Initialize the GPU service.\n\n    Args:\n        device (str): Device to use for inference. Can be \"cpu\" or \"cuda\".\n        device_index (List[int]): Index of the device to use for inference.\n    \"\"\"\n    self.device: str = device\n    self.device_index: List[int] = device_index\n\n    self.queue = asyncio.Queue(maxsize=len(self.device_index))\n    for idx in self.device_index:\n        self.queue.put_nowait(idx)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.GPUService.get_device","title":"<code>get_device()</code>  <code>async</code>","text":"<p>Get the next available device.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Index of the next available device.</p> Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>async def get_device(self) -&gt; int:\n    \"\"\"\n    Get the next available device.\n\n    Returns:\n        int: Index of the next available device.\n    \"\"\"\n    while True:\n        try:\n            device_index = self.queue.get_nowait()\n            return device_index\n        except asyncio.QueueEmpty:\n            await asyncio.sleep(1.0)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.GPUService.release_device","title":"<code>release_device(device_index)</code>","text":"<p>Return a device to the available devices list.</p> <p>Parameters:</p> Name Type Description Default <code>device_index</code> <code>int</code> <p>Index of the device to add to the available devices list.</p> required Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>def release_device(self, device_index: int) -&gt; None:\n    \"\"\"\n    Return a device to the available devices list.\n\n    Args:\n        device_index (int): Index of the device to add to the available devices list.\n    \"\"\"\n    if not any(item == device_index for item in self.queue._queue):\n        self.queue.put_nowait(device_index)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService","title":"<code>URLService</code>","text":"<p>URL service class to handle multiple remote URLs.</p> Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>class URLService:\n    \"\"\"URL service class to handle multiple remote URLs.\"\"\"\n\n    def __init__(self, remote_urls: List[str]) -&gt; None:\n        \"\"\"\n        Initialize the URL service.\n\n        Args:\n            remote_urls (List[str]): List of remote URLs to use.\n        \"\"\"\n        self.remote_urls: List[str] = remote_urls\n        self._init_queue()\n\n    def _init_queue(self) -&gt; None:\n        \"\"\"Initialize the queue with the available URLs.\"\"\"\n        self.queue = asyncio.Queue(maxsize=len(self.remote_urls))\n        for url in self.remote_urls:\n            self.queue.put_nowait(url)\n\n    def get_queue_size(self) -&gt; int:\n        \"\"\"\n        Get the current queue size.\n\n        Returns:\n            int: Current queue size.\n        \"\"\"\n        return self.queue.qsize()\n\n    def get_urls(self) -&gt; List[str]:\n        \"\"\"\n        Get the list of available URLs.\n\n        Returns:\n            List[str]: List of available URLs.\n        \"\"\"\n        return self.remote_urls\n\n    async def next_url(self) -&gt; str:\n        \"\"\"\n        We use this to iterate equally over the available URLs.\n\n        Returns:\n            str: Next available URL.\n        \"\"\"\n        url = self.queue.get_nowait()\n        # Unlike GPU we don't want to block remote ASR requests.\n        # So we re-insert the URL back into the queue after getting it.\n        self.queue.put_nowait(url)\n\n        return url\n\n    async def add_url(self, url: str) -&gt; None:\n        \"\"\"\n        Add a URL to the pool of available URLs.\n\n        Args:\n            url (str): URL to add to the queue.\n        \"\"\"\n        if url not in self.remote_urls:\n            self.remote_urls.append(url)\n\n            # Re-initialize the queue with the new URL.\n            self._init_queue()\n\n    async def remove_url(self, url: str) -&gt; None:\n        \"\"\"\n        Remove a URL from the pool of available URLs.\n\n        Args:\n            url (str): URL to remove from the queue.\n        \"\"\"\n        if url in self.remote_urls:\n            self.remote_urls.remove(url)\n\n            # Re-initialize the queue without the removed URL.\n            self._init_queue()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService.__init__","title":"<code>__init__(remote_urls)</code>","text":"<p>Initialize the URL service.</p> <p>Parameters:</p> Name Type Description Default <code>remote_urls</code> <code>List[str]</code> <p>List of remote URLs to use.</p> required Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>def __init__(self, remote_urls: List[str]) -&gt; None:\n    \"\"\"\n    Initialize the URL service.\n\n    Args:\n        remote_urls (List[str]): List of remote URLs to use.\n    \"\"\"\n    self.remote_urls: List[str] = remote_urls\n    self._init_queue()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService.add_url","title":"<code>add_url(url)</code>  <code>async</code>","text":"<p>Add a URL to the pool of available URLs.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to add to the queue.</p> required Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>async def add_url(self, url: str) -&gt; None:\n    \"\"\"\n    Add a URL to the pool of available URLs.\n\n    Args:\n        url (str): URL to add to the queue.\n    \"\"\"\n    if url not in self.remote_urls:\n        self.remote_urls.append(url)\n\n        # Re-initialize the queue with the new URL.\n        self._init_queue()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService.get_queue_size","title":"<code>get_queue_size()</code>","text":"<p>Get the current queue size.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Current queue size.</p> Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>def get_queue_size(self) -&gt; int:\n    \"\"\"\n    Get the current queue size.\n\n    Returns:\n        int: Current queue size.\n    \"\"\"\n    return self.queue.qsize()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService.get_urls","title":"<code>get_urls()</code>","text":"<p>Get the list of available URLs.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of available URLs.</p> Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>def get_urls(self) -&gt; List[str]:\n    \"\"\"\n    Get the list of available URLs.\n\n    Returns:\n        List[str]: List of available URLs.\n    \"\"\"\n    return self.remote_urls\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService.next_url","title":"<code>next_url()</code>  <code>async</code>","text":"<p>We use this to iterate equally over the available URLs.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Next available URL.</p> Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>async def next_url(self) -&gt; str:\n    \"\"\"\n    We use this to iterate equally over the available URLs.\n\n    Returns:\n        str: Next available URL.\n    \"\"\"\n    url = self.queue.get_nowait()\n    # Unlike GPU we don't want to block remote ASR requests.\n    # So we re-insert the URL back into the queue after getting it.\n    self.queue.put_nowait(url)\n\n    return url\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.concurrency_services.URLService.remove_url","title":"<code>remove_url(url)</code>  <code>async</code>","text":"<p>Remove a URL from the pool of available URLs.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to remove from the queue.</p> required Source code in <code>src/wordcab_transcribe/services/concurrency_services.py</code> <pre><code>async def remove_url(self, url: str) -&gt; None:\n    \"\"\"\n    Remove a URL from the pool of available URLs.\n\n    Args:\n        url (str): URL to remove from the queue.\n    \"\"\"\n    if url in self.remote_urls:\n        self.remote_urls.remove(url)\n\n        # Re-initialize the queue without the removed URL.\n        self._init_queue()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.diarization.diarize_service.DiarizationModels","title":"<code>DiarizationModels</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Diarization Models.</p> Source code in <code>src/wordcab_transcribe/services/diarization/diarize_service.py</code> <pre><code>class DiarizationModels(NamedTuple):\n    \"\"\"Diarization Models.\"\"\"\n\n    segmentation: SegmentationModule\n    clustering: ClusteringModule\n    device: str\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.diarization.diarize_service.DiarizeService","title":"<code>DiarizeService</code>","text":"<p>Diarize Service for audio files.</p> Source code in <code>src/wordcab_transcribe/services/diarization/diarize_service.py</code> <pre><code>class DiarizeService:\n    \"\"\"Diarize Service for audio files.\"\"\"\n\n    def __init__(\n        self,\n        device: str,\n        device_index: List[int],\n        window_lengths: List[float],\n        shift_lengths: List[float],\n        multiscale_weights: List[int],\n        max_num_speakers: int = 8,\n    ) -&gt; None:\n        \"\"\"Initialize the Diarize Service.\n\n        This service uses the NVIDIA NeMo diarization models.\n\n        Args:\n            device (str): Device to use for inference. Can be \"cpu\" or \"cuda\".\n            device_index (Union[int, List[int]]): Index of the device to use for inference.\n            window_lengths (List[float]): List of window lengths.\n            shift_lengths (List[float]): List of shift lengths.\n            multiscale_weights (List[int]): List of weights for each scale.\n            max_num_speakers (int): Maximum number of speakers. Defaults to 8.\n        \"\"\"\n        self.device = device\n        self.models = {}\n\n        self.max_num_speakers = max_num_speakers\n        self.default_window_lengths = window_lengths\n        self.default_shift_lengths = shift_lengths\n        self.default_multiscale_weights = multiscale_weights\n\n        if len(self.default_multiscale_weights) &gt; 3:\n            self.default_segmentation_batch_size = 64\n        elif len(self.default_multiscale_weights) &gt; 1:\n            self.default_segmentation_batch_size = 128\n        else:\n            self.default_segmentation_batch_size = 256\n\n        self.default_scale_dict = dict(enumerate(zip(window_lengths, shift_lengths)))\n\n        for idx in device_index:\n            _device = f\"cuda:{idx}\" if self.device == \"cuda\" else \"cpu\"\n\n            segmentation_module = SegmentationModule(_device)\n            clustering_module = ClusteringModule(_device, self.max_num_speakers)\n\n            self.models[idx] = DiarizationModels(\n                segmentation=segmentation_module,\n                clustering=clustering_module,\n                device=_device,\n            )\n\n    def __call__(\n        self,\n        waveform: Union[torch.Tensor, TensorShare],\n        audio_duration: float,\n        oracle_num_speakers: int,\n        model_index: int,\n        vad_service: VadService,\n    ) -&gt; DiarizationOutput:\n        \"\"\"\n        Run inference with the diarization model.\n\n        Args:\n            waveform (Union[torch.Tensor, TensorShare]):\n                Waveform to run inference on.\n            audio_duration (float):\n                Duration of the audio file in seconds.\n            oracle_num_speakers (int):\n                Number of speakers in the audio file.\n            model_index (int):\n                Index of the model to use for inference.\n            vad_service (VadService):\n                VAD service instance to use for Voice Activity Detection.\n\n        Returns:\n            DiarizationOutput:\n                List of segments with the following keys: \"start\", \"end\", \"speaker\".\n        \"\"\"\n        if isinstance(waveform, TensorShare):\n            ts = waveform.to_tensors(backend=Backend.TORCH)\n            waveform = ts[\"audio\"]\n\n        vad_outputs, _ = vad_service(waveform, group_timestamps=False)\n\n        if len(vad_outputs) == 0:  # Empty audio\n            return None\n\n        if audio_duration &lt; 3600:\n            scale_dict = self.default_scale_dict\n            segmentation_batch_size = self.default_segmentation_batch_size\n            multiscale_weights = self.default_multiscale_weights\n        elif audio_duration &lt; 10800:\n            scale_dict = dict(\n                enumerate(\n                    zip(\n                        [3.0, 2.5, 2.0, 1.5, 1.0],\n                        self.default_shift_lengths,\n                    )\n                )\n            )\n            segmentation_batch_size = 64\n            multiscale_weights = self.default_multiscale_weights\n        else:\n            scale_dict = dict(enumerate(zip([3.0, 2.0, 1.0], [0.75, 0.5, 0.25])))\n            segmentation_batch_size = 32\n            multiscale_weights = [1.0, 1.0, 1.0]\n\n        ms_emb_ts: MultiscaleEmbeddingsAndTimestamps = self.models[\n            model_index\n        ].segmentation(\n            waveform=waveform,\n            batch_size=segmentation_batch_size,\n            vad_outputs=vad_outputs,\n            scale_dict=scale_dict,\n            multiscale_weights=multiscale_weights,\n        )\n\n        clustering_outputs = self.models[model_index].clustering(\n            ms_emb_ts, oracle_num_speakers\n        )\n\n        _outputs = self.get_contiguous_stamps(clustering_outputs)\n        outputs = self.merge_stamps(_outputs)\n\n        return DiarizationOutput(segments=outputs)\n\n    @staticmethod\n    def get_contiguous_stamps(\n        stamps: List[Tuple[float, float, int]]\n    ) -&gt; List[Tuple[float, float, int]]:\n        \"\"\"\n        Return contiguous timestamps.\n\n        Args:\n            stamps (List[Tuple[float, float, int]]): List of segments containing the start time, end time and speaker.\n\n        Returns:\n            List[Tuple[float, float, int]]: List of segments containing the start time, end time and speaker.\n        \"\"\"\n        contiguous_stamps = []\n        for i in range(len(stamps) - 1):\n            start, end, speaker = stamps[i]\n            next_start, next_end, next_speaker = stamps[i + 1]\n\n            if end &gt; next_start:\n                avg = (next_start + end) / 2.0\n                stamps[i + 1] = (avg, next_end, next_speaker)\n                contiguous_stamps.append((start, avg, speaker))\n            else:\n                contiguous_stamps.append((start, end, speaker))\n\n        start, end, speaker = stamps[-1]\n        contiguous_stamps.append((start, end, speaker))\n\n        return contiguous_stamps\n\n    @staticmethod\n    def merge_stamps(\n        stamps: List[Tuple[float, float, int]]\n    ) -&gt; List[Tuple[float, float, int]]:\n        \"\"\"\n        Merge timestamps of the same speaker.\n\n        Args:\n            stamps (List[Tuple[float, float, int]]): List of segments containing the start time, end time and speaker.\n\n        Returns:\n            List[Tuple[float, float, int]]: List of segments containing the start time, end time and speaker.\n        \"\"\"\n        overlap_stamps = []\n        for i in range(len(stamps) - 1):\n            start, end, speaker = stamps[i]\n            next_start, next_end, next_speaker = stamps[i + 1]\n\n            if end == next_start and speaker == next_speaker:\n                stamps[i + 1] = (start, next_end, next_speaker)\n            else:\n                overlap_stamps.append((start, end, speaker))\n\n        start, end, speaker = stamps[-1]\n        overlap_stamps.append((start, end, speaker))\n\n        return overlap_stamps\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.diarization.diarize_service.DiarizeService.__call__","title":"<code>__call__(waveform, audio_duration, oracle_num_speakers, model_index, vad_service)</code>","text":"<p>Run inference with the diarization model.</p> <p>Parameters:</p> Name Type Description Default <code>waveform</code> <code>Union[Tensor, TensorShare]</code> <p>Waveform to run inference on.</p> required <code>audio_duration</code> <code>float</code> <p>Duration of the audio file in seconds.</p> required <code>oracle_num_speakers</code> <code>int</code> <p>Number of speakers in the audio file.</p> required <code>model_index</code> <code>int</code> <p>Index of the model to use for inference.</p> required <code>vad_service</code> <code>VadService</code> <p>VAD service instance to use for Voice Activity Detection.</p> required <p>Returns:</p> Name Type Description <code>DiarizationOutput</code> <code>DiarizationOutput</code> <p>List of segments with the following keys: \"start\", \"end\", \"speaker\".</p> Source code in <code>src/wordcab_transcribe/services/diarization/diarize_service.py</code> <pre><code>def __call__(\n    self,\n    waveform: Union[torch.Tensor, TensorShare],\n    audio_duration: float,\n    oracle_num_speakers: int,\n    model_index: int,\n    vad_service: VadService,\n) -&gt; DiarizationOutput:\n    \"\"\"\n    Run inference with the diarization model.\n\n    Args:\n        waveform (Union[torch.Tensor, TensorShare]):\n            Waveform to run inference on.\n        audio_duration (float):\n            Duration of the audio file in seconds.\n        oracle_num_speakers (int):\n            Number of speakers in the audio file.\n        model_index (int):\n            Index of the model to use for inference.\n        vad_service (VadService):\n            VAD service instance to use for Voice Activity Detection.\n\n    Returns:\n        DiarizationOutput:\n            List of segments with the following keys: \"start\", \"end\", \"speaker\".\n    \"\"\"\n    if isinstance(waveform, TensorShare):\n        ts = waveform.to_tensors(backend=Backend.TORCH)\n        waveform = ts[\"audio\"]\n\n    vad_outputs, _ = vad_service(waveform, group_timestamps=False)\n\n    if len(vad_outputs) == 0:  # Empty audio\n        return None\n\n    if audio_duration &lt; 3600:\n        scale_dict = self.default_scale_dict\n        segmentation_batch_size = self.default_segmentation_batch_size\n        multiscale_weights = self.default_multiscale_weights\n    elif audio_duration &lt; 10800:\n        scale_dict = dict(\n            enumerate(\n                zip(\n                    [3.0, 2.5, 2.0, 1.5, 1.0],\n                    self.default_shift_lengths,\n                )\n            )\n        )\n        segmentation_batch_size = 64\n        multiscale_weights = self.default_multiscale_weights\n    else:\n        scale_dict = dict(enumerate(zip([3.0, 2.0, 1.0], [0.75, 0.5, 0.25])))\n        segmentation_batch_size = 32\n        multiscale_weights = [1.0, 1.0, 1.0]\n\n    ms_emb_ts: MultiscaleEmbeddingsAndTimestamps = self.models[\n        model_index\n    ].segmentation(\n        waveform=waveform,\n        batch_size=segmentation_batch_size,\n        vad_outputs=vad_outputs,\n        scale_dict=scale_dict,\n        multiscale_weights=multiscale_weights,\n    )\n\n    clustering_outputs = self.models[model_index].clustering(\n        ms_emb_ts, oracle_num_speakers\n    )\n\n    _outputs = self.get_contiguous_stamps(clustering_outputs)\n    outputs = self.merge_stamps(_outputs)\n\n    return DiarizationOutput(segments=outputs)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.diarization.diarize_service.DiarizeService.__init__","title":"<code>__init__(device, device_index, window_lengths, shift_lengths, multiscale_weights, max_num_speakers=8)</code>","text":"<p>Initialize the Diarize Service.</p> <p>This service uses the NVIDIA NeMo diarization models.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device to use for inference. Can be \"cpu\" or \"cuda\".</p> required <code>device_index</code> <code>Union[int, List[int]]</code> <p>Index of the device to use for inference.</p> required <code>window_lengths</code> <code>List[float]</code> <p>List of window lengths.</p> required <code>shift_lengths</code> <code>List[float]</code> <p>List of shift lengths.</p> required <code>multiscale_weights</code> <code>List[int]</code> <p>List of weights for each scale.</p> required <code>max_num_speakers</code> <code>int</code> <p>Maximum number of speakers. Defaults to 8.</p> <code>8</code> Source code in <code>src/wordcab_transcribe/services/diarization/diarize_service.py</code> <pre><code>def __init__(\n    self,\n    device: str,\n    device_index: List[int],\n    window_lengths: List[float],\n    shift_lengths: List[float],\n    multiscale_weights: List[int],\n    max_num_speakers: int = 8,\n) -&gt; None:\n    \"\"\"Initialize the Diarize Service.\n\n    This service uses the NVIDIA NeMo diarization models.\n\n    Args:\n        device (str): Device to use for inference. Can be \"cpu\" or \"cuda\".\n        device_index (Union[int, List[int]]): Index of the device to use for inference.\n        window_lengths (List[float]): List of window lengths.\n        shift_lengths (List[float]): List of shift lengths.\n        multiscale_weights (List[int]): List of weights for each scale.\n        max_num_speakers (int): Maximum number of speakers. Defaults to 8.\n    \"\"\"\n    self.device = device\n    self.models = {}\n\n    self.max_num_speakers = max_num_speakers\n    self.default_window_lengths = window_lengths\n    self.default_shift_lengths = shift_lengths\n    self.default_multiscale_weights = multiscale_weights\n\n    if len(self.default_multiscale_weights) &gt; 3:\n        self.default_segmentation_batch_size = 64\n    elif len(self.default_multiscale_weights) &gt; 1:\n        self.default_segmentation_batch_size = 128\n    else:\n        self.default_segmentation_batch_size = 256\n\n    self.default_scale_dict = dict(enumerate(zip(window_lengths, shift_lengths)))\n\n    for idx in device_index:\n        _device = f\"cuda:{idx}\" if self.device == \"cuda\" else \"cpu\"\n\n        segmentation_module = SegmentationModule(_device)\n        clustering_module = ClusteringModule(_device, self.max_num_speakers)\n\n        self.models[idx] = DiarizationModels(\n            segmentation=segmentation_module,\n            clustering=clustering_module,\n            device=_device,\n        )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.diarization.diarize_service.DiarizeService.get_contiguous_stamps","title":"<code>get_contiguous_stamps(stamps)</code>  <code>staticmethod</code>","text":"<p>Return contiguous timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>stamps</code> <code>List[Tuple[float, float, int]]</code> <p>List of segments containing the start time, end time and speaker.</p> required <p>Returns:</p> Type Description <code>List[Tuple[float, float, int]]</code> <p>List[Tuple[float, float, int]]: List of segments containing the start time, end time and speaker.</p> Source code in <code>src/wordcab_transcribe/services/diarization/diarize_service.py</code> <pre><code>@staticmethod\ndef get_contiguous_stamps(\n    stamps: List[Tuple[float, float, int]]\n) -&gt; List[Tuple[float, float, int]]:\n    \"\"\"\n    Return contiguous timestamps.\n\n    Args:\n        stamps (List[Tuple[float, float, int]]): List of segments containing the start time, end time and speaker.\n\n    Returns:\n        List[Tuple[float, float, int]]: List of segments containing the start time, end time and speaker.\n    \"\"\"\n    contiguous_stamps = []\n    for i in range(len(stamps) - 1):\n        start, end, speaker = stamps[i]\n        next_start, next_end, next_speaker = stamps[i + 1]\n\n        if end &gt; next_start:\n            avg = (next_start + end) / 2.0\n            stamps[i + 1] = (avg, next_end, next_speaker)\n            contiguous_stamps.append((start, avg, speaker))\n        else:\n            contiguous_stamps.append((start, end, speaker))\n\n    start, end, speaker = stamps[-1]\n    contiguous_stamps.append((start, end, speaker))\n\n    return contiguous_stamps\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.diarization.diarize_service.DiarizeService.merge_stamps","title":"<code>merge_stamps(stamps)</code>  <code>staticmethod</code>","text":"<p>Merge timestamps of the same speaker.</p> <p>Parameters:</p> Name Type Description Default <code>stamps</code> <code>List[Tuple[float, float, int]]</code> <p>List of segments containing the start time, end time and speaker.</p> required <p>Returns:</p> Type Description <code>List[Tuple[float, float, int]]</code> <p>List[Tuple[float, float, int]]: List of segments containing the start time, end time and speaker.</p> Source code in <code>src/wordcab_transcribe/services/diarization/diarize_service.py</code> <pre><code>@staticmethod\ndef merge_stamps(\n    stamps: List[Tuple[float, float, int]]\n) -&gt; List[Tuple[float, float, int]]:\n    \"\"\"\n    Merge timestamps of the same speaker.\n\n    Args:\n        stamps (List[Tuple[float, float, int]]): List of segments containing the start time, end time and speaker.\n\n    Returns:\n        List[Tuple[float, float, int]]: List of segments containing the start time, end time and speaker.\n    \"\"\"\n    overlap_stamps = []\n    for i in range(len(stamps) - 1):\n        start, end, speaker = stamps[i]\n        next_start, next_end, next_speaker = stamps[i + 1]\n\n        if end == next_start and speaker == next_speaker:\n            stamps[i + 1] = (start, next_end, next_speaker)\n        else:\n            overlap_stamps.append((start, end, speaker))\n\n    start, end, speaker = stamps[-1]\n    overlap_stamps.append((start, end, speaker))\n\n    return overlap_stamps\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService","title":"<code>PostProcessingService</code>","text":"<p>Post-Processing Service for audio files.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>class PostProcessingService:\n    \"\"\"Post-Processing Service for audio files.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the PostProcessingService.\"\"\"\n        self.sample_rate = 16000\n\n    def single_channel_speaker_mapping(\n        self,\n        transcript_segments: List[Utterance],\n        speaker_timestamps: DiarizationOutput,\n        word_timestamps: bool,\n    ) -&gt; List[Utterance]:\n        \"\"\"Run the post-processing functions on the inputs.\n\n        The postprocessing pipeline is as follows:\n        1. Map each transcript segment to its corresponding speaker.\n        2. Group utterances of the same speaker together.\n\n        Args:\n            transcript_segments (List[Utterance]):\n                List of transcript utterances.\n            speaker_timestamps (DiarizationOutput):\n                List of speaker timestamps.\n            word_timestamps (bool):\n                Whether to include word timestamps.\n\n        Returns:\n            List[Utterance]:\n                List of utterances with speaker mapping.\n        \"\"\"\n        segments_with_speaker_mapping = self.segments_speaker_mapping(\n            transcript_segments,\n            speaker_timestamps.segments,\n        )\n\n        utterances = self.reconstruct_utterances(\n            segments_with_speaker_mapping, word_timestamps\n        )\n\n        return utterances\n\n    def multi_channel_speaker_mapping(\n        self, multi_channel_segments: List[MultiChannelTranscriptionOutput]\n    ) -&gt; TranscriptionOutput:\n        \"\"\"\n        Run the multi-channel post-processing functions on the inputs by merging the segments based on the timestamps.\n\n        Args:\n            multi_channel_segments (List[MultiChannelTranscriptionOutput]):\n                List of segments from multi speakers.\n\n        Returns:\n            TranscriptionOutput: List of sentences with speaker mapping.\n        \"\"\"\n        words_with_speaker_mapping = [\n            (segment.speaker, word)\n            for output in multi_channel_segments\n            for segment in output.segments\n            for word in segment.words\n        ]\n        words_with_speaker_mapping.sort(key=lambda x: x[1].start)\n\n        utterances: List[Utterance] = self.reconstruct_multi_channel_utterances(\n            words_with_speaker_mapping\n        )\n\n        return utterances\n\n    def segments_speaker_mapping(\n        self,\n        transcript_segments: List[Utterance],\n        speaker_timestamps: List[DiarizationSegment],\n    ) -&gt; List[dict]:\n        \"\"\"Function to map transcription and diarization results.\n\n        Map each segment to its corresponding speaker based on the speaker timestamps and reconstruct the utterances\n        when the speaker changes in the middle of a segment.\n\n        Args:\n            transcript_segments (List[dict]): List of transcript segments.\n            speaker_timestamps (List[dict]): List of speaker timestamps.\n\n        Returns:\n            List[dict]: List of sentences with speaker mapping.\n        \"\"\"\n\n        def _assign_speaker(\n            mapping: list,\n            seg_index: int,\n            split: bool,\n            current_speaker: str,\n            current_split_len: int,\n        ):\n            \"\"\"Assign speaker to the segment.\"\"\"\n            if split and len(mapping) &gt; 1:\n                last_split_len = len(mapping[seg_index - 1].text)\n                if last_split_len &gt; current_split_len:\n                    current_speaker = mapping[seg_index - 1].speaker\n                elif last_split_len &lt; current_split_len:\n                    mapping[seg_index - 1].speaker = current_speaker\n            return current_speaker\n\n        threshold = 0.3\n        turn_idx = 0\n        was_split = False\n        _, end, speaker = speaker_timestamps[turn_idx]\n\n        segment_index = 0\n        segment_speaker_mapping = []\n        while segment_index &lt; len(transcript_segments):\n            segment: Utterance = transcript_segments[segment_index]\n            segment_start, segment_end, segment_text = (\n                segment.start,\n                segment.end,\n                segment.text,\n            )\n            while (\n                segment_start &gt; float(end)\n                or abs(segment_start - float(end)) &lt; threshold\n            ):\n                turn_idx += 1\n                turn_idx = min(turn_idx, len(speaker_timestamps) - 1)\n                _, end, speaker = speaker_timestamps[turn_idx]\n                if turn_idx == len(speaker_timestamps) - 1:\n                    end = segment_end\n                    break\n\n            if segment_end &gt; float(end) and abs(segment_end - float(end)) &gt; threshold:\n                words = segment.words\n                word_index = next(\n                    (\n                        i\n                        for i, word in enumerate(words)\n                        if word.start &gt; float(end)\n                        or abs(word.start - float(end)) &lt; threshold\n                    ),\n                    None,\n                )\n\n                if word_index is not None:\n                    _split_segment = segment_text.split()\n\n                    if word_index &gt; 0:\n                        text = \" \".join(_split_segment[:word_index])\n                        speaker = _assign_speaker(\n                            segment_speaker_mapping,\n                            segment_index,\n                            was_split,\n                            speaker,\n                            len(text),\n                        )\n\n                        _segment_to_add = Utterance(\n                            start=words[0].start,\n                            end=words[word_index - 1].end,\n                            text=text,\n                            speaker=speaker,\n                            words=words[:word_index],\n                        )\n                    else:\n                        text = _split_segment[0]\n                        speaker = _assign_speaker(\n                            segment_speaker_mapping,\n                            segment_index,\n                            was_split,\n                            speaker,\n                            len(text),\n                        )\n\n                        _segment_to_add = Utterance(\n                            start=words[0].start,\n                            end=words[0].end,\n                            text=_split_segment[0],\n                            speaker=speaker,\n                            words=words[:1],\n                        )\n                    segment_speaker_mapping.append(_segment_to_add)\n                    transcript_segments.insert(\n                        segment_index + 1,\n                        Utterance(\n                            start=words[word_index].start,\n                            end=segment_end,\n                            text=\" \".join(_split_segment[word_index:]),\n                            words=words[word_index:],\n                        ),\n                    )\n                    was_split = True\n                else:\n                    speaker = _assign_speaker(\n                        segment_speaker_mapping,\n                        segment_index,\n                        was_split,\n                        speaker,\n                        len(segment_text),\n                    )\n                    was_split = False\n\n                    segment_speaker_mapping.append(\n                        Utterance(\n                            start=segment_start,\n                            end=segment_end,\n                            text=segment_text,\n                            speaker=speaker,\n                            words=words,\n                        )\n                    )\n            else:\n                speaker = _assign_speaker(\n                    segment_speaker_mapping,\n                    segment_index,\n                    was_split,\n                    speaker,\n                    len(segment_text),\n                )\n                was_split = False\n\n                segment_speaker_mapping.append(\n                    Utterance(\n                        start=segment_start,\n                        end=segment_end,\n                        text=segment_text,\n                        speaker=speaker,\n                        words=segment.words,\n                    )\n                )\n            segment_index += 1\n\n        return segment_speaker_mapping\n\n    def reconstruct_utterances(\n        self,\n        transcript_segments: List[Utterance],\n        word_timestamps: bool,\n    ) -&gt; List[Utterance]:\n        \"\"\"\n        Reconstruct the utterances based on the speaker mapping.\n\n        Args:\n            transcript_words (List[Utterance]):\n                List of transcript segments.\n            word_timestamps (bool):\n                Whether to include word timestamps.\n\n        Returns:\n            List[Utterance]:\n                List of sentences with speaker mapping.\n        \"\"\"\n        start_t0, end_t0, speaker_t0 = (\n            transcript_segments[0].start,\n            transcript_segments[0].end,\n            transcript_segments[0].speaker,\n        )\n\n        previous_speaker = speaker_t0\n        current_sentence = {\n            \"speaker\": speaker_t0,\n            \"start\": start_t0,\n            \"end\": end_t0,\n            \"text\": \"\",\n        }\n        if word_timestamps:\n            current_sentence[\"words\"] = []\n\n        sentences = []\n        for segment in transcript_segments:\n            text, speaker = segment.text, segment.speaker\n            start_t, end_t = segment.start, segment.end\n\n            if speaker != previous_speaker:\n                sentences.append(Utterance(**current_sentence))\n                current_sentence = {\n                    \"speaker\": speaker,\n                    \"start\": start_t,\n                    \"end\": end_t,\n                    \"text\": \"\",\n                }\n                if word_timestamps:\n                    current_sentence[\"words\"] = []\n            else:\n                current_sentence[\"end\"] = end_t\n\n            current_sentence[\"text\"] += text + \" \"\n            previous_speaker = speaker\n            if word_timestamps:\n                current_sentence[\"words\"].extend(segment.words)\n\n        # Catch the last sentence\n        sentences.append(Utterance(**current_sentence))\n\n        return sentences\n\n    def reconstruct_multi_channel_utterances(\n        self,\n        transcript_words: List[Tuple[int, Word]],\n    ) -&gt; List[Utterance]:\n        \"\"\"\n        Reconstruct multi-channel utterances based on the speaker mapping.\n\n        Args:\n            transcript_words (List[Tuple[int, Word]]):\n                List of tuples containing the speaker and the word.\n\n        Returns:\n            List[Utterance]: List of sentences with speaker mapping.\n        \"\"\"\n        speaker_t0, word = transcript_words[0]\n        start_t0, end_t0 = word.start, word.end\n\n        previous_speaker = speaker_t0\n        current_sentence = {\n            \"speaker\": speaker_t0,\n            \"start\": start_t0,\n            \"end\": end_t0,\n            \"text\": \"\",\n            \"words\": [],\n        }\n\n        sentences = []\n        for speaker, word in transcript_words:\n            start_t, end_t, text = word.start, word.end, word.word\n\n            if speaker != previous_speaker:\n                sentences.append(current_sentence)\n                current_sentence = {\n                    \"speaker\": speaker,\n                    \"start\": start_t,\n                    \"end\": end_t,\n                    \"text\": \"\",\n                }\n                current_sentence[\"words\"] = []\n            else:\n                current_sentence[\"end\"] = end_t\n\n            current_sentence[\"text\"] += text\n            previous_speaker = speaker\n            current_sentence[\"words\"].append(word)\n\n        # Catch the last sentence\n        sentences.append(current_sentence)\n\n        for sentence in sentences:\n            sentence[\"text\"] = sentence[\"text\"].strip()\n\n        return [Utterance(**sentence) for sentence in sentences]\n\n    def final_processing_before_returning(\n        self,\n        utterances: List[Utterance],\n        offset_start: Union[float, None],\n        timestamps_format: Timestamps,\n        word_timestamps: bool,\n    ) -&gt; List[Utterance]:\n        \"\"\"\n        Do final processing before returning the utterances to the API.\n\n        Args:\n            utterances (List[Utterance]):\n                List of utterances.\n            offset_start (Union[float, None]):\n                Offset start.\n            timestamps_format (Timestamps):\n                Timestamps format. Can be `s`, `ms`, or `hms`.\n            word_timestamps (bool):\n                Whether to include word timestamps.\n\n        Returns:\n            List[Utterance]:\n                List of utterances after final processing.\n        \"\"\"\n        if offset_start is not None:\n            offset_start = float(offset_start)\n        else:\n            offset_start = 0.0\n\n        final_utterances = []\n        for utterance in utterances:\n            # Check if the utterance is not empty\n            if utterance.text.strip():\n                utterance.text = format_punct(utterance.text)\n                utterance.start = convert_timestamp(\n                    (utterance.start + offset_start), timestamps_format\n                )\n                utterance.end = convert_timestamp(\n                    (utterance.end + offset_start), timestamps_format\n                )\n                utterance.words = utterance.words if word_timestamps else None\n\n                final_utterances.append(utterance)\n\n        return final_utterances\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the PostProcessingService.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the PostProcessingService.\"\"\"\n    self.sample_rate = 16000\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.final_processing_before_returning","title":"<code>final_processing_before_returning(utterances, offset_start, timestamps_format, word_timestamps)</code>","text":"<p>Do final processing before returning the utterances to the API.</p> <p>Parameters:</p> Name Type Description Default <code>utterances</code> <code>List[Utterance]</code> <p>List of utterances.</p> required <code>offset_start</code> <code>Union[float, None]</code> <p>Offset start.</p> required <code>timestamps_format</code> <code>Timestamps</code> <p>Timestamps format. Can be <code>s</code>, <code>ms</code>, or <code>hms</code>.</p> required <code>word_timestamps</code> <code>bool</code> <p>Whether to include word timestamps.</p> required <p>Returns:</p> Type Description <code>List[Utterance]</code> <p>List[Utterance]: List of utterances after final processing.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def final_processing_before_returning(\n    self,\n    utterances: List[Utterance],\n    offset_start: Union[float, None],\n    timestamps_format: Timestamps,\n    word_timestamps: bool,\n) -&gt; List[Utterance]:\n    \"\"\"\n    Do final processing before returning the utterances to the API.\n\n    Args:\n        utterances (List[Utterance]):\n            List of utterances.\n        offset_start (Union[float, None]):\n            Offset start.\n        timestamps_format (Timestamps):\n            Timestamps format. Can be `s`, `ms`, or `hms`.\n        word_timestamps (bool):\n            Whether to include word timestamps.\n\n    Returns:\n        List[Utterance]:\n            List of utterances after final processing.\n    \"\"\"\n    if offset_start is not None:\n        offset_start = float(offset_start)\n    else:\n        offset_start = 0.0\n\n    final_utterances = []\n    for utterance in utterances:\n        # Check if the utterance is not empty\n        if utterance.text.strip():\n            utterance.text = format_punct(utterance.text)\n            utterance.start = convert_timestamp(\n                (utterance.start + offset_start), timestamps_format\n            )\n            utterance.end = convert_timestamp(\n                (utterance.end + offset_start), timestamps_format\n            )\n            utterance.words = utterance.words if word_timestamps else None\n\n            final_utterances.append(utterance)\n\n    return final_utterances\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.multi_channel_speaker_mapping","title":"<code>multi_channel_speaker_mapping(multi_channel_segments)</code>","text":"<p>Run the multi-channel post-processing functions on the inputs by merging the segments based on the timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>multi_channel_segments</code> <code>List[MultiChannelTranscriptionOutput]</code> <p>List of segments from multi speakers.</p> required <p>Returns:</p> Name Type Description <code>TranscriptionOutput</code> <code>TranscriptionOutput</code> <p>List of sentences with speaker mapping.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def multi_channel_speaker_mapping(\n    self, multi_channel_segments: List[MultiChannelTranscriptionOutput]\n) -&gt; TranscriptionOutput:\n    \"\"\"\n    Run the multi-channel post-processing functions on the inputs by merging the segments based on the timestamps.\n\n    Args:\n        multi_channel_segments (List[MultiChannelTranscriptionOutput]):\n            List of segments from multi speakers.\n\n    Returns:\n        TranscriptionOutput: List of sentences with speaker mapping.\n    \"\"\"\n    words_with_speaker_mapping = [\n        (segment.speaker, word)\n        for output in multi_channel_segments\n        for segment in output.segments\n        for word in segment.words\n    ]\n    words_with_speaker_mapping.sort(key=lambda x: x[1].start)\n\n    utterances: List[Utterance] = self.reconstruct_multi_channel_utterances(\n        words_with_speaker_mapping\n    )\n\n    return utterances\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.reconstruct_multi_channel_utterances","title":"<code>reconstruct_multi_channel_utterances(transcript_words)</code>","text":"<p>Reconstruct multi-channel utterances based on the speaker mapping.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_words</code> <code>List[Tuple[int, Word]]</code> <p>List of tuples containing the speaker and the word.</p> required <p>Returns:</p> Type Description <code>List[Utterance]</code> <p>List[Utterance]: List of sentences with speaker mapping.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def reconstruct_multi_channel_utterances(\n    self,\n    transcript_words: List[Tuple[int, Word]],\n) -&gt; List[Utterance]:\n    \"\"\"\n    Reconstruct multi-channel utterances based on the speaker mapping.\n\n    Args:\n        transcript_words (List[Tuple[int, Word]]):\n            List of tuples containing the speaker and the word.\n\n    Returns:\n        List[Utterance]: List of sentences with speaker mapping.\n    \"\"\"\n    speaker_t0, word = transcript_words[0]\n    start_t0, end_t0 = word.start, word.end\n\n    previous_speaker = speaker_t0\n    current_sentence = {\n        \"speaker\": speaker_t0,\n        \"start\": start_t0,\n        \"end\": end_t0,\n        \"text\": \"\",\n        \"words\": [],\n    }\n\n    sentences = []\n    for speaker, word in transcript_words:\n        start_t, end_t, text = word.start, word.end, word.word\n\n        if speaker != previous_speaker:\n            sentences.append(current_sentence)\n            current_sentence = {\n                \"speaker\": speaker,\n                \"start\": start_t,\n                \"end\": end_t,\n                \"text\": \"\",\n            }\n            current_sentence[\"words\"] = []\n        else:\n            current_sentence[\"end\"] = end_t\n\n        current_sentence[\"text\"] += text\n        previous_speaker = speaker\n        current_sentence[\"words\"].append(word)\n\n    # Catch the last sentence\n    sentences.append(current_sentence)\n\n    for sentence in sentences:\n        sentence[\"text\"] = sentence[\"text\"].strip()\n\n    return [Utterance(**sentence) for sentence in sentences]\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.reconstruct_utterances","title":"<code>reconstruct_utterances(transcript_segments, word_timestamps)</code>","text":"<p>Reconstruct the utterances based on the speaker mapping.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_words</code> <code>List[Utterance]</code> <p>List of transcript segments.</p> required <code>word_timestamps</code> <code>bool</code> <p>Whether to include word timestamps.</p> required <p>Returns:</p> Type Description <code>List[Utterance]</code> <p>List[Utterance]: List of sentences with speaker mapping.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def reconstruct_utterances(\n    self,\n    transcript_segments: List[Utterance],\n    word_timestamps: bool,\n) -&gt; List[Utterance]:\n    \"\"\"\n    Reconstruct the utterances based on the speaker mapping.\n\n    Args:\n        transcript_words (List[Utterance]):\n            List of transcript segments.\n        word_timestamps (bool):\n            Whether to include word timestamps.\n\n    Returns:\n        List[Utterance]:\n            List of sentences with speaker mapping.\n    \"\"\"\n    start_t0, end_t0, speaker_t0 = (\n        transcript_segments[0].start,\n        transcript_segments[0].end,\n        transcript_segments[0].speaker,\n    )\n\n    previous_speaker = speaker_t0\n    current_sentence = {\n        \"speaker\": speaker_t0,\n        \"start\": start_t0,\n        \"end\": end_t0,\n        \"text\": \"\",\n    }\n    if word_timestamps:\n        current_sentence[\"words\"] = []\n\n    sentences = []\n    for segment in transcript_segments:\n        text, speaker = segment.text, segment.speaker\n        start_t, end_t = segment.start, segment.end\n\n        if speaker != previous_speaker:\n            sentences.append(Utterance(**current_sentence))\n            current_sentence = {\n                \"speaker\": speaker,\n                \"start\": start_t,\n                \"end\": end_t,\n                \"text\": \"\",\n            }\n            if word_timestamps:\n                current_sentence[\"words\"] = []\n        else:\n            current_sentence[\"end\"] = end_t\n\n        current_sentence[\"text\"] += text + \" \"\n        previous_speaker = speaker\n        if word_timestamps:\n            current_sentence[\"words\"].extend(segment.words)\n\n    # Catch the last sentence\n    sentences.append(Utterance(**current_sentence))\n\n    return sentences\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.segments_speaker_mapping","title":"<code>segments_speaker_mapping(transcript_segments, speaker_timestamps)</code>","text":"<p>Function to map transcription and diarization results.</p> <p>Map each segment to its corresponding speaker based on the speaker timestamps and reconstruct the utterances when the speaker changes in the middle of a segment.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_segments</code> <code>List[dict]</code> <p>List of transcript segments.</p> required <code>speaker_timestamps</code> <code>List[dict]</code> <p>List of speaker timestamps.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of sentences with speaker mapping.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def segments_speaker_mapping(\n    self,\n    transcript_segments: List[Utterance],\n    speaker_timestamps: List[DiarizationSegment],\n) -&gt; List[dict]:\n    \"\"\"Function to map transcription and diarization results.\n\n    Map each segment to its corresponding speaker based on the speaker timestamps and reconstruct the utterances\n    when the speaker changes in the middle of a segment.\n\n    Args:\n        transcript_segments (List[dict]): List of transcript segments.\n        speaker_timestamps (List[dict]): List of speaker timestamps.\n\n    Returns:\n        List[dict]: List of sentences with speaker mapping.\n    \"\"\"\n\n    def _assign_speaker(\n        mapping: list,\n        seg_index: int,\n        split: bool,\n        current_speaker: str,\n        current_split_len: int,\n    ):\n        \"\"\"Assign speaker to the segment.\"\"\"\n        if split and len(mapping) &gt; 1:\n            last_split_len = len(mapping[seg_index - 1].text)\n            if last_split_len &gt; current_split_len:\n                current_speaker = mapping[seg_index - 1].speaker\n            elif last_split_len &lt; current_split_len:\n                mapping[seg_index - 1].speaker = current_speaker\n        return current_speaker\n\n    threshold = 0.3\n    turn_idx = 0\n    was_split = False\n    _, end, speaker = speaker_timestamps[turn_idx]\n\n    segment_index = 0\n    segment_speaker_mapping = []\n    while segment_index &lt; len(transcript_segments):\n        segment: Utterance = transcript_segments[segment_index]\n        segment_start, segment_end, segment_text = (\n            segment.start,\n            segment.end,\n            segment.text,\n        )\n        while (\n            segment_start &gt; float(end)\n            or abs(segment_start - float(end)) &lt; threshold\n        ):\n            turn_idx += 1\n            turn_idx = min(turn_idx, len(speaker_timestamps) - 1)\n            _, end, speaker = speaker_timestamps[turn_idx]\n            if turn_idx == len(speaker_timestamps) - 1:\n                end = segment_end\n                break\n\n        if segment_end &gt; float(end) and abs(segment_end - float(end)) &gt; threshold:\n            words = segment.words\n            word_index = next(\n                (\n                    i\n                    for i, word in enumerate(words)\n                    if word.start &gt; float(end)\n                    or abs(word.start - float(end)) &lt; threshold\n                ),\n                None,\n            )\n\n            if word_index is not None:\n                _split_segment = segment_text.split()\n\n                if word_index &gt; 0:\n                    text = \" \".join(_split_segment[:word_index])\n                    speaker = _assign_speaker(\n                        segment_speaker_mapping,\n                        segment_index,\n                        was_split,\n                        speaker,\n                        len(text),\n                    )\n\n                    _segment_to_add = Utterance(\n                        start=words[0].start,\n                        end=words[word_index - 1].end,\n                        text=text,\n                        speaker=speaker,\n                        words=words[:word_index],\n                    )\n                else:\n                    text = _split_segment[0]\n                    speaker = _assign_speaker(\n                        segment_speaker_mapping,\n                        segment_index,\n                        was_split,\n                        speaker,\n                        len(text),\n                    )\n\n                    _segment_to_add = Utterance(\n                        start=words[0].start,\n                        end=words[0].end,\n                        text=_split_segment[0],\n                        speaker=speaker,\n                        words=words[:1],\n                    )\n                segment_speaker_mapping.append(_segment_to_add)\n                transcript_segments.insert(\n                    segment_index + 1,\n                    Utterance(\n                        start=words[word_index].start,\n                        end=segment_end,\n                        text=\" \".join(_split_segment[word_index:]),\n                        words=words[word_index:],\n                    ),\n                )\n                was_split = True\n            else:\n                speaker = _assign_speaker(\n                    segment_speaker_mapping,\n                    segment_index,\n                    was_split,\n                    speaker,\n                    len(segment_text),\n                )\n                was_split = False\n\n                segment_speaker_mapping.append(\n                    Utterance(\n                        start=segment_start,\n                        end=segment_end,\n                        text=segment_text,\n                        speaker=speaker,\n                        words=words,\n                    )\n                )\n        else:\n            speaker = _assign_speaker(\n                segment_speaker_mapping,\n                segment_index,\n                was_split,\n                speaker,\n                len(segment_text),\n            )\n            was_split = False\n\n            segment_speaker_mapping.append(\n                Utterance(\n                    start=segment_start,\n                    end=segment_end,\n                    text=segment_text,\n                    speaker=speaker,\n                    words=segment.words,\n                )\n            )\n        segment_index += 1\n\n    return segment_speaker_mapping\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.post_processing_service.PostProcessingService.single_channel_speaker_mapping","title":"<code>single_channel_speaker_mapping(transcript_segments, speaker_timestamps, word_timestamps)</code>","text":"<p>Run the post-processing functions on the inputs.</p> <p>The postprocessing pipeline is as follows: 1. Map each transcript segment to its corresponding speaker. 2. Group utterances of the same speaker together.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_segments</code> <code>List[Utterance]</code> <p>List of transcript utterances.</p> required <code>speaker_timestamps</code> <code>DiarizationOutput</code> <p>List of speaker timestamps.</p> required <code>word_timestamps</code> <code>bool</code> <p>Whether to include word timestamps.</p> required <p>Returns:</p> Type Description <code>List[Utterance]</code> <p>List[Utterance]: List of utterances with speaker mapping.</p> Source code in <code>src/wordcab_transcribe/services/post_processing_service.py</code> <pre><code>def single_channel_speaker_mapping(\n    self,\n    transcript_segments: List[Utterance],\n    speaker_timestamps: DiarizationOutput,\n    word_timestamps: bool,\n) -&gt; List[Utterance]:\n    \"\"\"Run the post-processing functions on the inputs.\n\n    The postprocessing pipeline is as follows:\n    1. Map each transcript segment to its corresponding speaker.\n    2. Group utterances of the same speaker together.\n\n    Args:\n        transcript_segments (List[Utterance]):\n            List of transcript utterances.\n        speaker_timestamps (DiarizationOutput):\n            List of speaker timestamps.\n        word_timestamps (bool):\n            Whether to include word timestamps.\n\n    Returns:\n        List[Utterance]:\n            List of utterances with speaker mapping.\n    \"\"\"\n    segments_with_speaker_mapping = self.segments_speaker_mapping(\n        transcript_segments,\n        speaker_timestamps.segments,\n    )\n\n    utterances = self.reconstruct_utterances(\n        segments_with_speaker_mapping, word_timestamps\n    )\n\n    return utterances\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.FasterWhisperModel","title":"<code>FasterWhisperModel</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Faster Whisper Model.</p> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>class FasterWhisperModel(NamedTuple):\n    \"\"\"Faster Whisper Model.\"\"\"\n\n    model: WhisperModel\n    lang: str\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.TranscribeService","title":"<code>TranscribeService</code>","text":"<p>Transcribe Service for audio files.</p> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>class TranscribeService:\n    \"\"\"Transcribe Service for audio files.\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        compute_type: str,\n        device: str,\n        device_index: Union[int, List[int]],\n        extra_languages: Union[List[str], None] = None,\n        extra_languages_model_paths: Union[List[str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Transcribe Service.\n\n        This service uses the WhisperModel from faster-whisper to transcribe audio files.\n\n        Args:\n            model_path (str):\n                Path to the model checkpoint. This can be a local path or a URL.\n            compute_type (str):\n                Compute type to use for inference. Can be \"int8\", \"int8_float16\", \"int16\" or \"float_16\".\n            device (str):\n                Device to use for inference. Can be \"cpu\" or \"cuda\".\n            device_index (Union[int, List[int]]):\n                Index of the device to use for inference.\n            extra_languages (Union[List[str], None]):\n                List of extra languages to transcribe. Defaults to None.\n            extra_languages_model_paths (Union[List[str], None]):\n                List of paths to the extra language models. Defaults to None.\n        \"\"\"\n        self.device = device\n        self.compute_type = compute_type\n        self.model_path = model_path\n\n        self.model = WhisperModel(\n            self.model_path,\n            device=self.device,\n            device_index=device_index,\n            compute_type=self.compute_type,\n        )\n\n        self.extra_lang = extra_languages\n        self.extra_lang_models = extra_languages_model_paths\n\n    def __call__(\n        self,\n        audio: Union[\n            str,\n            torch.Tensor,\n            TensorShare,\n            List[str],\n            List[torch.Tensor],\n            List[TensorShare],\n        ],\n        source_lang: str,\n        model_index: int,\n        suppress_blank: bool = False,\n        vocab: Union[List[str], None] = None,\n        word_timestamps: bool = True,\n        internal_vad: bool = False,\n        repetition_penalty: float = 1.0,\n        compression_ratio_threshold: float = 2.4,\n        log_prob_threshold: float = -1.0,\n        no_speech_threshold: float = 0.6,\n        condition_on_previous_text: bool = True,\n    ) -&gt; Union[TranscriptionOutput, List[TranscriptionOutput]]:\n        \"\"\"\n        Run inference with the transcribe model.\n\n        Args:\n            audio (Union[str, torch.Tensor, TensorShare, List[str], List[torch.Tensor], List[TensorShare]]):\n                Audio file path or audio tensor. If a tuple is passed, the task is assumed\n                to be a multi_channel task and the list of audio files or tensors is passed.\n            source_lang (str):\n                Language of the audio file.\n            model_index (int):\n                Index of the model to use.\n            suppress_blank (bool):\n                Whether to suppress blank at the beginning of the sampling.\n            vocab (Union[List[str], None]):\n                Vocabulary to use during generation if not None. Defaults to None.\n            word_timestamps (bool):\n                Whether to return word timestamps.\n            internal_vad (bool):\n                Whether to use faster-whisper's VAD or not.\n            repetition_penalty (float):\n                Repetition penalty to use during generation beamed search.\n            compression_ratio_threshold (float):\n                If the gzip compression ratio is above this value, treat as failed.\n            log_prob_threshold (float):\n                If the average log probability over sampled tokens is below this value, treat as failed.\n            no_speech_threshold (float):\n                If the no_speech probability is higher than this value AND the average log probability\n                over sampled tokens is below `log_prob_threshold`, consider the segment as silent.\n            condition_on_previous_text (bool):\n                If True, the previous output of the model is provided as a prompt for the next window;\n                disabling may make the text inconsistent across windows, but the model becomes less prone\n                to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n\n        Returns:\n            Union[TranscriptionOutput, List[TranscriptionOutput]]:\n                Transcription output. If the task is a multi_channel task, a list of TranscriptionOutput is returned.\n        \"\"\"\n        # Extra language models are disabled until we can handle an index mapping\n        # if (\n        #     source_lang in self.extra_lang\n        #     and self.models[model_index].lang != source_lang\n        # ):\n        #     logger.debug(f\"Loading model for language {source_lang} on GPU {model_index}.\")\n        #     self.models[model_index] = FasterWhisperModel(\n        #         model=WhisperModel(\n        #             self.extra_lang_models[source_lang],\n        #             device=self.device,\n        #             device_index=model_index,\n        #             compute_type=self.compute_type,\n        #         ),\n        #         lang=source_lang,\n        #     )\n        #     self.loaded_model_lang = source_lang\n\n        # elif source_lang not in self.extra_lang and self.models[model_index].lang != \"multi\":\n        #     logger.debug(f\"Re-loading multi-language model on GPU {model_index}.\")\n        #     self.models[model_index] = FasterWhisperModel(\n        #         model=WhisperModel(\n        #             self.model_path,\n        #             device=self.device,\n        #             device_index=model_index,\n        #             compute_type=self.compute_type,\n        #         ),\n        #         lang=source_lang,\n        #     )\n\n        if (\n            vocab is not None\n            and isinstance(vocab, list)\n            and len(vocab) &gt; 0\n            and vocab[0].strip()\n        ):\n            words = \", \".join(vocab)\n            prompt = f\"Vocab: {words.strip()}\"\n        else:\n            prompt = None\n\n        if not isinstance(audio, list):\n            if isinstance(audio, torch.Tensor):\n                audio = audio.numpy()\n            elif isinstance(audio, TensorShare):\n                ts = audio.to_tensors(backend=Backend.NUMPY)\n                audio = ts[\"audio\"]\n\n            segments, _ = self.model.transcribe(\n                audio,\n                language=source_lang,\n                initial_prompt=prompt,\n                repetition_penalty=repetition_penalty,\n                compression_ratio_threshold=compression_ratio_threshold,\n                log_prob_threshold=log_prob_threshold,\n                no_speech_threshold=no_speech_threshold,\n                condition_on_previous_text=condition_on_previous_text,\n                suppress_blank=suppress_blank,\n                word_timestamps=word_timestamps,\n                vad_filter=internal_vad,\n                vad_parameters={\n                    \"threshold\": 0.5,\n                    \"min_speech_duration_ms\": 250,\n                    \"min_silence_duration_ms\": 100,\n                    \"speech_pad_ms\": 30,\n                    \"window_size_samples\": 512,\n                },\n            )\n\n            segments = list(segments)\n            if not segments:\n                logger.warning(\n                    \"Empty transcription result. Trying with vad_filter=True.\"\n                )\n                segments, _ = self.model.transcribe(\n                    audio,\n                    language=source_lang,\n                    initial_prompt=prompt,\n                    repetition_penalty=repetition_penalty,\n                    compression_ratio_threshold=compression_ratio_threshold,\n                    log_prob_threshold=log_prob_threshold,\n                    no_speech_threshold=no_speech_threshold,\n                    condition_on_previous_text=condition_on_previous_text,\n                    suppress_blank=False,\n                    word_timestamps=True,\n                    vad_filter=False if internal_vad else True,\n                )\n\n            _outputs = [segment._asdict() for segment in segments]\n            outputs = TranscriptionOutput(segments=_outputs)\n\n        else:\n            outputs = []\n            for audio_index, audio_file in enumerate(audio):\n                outputs.append(\n                    self.multi_channel(\n                        audio_file,\n                        source_lang=source_lang,\n                        speaker_id=audio_index,\n                        suppress_blank=suppress_blank,\n                        word_timestamps=word_timestamps,\n                        internal_vad=internal_vad,\n                        repetition_penalty=repetition_penalty,\n                        compression_ratio_threshold=compression_ratio_threshold,\n                        log_prob_threshold=log_prob_threshold,\n                        no_speech_threshold=no_speech_threshold,\n                        prompt=prompt,\n                    )\n                )\n\n        return outputs\n\n    async def async_live_transcribe(\n        self,\n        audio: torch.Tensor,\n        source_lang: str,\n        model_index: int,\n    ) -&gt; Iterable[dict]:\n        \"\"\"Async generator for live transcriptions.\n\n        This method wraps the live_transcribe method to make it async.\n\n        Args:\n            audio (torch.Tensor): Audio tensor.\n            source_lang (str): Language of the audio file.\n            model_index (int): Index of the model to use.\n\n        Yields:\n            Iterable[dict]: Iterable of transcribed segments.\n        \"\"\"\n        for result in self.live_transcribe(audio, source_lang, model_index):\n            yield result\n\n    def live_transcribe(\n        self,\n        audio: torch.Tensor,\n        source_lang: str,\n        model_index: int,\n    ) -&gt; Iterable[dict]:\n        \"\"\"\n        Transcribe audio from a WebSocket connection.\n\n        Args:\n            audio (torch.Tensor): Audio tensor.\n            source_lang (str): Language of the audio file.\n            model_index (int): Index of the model to use.\n\n        Yields:\n            Iterable[dict]: Iterable of transcribed segments.\n        \"\"\"\n        segments, _ = self.model.transcribe(\n            audio.numpy(),\n            language=source_lang,\n            suppress_blank=True,\n            word_timestamps=False,\n        )\n\n        for segment in segments:\n            yield segment._asdict()\n\n    def multi_channel(\n        self,\n        audio: Union[str, torch.Tensor, TensorShare],\n        source_lang: str,\n        speaker_id: int,\n        suppress_blank: bool = False,\n        word_timestamps: bool = True,\n        internal_vad: bool = True,\n        repetition_penalty: float = 1.0,\n        compression_ratio_threshold: float = 2.4,\n        log_prob_threshold: float = -1.0,\n        no_speech_threshold: float = 0.6,\n        condition_on_previous_text: bool = False,\n        prompt: Optional[str] = None,\n    ) -&gt; MultiChannelTranscriptionOutput:\n        \"\"\"\n        Transcribe an audio file using the faster-whisper original pipeline.\n\n        Args:\n            audio (Union[str, torch.Tensor, TensorShare]): Audio file path or loaded audio.\n            source_lang (str): Language of the audio file.\n            speaker_id (int): Speaker ID used in the diarization.\n            suppress_blank (bool):\n                Whether to suppress blank at the beginning of the sampling.\n            word_timestamps (bool):\n                Whether to return word timestamps.\n            internal_vad (bool):\n                Whether to use faster-whisper's VAD or not.\n            repetition_penalty (float):\n                Repetition penalty to use during generation beamed search.\n            compression_ratio_threshold (float):\n                If the gzip compression ratio is above this value, treat as failed.\n            log_prob_threshold (float):\n                If the average log probability over sampled tokens is below this value, treat as failed.\n            no_speech_threshold (float):\n                If the no_speech probability is higher than this value AND the average log probability\n                over sampled tokens is below `log_prob_threshold`, consider the segment as silent.\n            condition_on_previous_text (bool):\n                If True, the previous output of the model is provided as a prompt for the next window;\n                disabling may make the text inconsistent across windows, but the model becomes less prone\n                to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n            prompt (Optional[str]): Initial prompt to use for the generation.\n\n        Returns:\n            MultiChannelTranscriptionOutput: Multi-channel transcription segments in a list.\n        \"\"\"\n        if isinstance(audio, torch.Tensor):\n            _audio = audio.numpy()\n        elif isinstance(audio, TensorShare):\n            ts = audio.to_tensors(backend=Backend.NUMPY)\n            _audio = ts[\"audio\"]\n\n        final_segments = []\n\n        segments, _ = self.model.transcribe(\n            _audio,\n            language=source_lang,\n            initial_prompt=prompt,\n            repetition_penalty=repetition_penalty,\n            compression_ratio_threshold=compression_ratio_threshold,\n            log_prob_threshold=log_prob_threshold,\n            no_speech_threshold=no_speech_threshold,\n            condition_on_previous_text=condition_on_previous_text,\n            suppress_blank=suppress_blank,\n            word_timestamps=word_timestamps,\n            vad_filter=internal_vad,\n            vad_parameters={\n                \"threshold\": 0.5,\n                \"min_speech_duration_ms\": 250,\n                \"min_silence_duration_ms\": 100,\n                \"speech_pad_ms\": 30,\n                \"window_size_samples\": 512,\n            },\n        )\n\n        for segment in segments:\n            _segment = MultiChannelSegment(\n                start=segment.start,\n                end=segment.end,\n                text=segment.text,\n                words=[Word(**word._asdict()) for word in segment.words],\n                speaker=speaker_id,\n            )\n            final_segments.append(_segment)\n\n        return MultiChannelTranscriptionOutput(segments=final_segments)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.TranscribeService.__call__","title":"<code>__call__(audio, source_lang, model_index, suppress_blank=False, vocab=None, word_timestamps=True, internal_vad=False, repetition_penalty=1.0, compression_ratio_threshold=2.4, log_prob_threshold=-1.0, no_speech_threshold=0.6, condition_on_previous_text=True)</code>","text":"<p>Run inference with the transcribe model.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Union[str, Tensor, TensorShare, List[str], List[Tensor], List[TensorShare]]</code> <p>Audio file path or audio tensor. If a tuple is passed, the task is assumed to be a multi_channel task and the list of audio files or tensors is passed.</p> required <code>source_lang</code> <code>str</code> <p>Language of the audio file.</p> required <code>model_index</code> <code>int</code> <p>Index of the model to use.</p> required <code>suppress_blank</code> <code>bool</code> <p>Whether to suppress blank at the beginning of the sampling.</p> <code>False</code> <code>vocab</code> <code>Union[List[str], None]</code> <p>Vocabulary to use during generation if not None. Defaults to None.</p> <code>None</code> <code>word_timestamps</code> <code>bool</code> <p>Whether to return word timestamps.</p> <code>True</code> <code>internal_vad</code> <code>bool</code> <p>Whether to use faster-whisper's VAD or not.</p> <code>False</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty to use during generation beamed search.</p> <code>1.0</code> <code>compression_ratio_threshold</code> <code>float</code> <p>If the gzip compression ratio is above this value, treat as failed.</p> <code>2.4</code> <code>log_prob_threshold</code> <code>float</code> <p>If the average log probability over sampled tokens is below this value, treat as failed.</p> <code>-1.0</code> <code>no_speech_threshold</code> <code>float</code> <p>If the no_speech probability is higher than this value AND the average log probability over sampled tokens is below <code>log_prob_threshold</code>, consider the segment as silent.</p> <code>0.6</code> <code>condition_on_previous_text</code> <code>bool</code> <p>If True, the previous output of the model is provided as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[TranscriptionOutput, List[TranscriptionOutput]]</code> <p>Union[TranscriptionOutput, List[TranscriptionOutput]]: Transcription output. If the task is a multi_channel task, a list of TranscriptionOutput is returned.</p> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>def __call__(\n    self,\n    audio: Union[\n        str,\n        torch.Tensor,\n        TensorShare,\n        List[str],\n        List[torch.Tensor],\n        List[TensorShare],\n    ],\n    source_lang: str,\n    model_index: int,\n    suppress_blank: bool = False,\n    vocab: Union[List[str], None] = None,\n    word_timestamps: bool = True,\n    internal_vad: bool = False,\n    repetition_penalty: float = 1.0,\n    compression_ratio_threshold: float = 2.4,\n    log_prob_threshold: float = -1.0,\n    no_speech_threshold: float = 0.6,\n    condition_on_previous_text: bool = True,\n) -&gt; Union[TranscriptionOutput, List[TranscriptionOutput]]:\n    \"\"\"\n    Run inference with the transcribe model.\n\n    Args:\n        audio (Union[str, torch.Tensor, TensorShare, List[str], List[torch.Tensor], List[TensorShare]]):\n            Audio file path or audio tensor. If a tuple is passed, the task is assumed\n            to be a multi_channel task and the list of audio files or tensors is passed.\n        source_lang (str):\n            Language of the audio file.\n        model_index (int):\n            Index of the model to use.\n        suppress_blank (bool):\n            Whether to suppress blank at the beginning of the sampling.\n        vocab (Union[List[str], None]):\n            Vocabulary to use during generation if not None. Defaults to None.\n        word_timestamps (bool):\n            Whether to return word timestamps.\n        internal_vad (bool):\n            Whether to use faster-whisper's VAD or not.\n        repetition_penalty (float):\n            Repetition penalty to use during generation beamed search.\n        compression_ratio_threshold (float):\n            If the gzip compression ratio is above this value, treat as failed.\n        log_prob_threshold (float):\n            If the average log probability over sampled tokens is below this value, treat as failed.\n        no_speech_threshold (float):\n            If the no_speech probability is higher than this value AND the average log probability\n            over sampled tokens is below `log_prob_threshold`, consider the segment as silent.\n        condition_on_previous_text (bool):\n            If True, the previous output of the model is provided as a prompt for the next window;\n            disabling may make the text inconsistent across windows, but the model becomes less prone\n            to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n\n    Returns:\n        Union[TranscriptionOutput, List[TranscriptionOutput]]:\n            Transcription output. If the task is a multi_channel task, a list of TranscriptionOutput is returned.\n    \"\"\"\n    # Extra language models are disabled until we can handle an index mapping\n    # if (\n    #     source_lang in self.extra_lang\n    #     and self.models[model_index].lang != source_lang\n    # ):\n    #     logger.debug(f\"Loading model for language {source_lang} on GPU {model_index}.\")\n    #     self.models[model_index] = FasterWhisperModel(\n    #         model=WhisperModel(\n    #             self.extra_lang_models[source_lang],\n    #             device=self.device,\n    #             device_index=model_index,\n    #             compute_type=self.compute_type,\n    #         ),\n    #         lang=source_lang,\n    #     )\n    #     self.loaded_model_lang = source_lang\n\n    # elif source_lang not in self.extra_lang and self.models[model_index].lang != \"multi\":\n    #     logger.debug(f\"Re-loading multi-language model on GPU {model_index}.\")\n    #     self.models[model_index] = FasterWhisperModel(\n    #         model=WhisperModel(\n    #             self.model_path,\n    #             device=self.device,\n    #             device_index=model_index,\n    #             compute_type=self.compute_type,\n    #         ),\n    #         lang=source_lang,\n    #     )\n\n    if (\n        vocab is not None\n        and isinstance(vocab, list)\n        and len(vocab) &gt; 0\n        and vocab[0].strip()\n    ):\n        words = \", \".join(vocab)\n        prompt = f\"Vocab: {words.strip()}\"\n    else:\n        prompt = None\n\n    if not isinstance(audio, list):\n        if isinstance(audio, torch.Tensor):\n            audio = audio.numpy()\n        elif isinstance(audio, TensorShare):\n            ts = audio.to_tensors(backend=Backend.NUMPY)\n            audio = ts[\"audio\"]\n\n        segments, _ = self.model.transcribe(\n            audio,\n            language=source_lang,\n            initial_prompt=prompt,\n            repetition_penalty=repetition_penalty,\n            compression_ratio_threshold=compression_ratio_threshold,\n            log_prob_threshold=log_prob_threshold,\n            no_speech_threshold=no_speech_threshold,\n            condition_on_previous_text=condition_on_previous_text,\n            suppress_blank=suppress_blank,\n            word_timestamps=word_timestamps,\n            vad_filter=internal_vad,\n            vad_parameters={\n                \"threshold\": 0.5,\n                \"min_speech_duration_ms\": 250,\n                \"min_silence_duration_ms\": 100,\n                \"speech_pad_ms\": 30,\n                \"window_size_samples\": 512,\n            },\n        )\n\n        segments = list(segments)\n        if not segments:\n            logger.warning(\n                \"Empty transcription result. Trying with vad_filter=True.\"\n            )\n            segments, _ = self.model.transcribe(\n                audio,\n                language=source_lang,\n                initial_prompt=prompt,\n                repetition_penalty=repetition_penalty,\n                compression_ratio_threshold=compression_ratio_threshold,\n                log_prob_threshold=log_prob_threshold,\n                no_speech_threshold=no_speech_threshold,\n                condition_on_previous_text=condition_on_previous_text,\n                suppress_blank=False,\n                word_timestamps=True,\n                vad_filter=False if internal_vad else True,\n            )\n\n        _outputs = [segment._asdict() for segment in segments]\n        outputs = TranscriptionOutput(segments=_outputs)\n\n    else:\n        outputs = []\n        for audio_index, audio_file in enumerate(audio):\n            outputs.append(\n                self.multi_channel(\n                    audio_file,\n                    source_lang=source_lang,\n                    speaker_id=audio_index,\n                    suppress_blank=suppress_blank,\n                    word_timestamps=word_timestamps,\n                    internal_vad=internal_vad,\n                    repetition_penalty=repetition_penalty,\n                    compression_ratio_threshold=compression_ratio_threshold,\n                    log_prob_threshold=log_prob_threshold,\n                    no_speech_threshold=no_speech_threshold,\n                    prompt=prompt,\n                )\n            )\n\n    return outputs\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.TranscribeService.__init__","title":"<code>__init__(model_path, compute_type, device, device_index, extra_languages=None, extra_languages_model_paths=None)</code>","text":"<p>Initialize the Transcribe Service.</p> <p>This service uses the WhisperModel from faster-whisper to transcribe audio files.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model checkpoint. This can be a local path or a URL.</p> required <code>compute_type</code> <code>str</code> <p>Compute type to use for inference. Can be \"int8\", \"int8_float16\", \"int16\" or \"float_16\".</p> required <code>device</code> <code>str</code> <p>Device to use for inference. Can be \"cpu\" or \"cuda\".</p> required <code>device_index</code> <code>Union[int, List[int]]</code> <p>Index of the device to use for inference.</p> required <code>extra_languages</code> <code>Union[List[str], None]</code> <p>List of extra languages to transcribe. Defaults to None.</p> <code>None</code> <code>extra_languages_model_paths</code> <code>Union[List[str], None]</code> <p>List of paths to the extra language models. Defaults to None.</p> <code>None</code> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>def __init__(\n    self,\n    model_path: str,\n    compute_type: str,\n    device: str,\n    device_index: Union[int, List[int]],\n    extra_languages: Union[List[str], None] = None,\n    extra_languages_model_paths: Union[List[str], None] = None,\n) -&gt; None:\n    \"\"\"Initialize the Transcribe Service.\n\n    This service uses the WhisperModel from faster-whisper to transcribe audio files.\n\n    Args:\n        model_path (str):\n            Path to the model checkpoint. This can be a local path or a URL.\n        compute_type (str):\n            Compute type to use for inference. Can be \"int8\", \"int8_float16\", \"int16\" or \"float_16\".\n        device (str):\n            Device to use for inference. Can be \"cpu\" or \"cuda\".\n        device_index (Union[int, List[int]]):\n            Index of the device to use for inference.\n        extra_languages (Union[List[str], None]):\n            List of extra languages to transcribe. Defaults to None.\n        extra_languages_model_paths (Union[List[str], None]):\n            List of paths to the extra language models. Defaults to None.\n    \"\"\"\n    self.device = device\n    self.compute_type = compute_type\n    self.model_path = model_path\n\n    self.model = WhisperModel(\n        self.model_path,\n        device=self.device,\n        device_index=device_index,\n        compute_type=self.compute_type,\n    )\n\n    self.extra_lang = extra_languages\n    self.extra_lang_models = extra_languages_model_paths\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.TranscribeService.async_live_transcribe","title":"<code>async_live_transcribe(audio, source_lang, model_index)</code>  <code>async</code>","text":"<p>Async generator for live transcriptions.</p> <p>This method wraps the live_transcribe method to make it async.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Tensor</code> <p>Audio tensor.</p> required <code>source_lang</code> <code>str</code> <p>Language of the audio file.</p> required <code>model_index</code> <code>int</code> <p>Index of the model to use.</p> required <p>Yields:</p> Type Description <code>Iterable[dict]</code> <p>Iterable[dict]: Iterable of transcribed segments.</p> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>async def async_live_transcribe(\n    self,\n    audio: torch.Tensor,\n    source_lang: str,\n    model_index: int,\n) -&gt; Iterable[dict]:\n    \"\"\"Async generator for live transcriptions.\n\n    This method wraps the live_transcribe method to make it async.\n\n    Args:\n        audio (torch.Tensor): Audio tensor.\n        source_lang (str): Language of the audio file.\n        model_index (int): Index of the model to use.\n\n    Yields:\n        Iterable[dict]: Iterable of transcribed segments.\n    \"\"\"\n    for result in self.live_transcribe(audio, source_lang, model_index):\n        yield result\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.TranscribeService.live_transcribe","title":"<code>live_transcribe(audio, source_lang, model_index)</code>","text":"<p>Transcribe audio from a WebSocket connection.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Tensor</code> <p>Audio tensor.</p> required <code>source_lang</code> <code>str</code> <p>Language of the audio file.</p> required <code>model_index</code> <code>int</code> <p>Index of the model to use.</p> required <p>Yields:</p> Type Description <code>Iterable[dict]</code> <p>Iterable[dict]: Iterable of transcribed segments.</p> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>def live_transcribe(\n    self,\n    audio: torch.Tensor,\n    source_lang: str,\n    model_index: int,\n) -&gt; Iterable[dict]:\n    \"\"\"\n    Transcribe audio from a WebSocket connection.\n\n    Args:\n        audio (torch.Tensor): Audio tensor.\n        source_lang (str): Language of the audio file.\n        model_index (int): Index of the model to use.\n\n    Yields:\n        Iterable[dict]: Iterable of transcribed segments.\n    \"\"\"\n    segments, _ = self.model.transcribe(\n        audio.numpy(),\n        language=source_lang,\n        suppress_blank=True,\n        word_timestamps=False,\n    )\n\n    for segment in segments:\n        yield segment._asdict()\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.transcribe_service.TranscribeService.multi_channel","title":"<code>multi_channel(audio, source_lang, speaker_id, suppress_blank=False, word_timestamps=True, internal_vad=True, repetition_penalty=1.0, compression_ratio_threshold=2.4, log_prob_threshold=-1.0, no_speech_threshold=0.6, condition_on_previous_text=False, prompt=None)</code>","text":"<p>Transcribe an audio file using the faster-whisper original pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Union[str, Tensor, TensorShare]</code> <p>Audio file path or loaded audio.</p> required <code>source_lang</code> <code>str</code> <p>Language of the audio file.</p> required <code>speaker_id</code> <code>int</code> <p>Speaker ID used in the diarization.</p> required <code>suppress_blank</code> <code>bool</code> <p>Whether to suppress blank at the beginning of the sampling.</p> <code>False</code> <code>word_timestamps</code> <code>bool</code> <p>Whether to return word timestamps.</p> <code>True</code> <code>internal_vad</code> <code>bool</code> <p>Whether to use faster-whisper's VAD or not.</p> <code>True</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty to use during generation beamed search.</p> <code>1.0</code> <code>compression_ratio_threshold</code> <code>float</code> <p>If the gzip compression ratio is above this value, treat as failed.</p> <code>2.4</code> <code>log_prob_threshold</code> <code>float</code> <p>If the average log probability over sampled tokens is below this value, treat as failed.</p> <code>-1.0</code> <code>no_speech_threshold</code> <code>float</code> <p>If the no_speech probability is higher than this value AND the average log probability over sampled tokens is below <code>log_prob_threshold</code>, consider the segment as silent.</p> <code>0.6</code> <code>condition_on_previous_text</code> <code>bool</code> <p>If True, the previous output of the model is provided as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.</p> <code>False</code> <code>prompt</code> <code>Optional[str]</code> <p>Initial prompt to use for the generation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MultiChannelTranscriptionOutput</code> <code>MultiChannelTranscriptionOutput</code> <p>Multi-channel transcription segments in a list.</p> Source code in <code>src/wordcab_transcribe/services/transcribe_service.py</code> <pre><code>def multi_channel(\n    self,\n    audio: Union[str, torch.Tensor, TensorShare],\n    source_lang: str,\n    speaker_id: int,\n    suppress_blank: bool = False,\n    word_timestamps: bool = True,\n    internal_vad: bool = True,\n    repetition_penalty: float = 1.0,\n    compression_ratio_threshold: float = 2.4,\n    log_prob_threshold: float = -1.0,\n    no_speech_threshold: float = 0.6,\n    condition_on_previous_text: bool = False,\n    prompt: Optional[str] = None,\n) -&gt; MultiChannelTranscriptionOutput:\n    \"\"\"\n    Transcribe an audio file using the faster-whisper original pipeline.\n\n    Args:\n        audio (Union[str, torch.Tensor, TensorShare]): Audio file path or loaded audio.\n        source_lang (str): Language of the audio file.\n        speaker_id (int): Speaker ID used in the diarization.\n        suppress_blank (bool):\n            Whether to suppress blank at the beginning of the sampling.\n        word_timestamps (bool):\n            Whether to return word timestamps.\n        internal_vad (bool):\n            Whether to use faster-whisper's VAD or not.\n        repetition_penalty (float):\n            Repetition penalty to use during generation beamed search.\n        compression_ratio_threshold (float):\n            If the gzip compression ratio is above this value, treat as failed.\n        log_prob_threshold (float):\n            If the average log probability over sampled tokens is below this value, treat as failed.\n        no_speech_threshold (float):\n            If the no_speech probability is higher than this value AND the average log probability\n            over sampled tokens is below `log_prob_threshold`, consider the segment as silent.\n        condition_on_previous_text (bool):\n            If True, the previous output of the model is provided as a prompt for the next window;\n            disabling may make the text inconsistent across windows, but the model becomes less prone\n            to getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n        prompt (Optional[str]): Initial prompt to use for the generation.\n\n    Returns:\n        MultiChannelTranscriptionOutput: Multi-channel transcription segments in a list.\n    \"\"\"\n    if isinstance(audio, torch.Tensor):\n        _audio = audio.numpy()\n    elif isinstance(audio, TensorShare):\n        ts = audio.to_tensors(backend=Backend.NUMPY)\n        _audio = ts[\"audio\"]\n\n    final_segments = []\n\n    segments, _ = self.model.transcribe(\n        _audio,\n        language=source_lang,\n        initial_prompt=prompt,\n        repetition_penalty=repetition_penalty,\n        compression_ratio_threshold=compression_ratio_threshold,\n        log_prob_threshold=log_prob_threshold,\n        no_speech_threshold=no_speech_threshold,\n        condition_on_previous_text=condition_on_previous_text,\n        suppress_blank=suppress_blank,\n        word_timestamps=word_timestamps,\n        vad_filter=internal_vad,\n        vad_parameters={\n            \"threshold\": 0.5,\n            \"min_speech_duration_ms\": 250,\n            \"min_silence_duration_ms\": 100,\n            \"speech_pad_ms\": 30,\n            \"window_size_samples\": 512,\n        },\n    )\n\n    for segment in segments:\n        _segment = MultiChannelSegment(\n            start=segment.start,\n            end=segment.end,\n            text=segment.text,\n            words=[Word(**word._asdict()) for word in segment.words],\n            speaker=speaker_id,\n        )\n        final_segments.append(_segment)\n\n    return MultiChannelTranscriptionOutput(segments=final_segments)\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.vad_service.VadService","title":"<code>VadService</code>","text":"<p>VAD Service for audio files.</p> Source code in <code>src/wordcab_transcribe/services/vad_service.py</code> <pre><code>class VadService:\n    \"\"\"VAD Service for audio files.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the VAD Service.\"\"\"\n        self.sample_rate = 16000\n        self.options = VadOptions(\n            threshold=0.5,\n            min_speech_duration_ms=250,\n            max_speech_duration_s=30,\n            min_silence_duration_ms=100,\n            window_size_samples=512,\n            speech_pad_ms=400,\n        )\n\n    def __call__(\n        self, waveform: torch.Tensor, group_timestamps: Optional[bool] = True\n    ) -&gt; Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]:\n        \"\"\"\n        Use the VAD model to get the speech timestamps. Multi-channel pipeline.\n\n        Args:\n            waveform (torch.Tensor): Audio tensor.\n            group_timestamps (Optional[bool], optional): Group timestamps. Defaults to True.\n\n        Returns:\n            Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]: Speech timestamps and audio tensor.\n        \"\"\"\n        if waveform.size(0) == 1:\n            waveform = waveform.squeeze(0)\n\n        speech_timestamps = get_speech_timestamps(\n            audio=waveform, vad_options=self.options\n        )\n\n        _speech_timestamps_list = [\n            {\"start\": ts[\"start\"], \"end\": ts[\"end\"]} for ts in speech_timestamps\n        ]\n\n        if group_timestamps:\n            speech_timestamps_list = self.group_timestamps(_speech_timestamps_list)\n        else:\n            speech_timestamps_list = _speech_timestamps_list\n\n        return speech_timestamps_list, waveform\n\n    def group_timestamps(\n        self, timestamps: List[dict], threshold: Optional[float] = 3.0\n    ) -&gt; List[List[dict]]:\n        \"\"\"\n        Group timestamps based on a threshold.\n\n        Args:\n            timestamps (List[dict]): List of timestamps.\n            threshold (float, optional): Threshold to use for grouping. Defaults to 3.0.\n\n        Returns:\n            List[List[dict]]: List of grouped timestamps.\n        \"\"\"\n        grouped_segments = [[]]\n\n        for i in range(len(timestamps)):\n            if (\n                i &gt; 0\n                and (timestamps[i][\"start\"] - timestamps[i - 1][\"end\"]) &gt; threshold\n            ):\n                grouped_segments.append([])\n\n            grouped_segments[-1].append(timestamps[i])\n\n        return grouped_segments\n\n    def save_audio(self, filepath: str, audio: torch.Tensor) -&gt; None:\n        \"\"\"\n        Save audio tensor to file.\n\n        Args:\n            filepath (str): Path to save the audio file.\n            audio (torch.Tensor): Audio tensor.\n        \"\"\"\n        torchaudio.save(\n            filepath, audio.unsqueeze(0), self.sample_rate, bits_per_sample=16\n        )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.vad_service.VadService.__call__","title":"<code>__call__(waveform, group_timestamps=True)</code>","text":"<p>Use the VAD model to get the speech timestamps. Multi-channel pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>waveform</code> <code>Tensor</code> <p>Audio tensor.</p> required <code>group_timestamps</code> <code>Optional[bool]</code> <p>Group timestamps. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[Union[List[dict], List[List[dict]]], Tensor]</code> <p>Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]: Speech timestamps and audio tensor.</p> Source code in <code>src/wordcab_transcribe/services/vad_service.py</code> <pre><code>def __call__(\n    self, waveform: torch.Tensor, group_timestamps: Optional[bool] = True\n) -&gt; Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]:\n    \"\"\"\n    Use the VAD model to get the speech timestamps. Multi-channel pipeline.\n\n    Args:\n        waveform (torch.Tensor): Audio tensor.\n        group_timestamps (Optional[bool], optional): Group timestamps. Defaults to True.\n\n    Returns:\n        Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]: Speech timestamps and audio tensor.\n    \"\"\"\n    if waveform.size(0) == 1:\n        waveform = waveform.squeeze(0)\n\n    speech_timestamps = get_speech_timestamps(\n        audio=waveform, vad_options=self.options\n    )\n\n    _speech_timestamps_list = [\n        {\"start\": ts[\"start\"], \"end\": ts[\"end\"]} for ts in speech_timestamps\n    ]\n\n    if group_timestamps:\n        speech_timestamps_list = self.group_timestamps(_speech_timestamps_list)\n    else:\n        speech_timestamps_list = _speech_timestamps_list\n\n    return speech_timestamps_list, waveform\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.vad_service.VadService.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the VAD Service.</p> Source code in <code>src/wordcab_transcribe/services/vad_service.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the VAD Service.\"\"\"\n    self.sample_rate = 16000\n    self.options = VadOptions(\n        threshold=0.5,\n        min_speech_duration_ms=250,\n        max_speech_duration_s=30,\n        min_silence_duration_ms=100,\n        window_size_samples=512,\n        speech_pad_ms=400,\n    )\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.vad_service.VadService.group_timestamps","title":"<code>group_timestamps(timestamps, threshold=3.0)</code>","text":"<p>Group timestamps based on a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>List[dict]</code> <p>List of timestamps.</p> required <code>threshold</code> <code>float</code> <p>Threshold to use for grouping. Defaults to 3.0.</p> <code>3.0</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>List[List[dict]]: List of grouped timestamps.</p> Source code in <code>src/wordcab_transcribe/services/vad_service.py</code> <pre><code>def group_timestamps(\n    self, timestamps: List[dict], threshold: Optional[float] = 3.0\n) -&gt; List[List[dict]]:\n    \"\"\"\n    Group timestamps based on a threshold.\n\n    Args:\n        timestamps (List[dict]): List of timestamps.\n        threshold (float, optional): Threshold to use for grouping. Defaults to 3.0.\n\n    Returns:\n        List[List[dict]]: List of grouped timestamps.\n    \"\"\"\n    grouped_segments = [[]]\n\n    for i in range(len(timestamps)):\n        if (\n            i &gt; 0\n            and (timestamps[i][\"start\"] - timestamps[i - 1][\"end\"]) &gt; threshold\n        ):\n            grouped_segments.append([])\n\n        grouped_segments[-1].append(timestamps[i])\n\n    return grouped_segments\n</code></pre>"},{"location":"reference/services/#src.wordcab_transcribe.services.vad_service.VadService.save_audio","title":"<code>save_audio(filepath, audio)</code>","text":"<p>Save audio tensor to file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the audio file.</p> required <code>audio</code> <code>Tensor</code> <p>Audio tensor.</p> required Source code in <code>src/wordcab_transcribe/services/vad_service.py</code> <pre><code>def save_audio(self, filepath: str, audio: torch.Tensor) -&gt; None:\n    \"\"\"\n    Save audio tensor to file.\n\n    Args:\n        filepath (str): Path to save the audio file.\n        audio (torch.Tensor): Audio tensor.\n    \"\"\"\n    torchaudio.save(\n        filepath, audio.unsqueeze(0), self.sample_rate, bits_per_sample=16\n    )\n</code></pre>"},{"location":"usage/asr/","title":"ASR Engines","text":"<p>There are four different ASR Engines and the right one is chosen based on the <code>asr_type</code> parameter.</p>"},{"location":"usage/asr/#engines","title":"Engines","text":"<ul> <li> <p><code>ASRAsyncService</code>: the main engine that handles jobs and define the execution mode for transcription and diarization (post-processing is always locally done by this engine).</p> </li> <li> <p><code>ASRLiveService</code>: the engine that handles live streaming requests.</p> </li> <li> <p><code>ASRTranscriptionOnly</code>: the engine when you want to deploy a single transcription remote server.</p> </li> <li> <p><code>ASRDiarizationOnly</code>: the engine when you want to deploy a single diarization remote server.</p> </li> </ul> <p>Warning</p> <p>The <code>ASRTranscriptionOnly</code> and <code>ASRDiarizationOnly</code> engines aren't meant to be used alone. They are used only when you want to deploy each service in a separate server and they will need to be used along with the <code>ASRAsyncService</code> engine.</p>"},{"location":"usage/asr/#endpoints","title":"Endpoints","text":"<p>Each Engine has its own endpoints as described below.</p>"},{"location":"usage/asr/#asrasyncservice","title":"ASRAsyncService","text":""},{"location":"usage/asr/#transcription-endpoints","title":"Transcription endpoints","text":"<p>These endpoints are the main endpoints for transcribing audio files.</p> <ul> <li><code>/audio</code> [POST] - The audio endpoint for transcribing local files.</li> </ul> <pre><code>@router.post(\n    \"\", response_model=Union[AudioResponse, str], status_code=http_status.HTTP_200_OK\n)\nasync def inference_with_audio(\n    background_tasks: BackgroundTasks,\n    offset_start: Union[float, None] = Form(None),\n    offset_end: Union[float, None] = Form(None),\n    num_speakers: int = Form(-1),\n    diarization: bool = Form(False),\n    multi_channel: bool = Form(False),\n    source_lang: str = Form(\"en\"),\n    timestamps: str = Form(\"s\"),\n    vocab: Union[List[str], None] = Form(None),\n    word_timestamps: bool = Form(False),\n    internal_vad: bool = Form(False),\n    repetition_penalty: float = Form(1.2),\n    compression_ratio_threshold: float = Form(2.4),\n    log_prob_threshold: float = Form(-1.0),\n    no_speech_threshold: float = Form(0.6),\n    condition_on_previous_text: bool = Form(True),\n    file: UploadFile = File(...),\n) -&gt; AudioResponse:\n    \"\"\"Inference endpoint with audio file.\"\"\"\n</code></pre> <p>Note</p> <p>The local <code>/audio</code> endpoint is expecting a <code>file</code> parameter and all the other parameters are optional and have default values.</p> <ul> <li><code>/audio-url</code> [POST] - The audio endpoint for transcribing remote files using a URL.</li> </ul> <pre><code>@router.post(\"\", response_model=AudioResponse, status_code=http_status.HTTP_200_OK)\nasync def inference_with_audio_url(\n    background_tasks: BackgroundTasks,\n    url: str,\n    data: Optional[AudioRequest] = None,\n) -&gt; AudioResponse:\n    \"\"\"Inference endpoint with audio url.\"\"\"\n</code></pre> <p>Here is the <code>AudioRequest</code> model which inherits from the <code>BaseRequest</code> model:</p> <pre><code>class BaseRequest(BaseModel):\n    \"\"\"Base request model for the API.\"\"\"\n\n    offset_start: Union[float, None] = None\n    offset_end: Union[float, None] = None\n    num_speakers: int = -1\n    diarization: bool = False\n    source_lang: str = \"en\"\n    timestamps: Timestamps = Timestamps.seconds\n    vocab: Union[List[str], None] = None\n    word_timestamps: bool = False\n    internal_vad: bool = False\n    repetition_penalty: float = 1.2\n    compression_ratio_threshold: float = 2.4\n    log_prob_threshold: float = -1.0\n    no_speech_threshold: float = 0.6\n    condition_on_previous_text: bool = True\n\nclass AudioRequest(BaseRequest):\n    \"\"\"Request model for the ASR audio file and url endpoint.\"\"\"\n\n    multi_channel: bool = False\n</code></pre> <p>Here is the <code>AudioResponse</code> model which inherits from the <code>BaseResponse</code> model:</p> <pre><code>class BaseResponse(BaseModel):\n    \"\"\"Base response model, not meant to be used directly.\"\"\"\n\n    utterances: List[Utterance]\n    audio_duration: float\n    offset_start: Union[float, None]\n    offset_end: Union[float, None]\n    num_speakers: int\n    diarization: bool\n    source_lang: str\n    timestamps: str\n    vocab: Union[List[str], None]\n    word_timestamps: bool\n    internal_vad: bool\n    repetition_penalty: float\n    compression_ratio_threshold: float\n    log_prob_threshold: float\n    no_speech_threshold: float\n    condition_on_previous_text: bool\n    process_times: ProcessTimes\n\n\nclass AudioResponse(BaseResponse):\n    \"\"\"Response model for the ASR audio file and url endpoint.\"\"\"\n\n    multi_channel: bool\n</code></pre> <ul> <li><code>youtube</code> [POST] - The audio endpoint for transcribing YouTube videos using a YouTube video link.</li> </ul> <pre><code>@router.post(\"\", response_model=YouTubeResponse, status_code=http_status.HTTP_200_OK)\nasync def inference_with_youtube(\n    background_tasks: BackgroundTasks,\n    url: str,\n    data: Optional[BaseRequest] = None,\n) -&gt; YouTubeResponse:\n    \"\"\"Inference endpoint with YouTube url.\"\"\"\n</code></pre> <p>Note</p> <p>As you can see the only difference is that the YouTube endpoint has the same <code>BaseRequest</code> model as the <code>/audio-url</code> endpoint but without the <code>multi_channel</code> parameter.</p> <p>Here is the <code>YouTubeResponse</code> model which inherits from the <code>BaseResponse</code> model:</p> <pre><code>class YouTubeResponse(BaseResponse):\n    \"\"\"Response model for the ASR YouTube endpoint.\"\"\"\n\n    video_url: str\n</code></pre>"},{"location":"usage/asr/#management-endpoints","title":"Management endpoints","text":"<p>These endpoints are used to manage the remote servers URLs, when you want to deploy the <code>ASRTranscriptionOnly</code> or <code>ASRDiarizationOnly</code> engines in separate servers.</p> <ul> <li><code>/url</code> [GET] - This endpoint allow listing the remote servers URLs.</li> </ul> <pre><code>@router.get(\n    \"\",\n    response_model=Union[List[HttpUrl], str],\n    status_code=http_status.HTTP_200_OK,\n)\nasync def get_url(task: Literal[\"transcription\", \"diarization\"]) -&gt; List[HttpUrl]:\n    \"\"\"Get Remote URL endpoint for remote transcription or diarization.\"\"\"\n</code></pre> <ul> <li><code>/url/add</code> [POST] - This endpoint allow adding a remote server URL.</li> </ul> <pre><code>@router.post(\n    \"/add\",\n    response_model=Union[UrlSchema, str],\n    status_code=http_status.HTTP_200_OK,\n)\nasync def add_url(data: UrlSchema) -&gt; UrlSchema:\n    \"\"\"Add Remote URL endpoint for remote transcription or diarization.\"\"\"\n</code></pre> <ul> <li><code>/url/remove</code> [POST] - This endpoint allow removing a remote server URL.</li> </ul> <pre><code>@router.post(\n    \"/remove\",\n    response_model=Union[UrlSchema, str],\n    status_code=http_status.HTTP_200_OK,\n)\nasync def remove_url(data: UrlSchema) -&gt; UrlSchema:\n    \"\"\"Remove Remote URL endpoint for remote transcription or diarization.\"\"\"\n</code></pre> <p>Here is the <code>UrlSchema</code> model:</p> <pre><code>class UrlSchema(BaseModel):\n    \"\"\"Request model for the add_url endpoint.\"\"\"\n\n    task: Literal[\"transcription\", \"diarization\"]\n    url: HttpUrl\n</code></pre> <p>The <code>url</code> parameter needs to be a valid URL (check pydantic HttpUrl) and the <code>task</code> parameter needs to be either <code>transcription</code> or <code>diarization</code>.</p>"},{"location":"usage/asr/#asrliveservice","title":"ASRLiveService","text":"<ul> <li><code>/live</code> [WEBSOCKET] - The live streaming endpoint.</li> </ul> <pre><code>@router.websocket(\"\")\nasync def websocket_endpoint(source_lang: str, websocket: WebSocket) -&gt; None:\n    \"\"\"Handle WebSocket connections.\"\"\"\n</code></pre> <p>This endpoint expects a WebSocket connection and a <code>source_lang</code> parameter as a string, and will return the transcription results in real-time.</p>"},{"location":"usage/asr/#asrtranscriptiononly","title":"ASRTranscriptionOnly","text":"<p>Warning</p> <p>This endpoint is not meant to be used alone. It is used only when you want to deploy the <code>ASRTranscriptionOnly</code> engine in a separate server and it will need to be used along with the <code>ASRAsyncService</code> engine.</p> <ul> <li><code>/transcribe</code> [POST] - The transcription endpoint.</li> </ul> <pre><code>@router.post(\n    \"\",\n    response_model=Union[TranscriptionOutput, List[TranscriptionOutput], str],\n    status_code=http_status.HTTP_200_OK,\n)\nasync def only_transcription(\n    data: TranscribeRequest,\n) -&gt; Union[TranscriptionOutput, List[TranscriptionOutput]]:\n    \"\"\"Transcribe endpoint for the `only_transcription` asr type.\"\"\"\n</code></pre> <p>This endpoint expects a <code>TranscribeRequest</code> and will return the transcription results as a <code>TranscriptionOutput</code> or a list of <code>TranscriptionOutput</code> if the task is a <code>multi_channel</code> task.</p> <p>Here is the <code>TranscribeRequest</code> model:</p> <pre><code>class TranscribeRequest(BaseModel):\n    \"\"\"Request model for the transcribe endpoint.\"\"\"\n\n    audio: Union[TensorShare, List[TensorShare]]\n    compression_ratio_threshold: float\n    condition_on_previous_text: bool\n    internal_vad: bool\n    log_prob_threshold: float\n    no_speech_threshold: float\n    repetition_penalty: float\n    source_lang: str\n    vocab: Union[List[str], None]\n</code></pre> <p>Here is the <code>TranscriptionOutput</code> model:</p> <pre><code>class TranscriptionOutput(BaseModel):\n    \"\"\"Transcription output model for the API.\"\"\"\n\n    segments: List[Segment]\n</code></pre>"},{"location":"usage/asr/#asrdiarizationonly","title":"ASRDiarizationOnly","text":"<p>Warning</p> <p>This endpoint is not meant to be used alone. It is used only when you want to deploy the <code>ASRDiarizationOnly</code> engine in a separate server and it will need to be used along with the <code>ASRAsyncService</code> engine.</p> <ul> <li><code>/diarize</code> [POST] - The diarization endpoint.</li> </ul> <pre><code>@router.post(\n    \"\",\n    response_model=Union[DiarizationOutput, str],\n    status_code=http_status.HTTP_200_OK,\n)\nasync def remote_diarization(\n    data: DiarizationRequest,\n) -&gt; DiarizationOutput:\n    \"\"\"Diarize endpoint for the `only_diarization` asr type.\"\"\"\n</code></pre> <p>This endpoint expects a <code>DiarizationRequest</code> and will return the diarization results as a <code>DiarizationOutput</code>.</p> <p>Here is the <code>DiarizationRequest</code> model:</p> <pre><code>class DiarizationRequest(BaseModel):\n    \"\"\"Request model for the diarize endpoint.\"\"\"\n\n    audio: TensorShare\n    duration: float\n    num_speakers: int\n</code></pre> <p>Here is the <code>DiarizationSegment</code> and <code>DiarizationOutput</code> models:</p> <pre><code>class DiarizationSegment(NamedTuple):\n    \"\"\"Diarization segment model for the API.\"\"\"\n\n    start: float\n    end: float\n    speaker: int\n\n\nclass DiarizationOutput(BaseModel):\n    \"\"\"Diarization output model for the API.\"\"\"\n\n    segments: List[DiarizationSegment]\n</code></pre>"},{"location":"usage/asr/#execution-modes","title":"Execution modes","text":"<p>The execution modes represent the way the tasks are executed, either locally or remotely. For each task, one execution mode is defined.</p> <p>There are two different execution modes: <code>LocalExecution</code> and <code>RemoteExecution</code>.</p> <ul> <li><code>LocalExecution</code> is the default execution mode. It executes the pipeline on the local machine. It is useful for testing and debugging.</li> </ul> <pre><code>class LocalExecution(BaseModel):\n    \"\"\"Local execution model.\"\"\"\n\n    index: Union[int, None]\n</code></pre> <p>The local execution is looking for any local GPU device. If there is no GPU device, it will use the CPU. If there are multiple GPU devices, it will use all alternatively. The <code>index</code> parameter keep the track of the GPU index assigned to the task.</p> <ul> <li><code>RemoteExecution</code> executes the pipeline on a remote machine. It is useful for production and scaling.</li> </ul> <p>Note</p> <p>The remote execution mode is only available if you have added <code>transcribe_server_urls</code> or <code>diarization_server_urls</code> in the configuration file or on the fly via the API. Check the Environment variables section for more information.</p> <pre><code>class RemoteExecution(BaseModel):\n    \"\"\"Remote execution model.\"\"\"\n\n    url: str\n</code></pre> <p>The <code>url</code> parameter is the URL of the remote machine that will be used for the execution of a task.</p>"},{"location":"usage/env/","title":"ENV variables","text":"<p>The configuration is defined in the <code>.env</code> file at the root of the project.</p> <p>On this page, we will list all the variables with their default values and a description of their usage.</p> <p>Warning</p> <p>Never remove this file or any of the variables below. You can only modify the values of the variables to customize the configuration of the API.</p>"},{"location":"usage/env/#general-configuration","title":"General configuration","text":"<p>The name of the project, used for API documentation.</p> <p>PROJECT_NAME=\"Wordcab Transcribe\"</p> <p>The version of the project, used for API documentation.</p> <p>VERSION=\"0.5.1\"</p> <p>The description of the project, used for API documentation.</p> <p>DESCRIPTION=\"\ud83d\udcac ASR FastAPI server using faster-whisper and Auto-Tuning Spectral Clustering for diarization.\"</p> <p>This API prefix is used for all endpoints in the API outside of the status and cortex endpoints.</p> <p>API_PREFIX=\"/api/v1\"</p> <p>Debug mode for the API, used to control the API authentication. Set to <code>True</code> to disable authentication and enable debug mode.</p> <p>DEBUG=True</p>"},{"location":"usage/env/#models-configuration","title":"Models configuration","text":"<p>The models configuration is divided between the transcription and diarization models.</p>"},{"location":"usage/env/#transcription-models","title":"Transcription models","text":"<p>The <code>whisper_model</code> parameter is used to control the model used for transcription.</p> <ul> <li>Cloud models:</li> </ul> <p>The available models are: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, or large-v2 You can try different model size, but you should see a trade-off between performance and speed.</p> <ul> <li>Local models:</li> </ul> <p>You can also link a local folder path to use a custom model. If you do so, you should also mount the folder in the docker run command as a volume.</p> <p>e.g. WHISPER_MODEL=\"/app/models/custom\"</p> <p>docker cmd: <code>-v /path/to/custom/model:/app/models/custom</code></p> <p>WHISPER_MODEL=\"large-v2\"</p> <p>The <code>compute_type</code> parameter is used to control the precision of the model. You can choose between: <code>\"int8\"</code>, <code>\"int8_float16\"</code>, <code>\"int8_bfloat16\"</code>, <code>\"int16\"</code>, <code>\"float_16\"</code>, <code>\"bfloat16\"</code>, <code>\"float32\"</code>.</p> <p>The default value is <code>\"float16\"</code>.</p> <p>COMPUTE_TYPE=\"float16\"</p> <p>The <code>extra_languages</code> parameter is used to control the languages that need an extra model to be loaded. You can specify multiple languages separated by a comma. The available languages are: </p> <ul> <li><code>he</code> (Hebrew).</li> <li><code>th</code> (Thai).</li> </ul> <p>EXTRA_LANGUAGES=</p>"},{"location":"usage/env/#diarization-models","title":"Diarization models","text":"<p>In a MSDD (Multiscale Diarization Decoder) model, the diarization model is trained on multiple window lengths. The <code>window_lengths</code> are specified in seconds, and separated by a comma. If not specified, the default value will be <code>\"1.5, 1.25, 1.0, 0.75, 0.5\"</code>.</p> <p>WINDOW_LENGTHS=\"1.5,1.25,1.0,0.75,0.5\"</p> <p>The <code>shift_lengths</code> are specified in seconds, and separated by a comma. If not specified, the default value will be <code>\"0.75, 0.625, 0.5, 0.375, 0.25\"</code>.</p> <p>SHIFT_LENGTHS=\"0.75,0.625,0.5,0.375,0.25\"</p> <p>The <code>multiscale_weights</code> are float values separated by a comma. If not specified, the default value will be <code>\"1.0, 1.0, 1.0, 1.0, 1.0\"</code>.</p> <p>MULTISCALE_WEIGHTS=\"1.0,1.0,1.0,1.0,1.0\"</p>"},{"location":"usage/env/#asr-type-configuration","title":"ASR type configuration","text":"<p>The <code>asr_type</code> parameter is used to control the type of ASR used. The available options are: <code>async</code> or <code>live</code>.</p> <ul> <li><code>async</code> is the default option. It will process the audio files in batches, and return the results when all the files are processed.</li> <li><code>live</code> is the option to use when you want to process a live audio stream. It will process the audio in chunks, and return the results as soon as they are available. Live option is still a feature in development.</li> <li><code>only_transcription</code> is used to deploy a single transcription server. This option is used when you want to deploy each service in a separate server.</li> <li><code>only_diarization</code> is used to deploy a single diarization server. This option is used when you want to deploy each service in a separate server. Use <code>live</code> only if you need live results, otherwise, use <code>async</code>.</li> </ul> <p>ASR_TYPE=\"async\"</p>"},{"location":"usage/env/#endpoints-configuration","title":"Endpoints configuration","text":"<p>Include the cortex endpoint in the API. This endpoint is used to process audio files from the Cortex API. Use this only if you deploy the API using Cortex and Kubernetes.</p> <p>CORTEX_ENDPOINT=True</p>"},{"location":"usage/env/#api-authentication-configuration","title":"API authentication configuration","text":"<p>The API authentication is used to control the access to the API endpoints. It's activated only when the debug mode is set to False.</p> <p>The <code>username</code> and <code>password</code> are the credentials used to authenticate with the API.</p> <p>USERNAME=\"admin\" PASSWORD=\"admin\"</p> <p>This <code>openssl_key</code> parameter is used to control the key used to encrypt the access tokens. You should absolutely change this value before deploying the API in production.</p> <p>OPENSSL_KEY=\"0123456789abcdefghijklmnopqrstuvwyz\" &lt;--- CHANGE ABSOLUTELY THIS VALUE</p> <p>This <code>openssl_algorithm</code> parameter is used to control the algorithm used to encrypt the access tokens. You should in most case not change this value.</p> <p>OPENSSL_ALGORITHM=\"HS256\"</p> <p>The <code>access_token_expire_minutes</code> parameter is used to control the expiration time of the access tokens. You can modify it, it's not a critical parameter. Note that this parameter is in minutes.</p> <p>ACCESS_TOKEN_EXPIRE_MINUTES=30</p>"},{"location":"usage/env/#cortex-configuration","title":"CORTEX configuration","text":"<p>At Wordcab, we are using Cortex to deploy the API in production. You probably don't need to change these values.</p> <p>The <code>cortex_api_key</code> parameter is used to control the API key used to authenticate the requests to the cortex endpoint.</p> <p>WORDCAB_TRANSCRIBE_API_KEY=</p>"},{"location":"usage/env/#svix-configuration","title":"SVIX configuration","text":"<p>The <code>svix_api_key</code> parameter is used in the cortex implementation to enable webhooks.</p> <p>SVIX_API_KEY=</p> <p>The <code>svix_app_id</code> parameter is used in the cortex implementation to enable webhooks.</p> <p>SVIX_APP_ID=</p>"},{"location":"usage/env/#remote-servers-configuration","title":"Remote servers configuration","text":"<p>The remote servers configuration is used to control the number of servers used to process the requests if you don't want to group all the services in one server.</p> <p>Tip</p> <p>Check the Execution Modes page to learn more about the different execution modes.</p> <p>The <code>transcribe_server_urls</code> parameter is used to control the URLs of the servers used to process the requests. Each url should be separated by a comma and have this format: \"host:port\".</p> <p>e.g. TRANSCRIBE_SERVER_URLS=\"http://1.2.3.4:8000,http://4.3.2.1:8000\"</p> <p>TRANSCRIBE_SERVER_URLS=</p> <p>The <code>diarize_server_urls</code> parameter is used to control the URLs of the servers used to process the requests. Each url should be separated by a comma and have this format: \"host:port\". e.g. DIARIZE_SERVER_URLS=\"http://1.2.3.4:8000,http://4.3.2.1:8000\"</p> <p>DIARIZE_SERVER_URLS=</p>"},{"location":"usage/launch/","title":"Launch","text":"<p>You can run the API either locally or using Docker. The API is built using FastAPI and Uvicorn. </p> <p>We are using environment variables to configure and customize the API runtime, find the list of available environment variables in the ENV section.</p>"},{"location":"usage/launch/#run-locally","title":"Run locally","text":"<pre><code>hatch run runtime:launch\n</code></pre> <p>Tip</p> <p>This is the recommended way to run the API in development as it's easier to debug and hot reload when code changes.</p>"},{"location":"usage/launch/#run-using-docker","title":"Run using Docker","text":"<p>Build the image.</p> <pre><code>docker build -t wordcab-transcribe:latest .\n</code></pre> <p>Run the container.</p> <pre><code>docker run -d --name wordcab-transcribe \\\n    --gpus all \\\n    --shm-size 1g \\\n    --restart unless-stopped \\\n    -p 5001:5001 \\\n    -v ~/.cache:/root/.cache \\\n    wordcab-transcribe:latest\n</code></pre> <p>You can mount a volume to the container to load local whisper models.</p> <p>If you mount a volume, you need to update the <code>WHISPER_MODEL</code> environment variable in the <code>.env</code> file.</p> <pre><code>docker run -d --name wordcab-transcribe \\\n    --gpus all \\\n    --shm-size 1g \\\n    --restart unless-stopped \\\n    -p 5001:5001 \\\n    -v ~/.cache:/root/.cache \\\n    -v /path/to/whisper/models:/app/whisper/models \\\n    wordcab-transcribe:latest\n</code></pre> <p>You can simply enter the container using the following command:</p> <pre><code>docker exec -it wordcab-transcribe /bin/bash\n</code></pre> <p>Check the logs to know when the API is ready.</p> <pre><code>docker logs -f wordcab-transcribe\n</code></pre> <p>This is useful to check everything is working as expected.</p>"},{"location":"usage/launch/#run-behind-a-reverse-proxy","title":"Run behind a reverse proxy","text":"<p>We have included a <code>nginx.conf</code> file to help you get started.</p> <pre><code># Create a docker network and connect the api container to it\ndocker network create transcribe\ndocker network connect transcribe wordcab-transcribe\n\n# Replace /absolute/path/to/nginx.conf with the absolute path to the nginx.conf\n# file on your machine (e.g. /home/user/wordcab-transcribe/nginx.conf).\ndocker run -d \\\n    --name nginx \\\n    --network transcribe \\\n    -p 80:80 \\\n    -v /absolute/path/to/nginx.conf:/etc/nginx/nginx.conf:ro \\\n    nginx\n\n# Check everything is working as expected\ndocker logs nginx\n</code></pre> <p>Your API should now be exposed on port 80.</p>"},{"location":"usage/launch/#run-only_transcription","title":"Run <code>only_transcription</code>","text":"<p>You can run the API in transcription only mode by setting the <code>asr_type</code> in the <code>.env</code> file to <code>only_transcription</code>.</p>"},{"location":"usage/launch/#run-only_diarization","title":"Run <code>only_diarization</code>","text":"<p>You can run the API in diarization only mode by setting the <code>asr_type</code> in the <code>.env</code> file to <code>only_diarization</code>.</p>"},{"location":"usage/launch/#use-remote-servers","title":"Use remote servers","text":"<p>You can use remote servers for transcription and diarization by setting the <code>asr_type</code> in the <code>.env</code> file to <code>async</code> and adding URLs to the <code>transcribe_server_urls</code> and <code>diarize_server_urls</code> environment variables.</p> <p>If an async server is already running, you can simply add remote servers to the list of URLs by using the endpoints, check Management endpoints for more details.</p>"}]}