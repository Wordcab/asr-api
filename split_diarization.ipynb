{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_diarize():\n",
    "    \"\"\"\"\"\"\n",
    "    # VAD\n",
    "    waveform, _ = read_audio(\"./mono_file.wav\")\n",
    "    vad_service = VadService()\n",
    "    speech_ts, _ = vad_service(waveform, False)\n",
    "\n",
    "    # Segmentation\n",
    "\n",
    "    # Clustering\n",
    "\n",
    "    # Scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:06:34 cloud:58] Found existing object /home/chainyo/.cache/torch/NeMo/NeMo_1.19.1/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2023-07-25 09:06:34 cloud:64] Re-using file from: /home/chainyo/.cache/torch/NeMo/NeMo_1.19.1/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo\n",
      "[NeMo I 2023-07-25 09:06:34 common:913] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-25 09:06:35 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-07-25 09:06:35 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:06:35 features:291] PADDING: 16\n",
      "[NeMo I 2023-07-25 09:06:35 save_restore_connector:249] Model EncDecSpeakerLabelModel was successfully restored from /home/chainyo/.cache/torch/NeMo/NeMo_1.19.1/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2023-07-25 09:06:35 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2023-07-25 09:06:35 cloud:58] Found existing object /home/chainyo/.cache/torch/NeMo/NeMo_1.19.1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2023-07-25 09:06:35 cloud:64] Re-using file from: /home/chainyo/.cache/torch/NeMo/NeMo_1.19.1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2023-07-25 09:06:35 common:913] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-25 09:06:35 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-07-25 09:06:35 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-07-25 09:06:35 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:06:35 features:291] PADDING: 16\n",
      "[NeMo I 2023-07-25 09:06:35 save_restore_connector:249] Model EncDecClassificationModel was successfully restored from /home/chainyo/.cache/torch/NeMo/NeMo_1.19.1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from nemo.collections.asr.models.label_models import EncDecSpeakerLabelModel\n",
    "from nemo.collections.asr.models.msdd_models import ClusteringDiarizer\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "# Load yaml file\n",
    "with open(\"./config/nemo/diar_infer_telephonic.yaml\") as f:\n",
    "    cfg = OmegaConf.load(f)\n",
    "\n",
    "meta = {\n",
    "    \"audio_filepath\":  \"mono_file.wav\",\n",
    "    \"offset\": 0,\n",
    "    \"duration\": None,\n",
    "    \"label\": \"infer\",\n",
    "    \"text\": \"-\",\n",
    "    \"rttm_filepath\": None,\n",
    "    \"uem_filepath\": None,\n",
    "}\n",
    "\n",
    "manifest_path = \"infer_manifest.json\"\n",
    "with open(\"infer_manifest.json\", \"w\") as fp:\n",
    "    json.dump(meta, fp)\n",
    "    fp.write(\"\\n\")\n",
    "\n",
    "cfg.diarizer.manifest_filepath = str(manifest_path)\n",
    "cfg.diarizer.out_dir = \"infer_out_dir\"\n",
    "\n",
    "speaker_model = EncDecSpeakerLabelModel.from_pretrained(\n",
    "    model_name=\"titanet_large\", map_location=None\n",
    ")\n",
    "speaker_params = {\n",
    "    \"window_length_in_sec\": [1.5, 1.25, 1.0, 0.75, 0.5],\n",
    "    \"shift_length_in_sec\": [0.75, 0.625, 0.5, 0.375, 0.25],\n",
    "    \"multiscale_weights\": [1, 1, 1, 1, 1],\n",
    "    \"save_embeddings\": True,\n",
    "}\n",
    "cluster_params = {\n",
    "    \"oracle_num_speakers\": False,\n",
    "    \"max_num_speakers\": 8,\n",
    "    \"enhanced_count_thres\": 80,\n",
    "    \"max_rp_threshold\": 0.25,\n",
    "    \"sparse_search_volume\": 30,\n",
    "    \"maj_vote_spk_count\": False,\n",
    "}\n",
    "\n",
    "clus_diar_model = ClusteringDiarizer(cfg=cfg, speaker_model=speaker_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "filepath = \"./mono_file.mp3\"\n",
    "waveform, sample_rate = librosa.load(filepath, sr=None)\n",
    "sf.write(\"./mono_file.wav\", waveform, sample_rate, \"PCM_16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-25 09:37:44 clustering_diarizer:411] Deleting previous clustering diarizer outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:37:44 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2023-07-25 09:37:44 clustering_diarizer:309] Split long audio file to avoid CUDA memory issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:37:44 vad_utils:101] The prepared manifest file exists. Overwriting!\n",
      "[NeMo I 2023-07-25 09:37:44 classification_models:268] Perform streaming frame-level VAD\n",
      "[NeMo I 2023-07-25 09:37:44 collections:298] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2023-07-25 09:37:44 collections:299] Dataset loaded with 3 items, total duration of  0.04 hours.\n",
      "[NeMo I 2023-07-25 09:37:44 collections:301] # 3 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:37:46 clustering_diarizer:250] Generating predictions with overlapping input segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:37:47 clustering_diarizer:262] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating speech segments: 100%|██████████| 1/1 [00:00<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:37:47 clustering_diarizer:287] Subsegmentation for embedding extraction: scale0, infer_out_dir/speaker_outputs/subsegments_scale0.json\n",
      "[NeMo I 2023-07-25 09:37:47 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-07-25 09:37:47 collections:298] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2023-07-25 09:37:47 collections:299] Dataset loaded with 104 items, total duration of  0.04 hours.\n",
      "[NeMo I 2023-07-25 09:37:47 collections:301] # 104 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:389] Saved embedding files to infer_out_dir/speaker_outputs/embeddings\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:287] Subsegmentation for embedding extraction: scale1, infer_out_dir/speaker_outputs/subsegments_scale1.json\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-07-25 09:37:48 collections:298] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2023-07-25 09:37:48 collections:299] Dataset loaded with 132 items, total duration of  0.04 hours.\n",
      "[NeMo I 2023-07-25 09:37:48 collections:301] # 132 files loaded accounting to # 1 labels\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:389] Saved embedding files to infer_out_dir/speaker_outputs/embeddings\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:287] Subsegmentation for embedding extraction: scale2, infer_out_dir/speaker_outputs/subsegments_scale2.json\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-07-25 09:37:48 collections:298] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2023-07-25 09:37:48 collections:299] Dataset loaded with 166 items, total duration of  0.04 hours.\n",
      "[NeMo I 2023-07-25 09:37:48 collections:301] # 166 files loaded accounting to # 1 labels\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:389] Saved embedding files to infer_out_dir/speaker_outputs/embeddings\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:287] Subsegmentation for embedding extraction: scale3, infer_out_dir/speaker_outputs/subsegments_scale3.json\n",
      "[NeMo I 2023-07-25 09:37:48 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-07-25 09:37:48 collections:298] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2023-07-25 09:37:48 collections:299] Dataset loaded with 222 items, total duration of  0.04 hours.\n",
      "[NeMo I 2023-07-25 09:37:48 collections:301] # 222 files loaded accounting to # 1 labels\n",
      "[NeMo I 2023-07-25 09:37:49 clustering_diarizer:389] Saved embedding files to infer_out_dir/speaker_outputs/embeddings\n",
      "[NeMo I 2023-07-25 09:37:49 clustering_diarizer:287] Subsegmentation for embedding extraction: scale4, infer_out_dir/speaker_outputs/subsegments_scale4.json\n",
      "[NeMo I 2023-07-25 09:37:49 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-07-25 09:37:49 collections:298] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2023-07-25 09:37:49 collections:299] Dataset loaded with 343 items, total duration of  0.05 hours.\n",
      "[NeMo I 2023-07-25 09:37:49 collections:301] # 343 files loaded accounting to # 1 labels\n",
      "[NeMo I 2023-07-25 09:37:49 clustering_diarizer:389] Saved embedding files to infer_out_dir/speaker_outputs/embeddings\n",
      "[NeMo I 2023-07-25 09:37:50 clustering_diarizer:464] Outputs are saved in /home/chainyo/wordcab-transcribe/infer_out_dir directory\n"
     ]
    }
   ],
   "source": [
    "clus_diar_model.diarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from faster_whisper.vad import VadOptions, get_speech_timestamps\n",
    "\n",
    "\n",
    "class VadService:\n",
    "    \"\"\"VAD Service for audio files.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the VAD Service.\"\"\"\n",
    "        self.sample_rate = 16000\n",
    "        self.options = VadOptions(\n",
    "            threshold=0.5,\n",
    "            min_speech_duration_ms=250,\n",
    "            max_speech_duration_s=30,\n",
    "            min_silence_duration_ms=100,\n",
    "            window_size_samples=512,\n",
    "            speech_pad_ms=30,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, waveform: torch.Tensor, group_timestamps: Optional[bool] = True\n",
    "    ) -> Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Use the VAD model to get the speech timestamps. Dual channel pipeline.\n",
    "\n",
    "        Args:\n",
    "            waveform (torch.Tensor): Audio tensor.\n",
    "            group_timestamps (Optional[bool], optional): Group timestamps. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Union[List[dict], List[List[dict]]], torch.Tensor]: Speech timestamps and audio tensor.\n",
    "        \"\"\"\n",
    "        if waveform.size(0) == 1:\n",
    "            waveform = waveform.squeeze(0)\n",
    "\n",
    "        speech_timestamps = get_speech_timestamps(\n",
    "            audio=waveform, vad_options=self.options\n",
    "        )\n",
    "\n",
    "        _speech_timestamps_list = [\n",
    "            {\"start\": ts[\"start\"], \"end\": ts[\"end\"]} for ts in speech_timestamps\n",
    "        ]\n",
    "\n",
    "        if group_timestamps:\n",
    "            speech_timestamps_list = self.group_timestamps(_speech_timestamps_list)\n",
    "        else:\n",
    "            speech_timestamps_list = _speech_timestamps_list\n",
    "\n",
    "        return speech_timestamps_list, waveform\n",
    "\n",
    "    def group_timestamps(\n",
    "        self, timestamps: List[dict], threshold: Optional[float] = 3.0\n",
    "    ) -> List[List[dict]]:\n",
    "        \"\"\"\n",
    "        Group timestamps based on a threshold.\n",
    "\n",
    "        Args:\n",
    "            timestamps (List[dict]): List of timestamps.\n",
    "            threshold (float, optional): Threshold to use for grouping. Defaults to 3.0.\n",
    "\n",
    "        Returns:\n",
    "            List[List[dict]]: List of grouped timestamps.\n",
    "        \"\"\"\n",
    "        grouped_segments = [[]]\n",
    "\n",
    "        for i in range(len(timestamps)):\n",
    "            if (\n",
    "                i > 0\n",
    "                and (timestamps[i][\"start\"] - timestamps[i - 1][\"end\"]) > threshold\n",
    "            ):\n",
    "                grouped_segments.append([])\n",
    "\n",
    "            grouped_segments[-1].append(timestamps[i])\n",
    "\n",
    "        return grouped_segments\n",
    "\n",
    "    def save_audio(self, filepath: str, audio: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Save audio tensor to file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to save the audio file.\n",
    "            audio (torch.Tensor): Audio tensor.\n",
    "        \"\"\"\n",
    "        torchaudio.save(\n",
    "            filepath, audio.unsqueeze(0), self.sample_rate, bits_per_sample=16\n",
    "        )\n",
    "\n",
    "def read_audio(filepath: str, sample_rate: int = 16000) -> Tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Read an audio file and return the audio tensor.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the audio file.\n",
    "        sample_rate (int): The sample rate of the audio file. Defaults to 16000.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, float]: The audio tensor and the audio duration.\n",
    "    \"\"\"\n",
    "    wav, sr = torchaudio.load(filepath)\n",
    "\n",
    "    if wav.size(0) > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if sr != sample_rate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)\n",
    "        wav = transform(wav)\n",
    "        sr = sample_rate\n",
    "\n",
    "    audio_duration = float(wav.shape[1]) / sample_rate\n",
    "\n",
    "    return wav.squeeze(0), audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr2s(v: int) -> float:\n",
    "    \"\"\"\n",
    "    Convert milliseconds to seconds.\n",
    "\n",
    "    Args:\n",
    "        v (int): Value in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "        float: Value in seconds.\n",
    "    \"\"\"\n",
    "    return v / 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "Start: 12.514, End: 12.99\n",
      "Start: 13.25, End: 14.366\n",
      "Start: 15.042, End: 16.862\n",
      "Start: 17.762, End: 18.814\n",
      "Start: 19.426, End: 20.766\n",
      "Start: 21.666, End: 24.83\n",
      "Start: 26.178, End: 29.886\n",
      "Start: 30.786, End: 33.022\n",
      "Start: 34.146, End: 37.214\n",
      "Start: 38.338, End: 40.318\n",
      "Start: 41.218, End: 42.782\n",
      "Start: 43.682, End: 44.318\n",
      "Start: 45.73, End: 46.494\n",
      "Start: 47.778, End: 50.366\n",
      "Start: 51.106, End: 52.926\n",
      "Start: 53.954, End: 55.582\n",
      "Start: 55.682, End: 56.926\n",
      "Start: 57.954, End: 60.286\n",
      "Start: 61.154, End: 64.254\n",
      "Start: 65.026, End: 67.614\n",
      "Start: 68.418, End: 68.99\n",
      "Start: 69.922, End: 71.55\n",
      "Start: 72.578, End: 75.838\n",
      "Start: 76.61, End: 77.918\n",
      "Start: 78.562, End: 79.454\n",
      "Start: 79.746, End: 81.086\n",
      "Start: 82.05, End: 83.902\n",
      "Start: 84.738, End: 86.462\n",
      "Start: 87.586, End: 90.782\n",
      "Start: 91.746, End: 96.542\n",
      "Start: 97.73, End: 98.27\n",
      "Start: 99.586, End: 100.03\n",
      "Start: 100.162, End: 100.862\n",
      "Start: 101.794, End: 103.454\n",
      "Start: 104.898, End: 107.486\n",
      "Start: 108.226, End: 109.854\n",
      "Start: 114.274, End: 114.75\n",
      "Start: 114.914, End: 116.19\n",
      "Start: 117.154, End: 119.358\n",
      "Start: 120.194, End: 120.67\n",
      "Start: 120.834, End: 121.886\n",
      "Start: 122.978, End: 127.102\n"
     ]
    }
   ],
   "source": [
    "waveform, _ = read_audio(\"./mono_file.wav\")\n",
    "\n",
    "vad_service = VadService()\n",
    "\n",
    "speech_ts, _ = vad_service(waveform, True)\n",
    "for ts in speech_ts:\n",
    "    _ts = ts[0]\n",
    "    print(\n",
    "        f\"Start: {sr2s(_ts['start'])}, End: {sr2s(_ts['end'])}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length_in_sec = [1.5, 1.25, 1.0, 0.75, 0.5] # Window length(s) in sec (floating-point number). either a number or a list. ex) 1.5 or [1.5,1.0,0.5]\n",
    "shift_length_in_sec = [0.75, 0.625, 0.5, 0.375, 0.25] # Shift length(s) in sec (floating-point number). either a number or a list. ex) 0.75 or [0.75,0.5,0.25]\n",
    "multiscale_weights = [1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_lengths_in_sec = window_length_in_sec\n",
    "shift_lengths_in_sec = shift_length_in_sec\n",
    "\n",
    "window_lengths, shift_lengths, multiscale_weights = (\n",
    "    window_lengths_in_sec,\n",
    "    shift_lengths_in_sec,\n",
    "    multiscale_weights,\n",
    ")\n",
    "\n",
    "multiscale_args_dict = {'use_single_scale_clustering': False}\n",
    "scale_dict = {k: (w, s) for k, (w, s) in enumerate(zip(window_lengths, shift_lengths))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_subsegments(segment_start: float, segment_end: float, window: float, shift: float) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Return a list of subsegments based on the segment start and end time and the window and shift length.\n",
    "\n",
    "    Args:\n",
    "        segment_start (float): Segment start time.\n",
    "        segment_end (float): Segment end time.\n",
    "        window (float): Window length.\n",
    "        shift (float): Shift length.\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: List of subsegments with start time and duration.\n",
    "    \"\"\"\n",
    "    start = segment_start\n",
    "    duration = segment_end - segment_start\n",
    "    base = math.ceil((duration - window) / shift)\n",
    "    \n",
    "    subsegments: List[List[float]] = []\n",
    "    slices = 1 if base < 0 else base + 1\n",
    "    for slice_id in range(slices):\n",
    "        end = start + window\n",
    "\n",
    "        if end > segment_end:\n",
    "            end = segment_end\n",
    "\n",
    "        subsegments.append([start, end - start])\n",
    "\n",
    "        start = segment_start + (slice_id + 1) * shift\n",
    "\n",
    "    return subsegments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_segmentation(\n",
    "    vad_outputs: List[dict],\n",
    "    window: float,\n",
    "    shift: float,\n",
    "    min_subsegment_duration: float = 0.05,\n",
    ") -> List[dict]:\n",
    "    \"\"\"\"\"\"\n",
    "    scale_segment = []\n",
    "    for segment in vad_outputs:\n",
    "        segment_start, segment_end = sr2s(segment[\"start\"]), sr2s(segment[\"end\"])\n",
    "        subsegments = get_subsegments(segment_start, segment_end, window, shift)\n",
    "\n",
    "        for subsegment in subsegments:\n",
    "            start, duration = subsegment\n",
    "            if duration > min_subsegment_duration:\n",
    "                scale_segment.append({\"offset\": start, \"duration\": duration})\n",
    "\n",
    "    return scale_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msdd_infer_collate_fn(self, batch):\n",
    "    \"\"\"\n",
    "    Collate batch of feats (speaker embeddings), feature lengths, target label sequences and cluster-average embeddings.\n",
    "\n",
    "    Args:\n",
    "        batch (tuple):\n",
    "            Batch tuple containing feats, feats_len, targets and ms_avg_embs.\n",
    "    Returns:\n",
    "        feats (torch.tensor):\n",
    "            Collated speaker embedding with unified length.\n",
    "        feats_len (torch.tensor):\n",
    "            The actual length of each embedding sequence without zero padding.\n",
    "        targets (torch.tensor):\n",
    "            Groundtruth Speaker label for the given input embedding sequence.\n",
    "        ms_avg_embs (torch.tensor):\n",
    "            Cluster-average speaker embedding vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    packed_batch = list(zip(*batch))\n",
    "    feats, feats_len, targets, ms_avg_embs = packed_batch\n",
    "    feats_list, flen_list, targets_list, ms_avg_embs_list = [], [], [], []\n",
    "    max_audio_len = max(feats_len)\n",
    "    max_target_len = max([x.shape[0] for x in targets])\n",
    "\n",
    "    for feature, feat_len, target, ivector in batch:\n",
    "        flen_list.append(feat_len)\n",
    "        ms_avg_embs_list.append(ivector)\n",
    "        if feat_len < max_audio_len:\n",
    "            pad_a = (0, 0, 0, 0, 0, max_audio_len - feat_len)\n",
    "            pad_t = (0, 0, 0, max_target_len - target.shape[0])\n",
    "            padded_feature = torch.nn.functional.pad(feature, pad_a)\n",
    "            padded_target = torch.nn.functional.pad(target, pad_t)\n",
    "            feats_list.append(padded_feature)\n",
    "            targets_list.append(padded_target)\n",
    "        else:\n",
    "            targets_list.append(target.clone().detach())\n",
    "            feats_list.append(feature.clone().detach())\n",
    "\n",
    "    feats = torch.stack(feats_list)\n",
    "    feats_len = torch.tensor(flen_list)\n",
    "    targets = torch.stack(targets_list)\n",
    "    ms_avg_embs = torch.stack(ms_avg_embs_list)\n",
    "    return feats, feats_len, targets, ms_avg_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class _AudioMSDDInferDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that loads a json file containing paths to audio files,\n",
    "    RTTM files and number of speakers. This Dataset class is built for diarization inference and\n",
    "    evaluation. Speaker embedding sequences, segment timestamps, cluster-average speaker embeddings\n",
    "    are loaded from memory and fed into the dataloader.\n",
    "\n",
    "    Example:\n",
    "    {\"audio_filepath\": \"/path/to/audio_0.wav\", \"num_speakers\": 2,\n",
    "    \"rttm_filepath\": \"/path/to/diar_label_0.rttm}\n",
    "    ...\n",
    "    {\"audio_filepath\": \"/path/to/audio_n.wav\", \"num_speakers\": 2,\n",
    "    \"rttm_filepath\": \"/path/to/diar_label_n.rttm}\n",
    "\n",
    "    Args:\n",
    "        manifest_filepath (str):\n",
    "             Path to input manifest json files.\n",
    "        emb_dict (dict):\n",
    "            Dictionary containing cluster-average embeddings and speaker mapping information.\n",
    "        emb_seq (dict):\n",
    "            Dictionary containing multiscale speaker embedding sequence, scale mapping and corresponding segment timestamps.\n",
    "        clus_label_dict (dict):\n",
    "            Subsegment-level (from base-scale) speaker labels from clustering results.\n",
    "        soft_label_thres (float):\n",
    "            A threshold that determines the label of each segment based on RTTM file information.\n",
    "        featurizer:\n",
    "            Featurizer instance for generating features from raw waveform.\n",
    "        seq_eval_mode (bool):\n",
    "            If True, F1 score will be calculated for each speaker pair during inference mode.\n",
    "        window_stride (float):\n",
    "            Window stride for acoustic feature. This value is used for calculating the numbers of feature-level frames.\n",
    "        use_single_scale_clus (bool):\n",
    "            Use only one scale for clustering instead of using multiple scales of embeddings for clustering.\n",
    "        pairwise_infer (bool):\n",
    "            This variable should be True if dataloader is created for an inference task.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\"\"\"\n",
    "        output_types = OrderedDict(\n",
    "            {\n",
    "                \"ms_emb_seq\": NeuralType(('B', 'T', 'C', 'D'), SpectrogramType()),\n",
    "                \"length\": NeuralType(tuple('B'), LengthsType()),\n",
    "                \"ms_avg_embs\": NeuralType(('B', 'C', 'D', 'C'), EncodedRepresentation()),\n",
    "                \"targets\": NeuralType(('B', 'T', 'C'), ProbsType()),\n",
    "            }\n",
    "        )\n",
    "        return output_types\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        manifest_filepath: str,\n",
    "        emb_dict: Dict,\n",
    "        emb_seq: Dict,\n",
    "        clus_label_dict: Dict,\n",
    "        soft_label_thres: float,\n",
    "        seq_eval_mode: bool,\n",
    "        window_stride: float,\n",
    "        use_single_scale_clus: bool,\n",
    "        pairwise_infer: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.collection = DiarizationSpeechLabel(\n",
    "            manifests_files=manifest_filepath.split(','),\n",
    "            emb_dict=emb_dict,\n",
    "            clus_label_dict=clus_label_dict,\n",
    "            seq_eval_mode=seq_eval_mode,\n",
    "            pairwise_infer=pairwise_infer,\n",
    "        )\n",
    "        self.emb_dict = emb_dict\n",
    "        self.emb_seq = emb_seq\n",
    "        self.clus_label_dict = clus_label_dict\n",
    "        self.round_digits = 2\n",
    "        self.decim = 10 ** self.round_digits\n",
    "        self.frame_per_sec = int(1 / window_stride)\n",
    "        self.soft_label_thres = soft_label_thres\n",
    "        self.pairwise_infer = pairwise_infer\n",
    "        self.max_spks = 2\n",
    "        self.use_single_scale_clus = use_single_scale_clus\n",
    "        self.seq_eval_mode = seq_eval_mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.collection)\n",
    "\n",
    "    def parse_rttm_multiscale(self, sample):\n",
    "        \"\"\"\n",
    "        Generate target tensor variable by extracting groundtruth diarization labels from an RTTM file.\n",
    "        This function is only used when ``self.seq_eval_mode=True`` and RTTM files are provided. This function converts\n",
    "        (start, end, speaker_id) format into base-scale (the finest scale) segment level diarization label in a matrix\n",
    "        form to create target matrix.\n",
    "\n",
    "        Args:\n",
    "            sample:\n",
    "                DiarizationSpeechLabel instance containing sample information such as audio filepath and RTTM filepath.\n",
    "            target_spks (tuple):\n",
    "                Two Indices of targeted speakers for evaluation.\n",
    "                Example of target_spks: (2, 3)\n",
    "        Returns:\n",
    "            seg_target (torch.tensor):\n",
    "                Tensor variable containing hard-labels of speaker activity in each base-scale segment.\n",
    "        \"\"\"\n",
    "        if sample.rttm_file is None:\n",
    "            raise ValueError(f\"RTTM file is not provided for this sample {sample}\")\n",
    "        rttm_lines = open(sample.rttm_file).readlines()\n",
    "        uniq_id = os.path.splitext(os.path.basename(sample.rttm_file))[0]\n",
    "        mapping_dict = self.emb_dict[max(self.emb_dict.keys())][uniq_id]['mapping']\n",
    "        rttm_timestamps = extract_seg_info_from_rttm(uniq_id, rttm_lines, mapping_dict, sample.target_spks)\n",
    "        fr_level_target = assign_frame_level_spk_vector(\n",
    "            rttm_timestamps, self.round_digits, self.frame_per_sec, sample.target_spks\n",
    "        )\n",
    "        seg_target = self.get_diar_target_labels_from_fr_target(uniq_id, fr_level_target)\n",
    "        return seg_target\n",
    "\n",
    "    def get_diar_target_labels_from_fr_target(self, uniq_id, fr_level_target):\n",
    "        \"\"\"\n",
    "        Generate base-scale level binary diarization label from frame-level target matrix. For the given frame-level\n",
    "        speaker target matrix fr_level_target, we count the number of frames that belong to each speaker and calculate\n",
    "        ratios for each speaker into the `soft_label_vec` variable. Finally, `soft_label_vec` variable is compared with `soft_label_thres`\n",
    "        to determine whether a label vector should contain 0 or 1 for each speaker bin. Note that seg_target variable has\n",
    "        dimension of (number of base-scale segments x 2) dimension.\n",
    "\n",
    "        Example of seg_target:\n",
    "            [[0., 1.], [0., 1.], [1., 1.], [1., 0.], [1., 0.], ..., [0., 1.]]\n",
    "\n",
    "        Args:\n",
    "            uniq_id (str):\n",
    "                Unique file ID that refers to an input audio file and corresponding RTTM (Annotation) file.\n",
    "            fr_level_target (torch.tensor):\n",
    "                frame-level binary speaker annotation (1: exist 0: non-exist) generated from RTTM file.\n",
    "\n",
    "        Returns:\n",
    "            seg_target (torch.tensor):\n",
    "                Tensor variable containing binary hard-labels of speaker activity in each base-scale segment.\n",
    "\n",
    "        \"\"\"\n",
    "        if fr_level_target is None:\n",
    "            return None\n",
    "        else:\n",
    "            seg_target_list = []\n",
    "            for (seg_stt, seg_end, label_int) in self.clus_label_dict[uniq_id]:\n",
    "                seg_stt_fr, seg_end_fr = int(seg_stt * self.frame_per_sec), int(seg_end * self.frame_per_sec)\n",
    "                soft_label_vec = torch.sum(fr_level_target[seg_stt_fr:seg_end_fr, :], axis=0) / (\n",
    "                    seg_end_fr - seg_stt_fr\n",
    "                )\n",
    "                label_vec = (soft_label_vec > self.soft_label_thres).int()\n",
    "                seg_target_list.append(label_vec)\n",
    "            seg_target = torch.stack(seg_target_list)\n",
    "            return seg_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.collection[index]\n",
    "        if sample.offset is None:\n",
    "            sample.offset = 0\n",
    "\n",
    "        uniq_id = os.path.splitext(os.path.basename(sample.audio_file))[0]\n",
    "        scale_n = len(self.emb_dict.keys())\n",
    "        _avg_embs = torch.stack([self.emb_dict[scale_index][uniq_id]['avg_embs'] for scale_index in range(scale_n)])\n",
    "\n",
    "        if self.pairwise_infer:\n",
    "            avg_embs = _avg_embs[:, :, self.collection[index].target_spks]\n",
    "        else:\n",
    "            avg_embs = _avg_embs\n",
    "\n",
    "        if avg_embs.shape[2] > self.max_spks:\n",
    "            raise ValueError(\n",
    "                f\" avg_embs.shape[2] {avg_embs.shape[2]} should be less than or equal to self.max_num_speakers {self.max_spks}\"\n",
    "            )\n",
    "\n",
    "        feats = []\n",
    "        for scale_index in range(scale_n):\n",
    "            repeat_mat = self.emb_seq[\"session_scale_mapping\"][uniq_id][scale_index]\n",
    "            feats.append(self.emb_seq[scale_index][uniq_id][repeat_mat, :])\n",
    "        feats_out = torch.stack(feats).permute(1, 0, 2)\n",
    "        feats_len = feats_out.shape[0]\n",
    "\n",
    "        if self.seq_eval_mode:\n",
    "            targets = self.parse_rttm_multiscale(sample)\n",
    "        else:\n",
    "            targets = torch.zeros(feats_len, 2).float()\n",
    "\n",
    "        return feats_out, feats_len, targets, avg_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioToSpeechMSDDInferDataset(_AudioMSDDInferDataset):\n",
    "    \"\"\"\n",
    "    Dataset class that loads a json file containing paths to audio files,\n",
    "    rttm files and number of speakers. The created labels are used for diarization inference.\n",
    "\n",
    "    Example:\n",
    "    {\"audio_filepath\": \"/path/to/audio_0.wav\", \"num_speakers\": 2,\n",
    "    \"rttm_filepath\": \"/path/to/diar_label_0.rttm}\n",
    "    ...\n",
    "    {\"audio_filepath\": \"/path/to/audio_n.wav\", \"num_speakers\": 2,\n",
    "    \"rttm_filepath\": \"/path/to/diar_label_n.rttm}\n",
    "\n",
    "    Args:\n",
    "        manifest_filepath (str):\n",
    "            Path to input manifest json files.\n",
    "        emb_dict (dict):\n",
    "            Dictionary containing cluster-average embeddings and speaker mapping information.\n",
    "        emb_seq (dict):\n",
    "            Dictionary containing multiscale speaker embedding sequence, scale mapping and corresponding segment timestamps.\n",
    "        clus_label_dict (dict):\n",
    "            Subsegment-level (from base-scale) speaker labels from clustering results.\n",
    "        soft_label_thres (float):\n",
    "            Threshold that determines speaker labels of segments depending on the overlap with groundtruth speaker timestamps.\n",
    "        featurizer:\n",
    "            Featurizer instance for generating features from raw waveform.\n",
    "        use_single_scale_clus (bool):\n",
    "            Use only one scale for clustering instead of using multiple scales of embeddings for clustering.\n",
    "        seq_eval_mode (bool):\n",
    "            If True, F1 score will be calculated for each speaker pair during inference mode.\n",
    "        window_stride (float):\n",
    "            Window stride for acoustic feature. This value is used for calculating the numbers of feature-level frames.\n",
    "        pairwise_infer (bool):\n",
    "            If True, this Dataset class operates in inference mode. In inference mode, a set of speakers in the input audio\n",
    "            is split into multiple pairs of speakers and speaker tuples (e.g. 3 speakers: [(0,1), (1,2), (0,2)]) and then\n",
    "            fed into the MSDD to merge the individual results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        manifest_filepath: str,\n",
    "        emb_dict: Dict,\n",
    "        emb_seq: Dict,\n",
    "        clus_label_dict: Dict,\n",
    "        soft_label_thres: float,\n",
    "        use_single_scale_clus: bool,\n",
    "        seq_eval_mode: bool,\n",
    "        window_stride: float,\n",
    "        pairwise_infer: bool,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            manifest_filepath=manifest_filepath,\n",
    "            emb_dict=emb_dict,\n",
    "            emb_seq=emb_seq,\n",
    "            clus_label_dict=clus_label_dict,\n",
    "            soft_label_thres=soft_label_thres,\n",
    "            use_single_scale_clus=use_single_scale_clus,\n",
    "            window_stride=window_stride,\n",
    "            seq_eval_mode=seq_eval_mode,\n",
    "            pairwise_infer=pairwise_infer,\n",
    "        )\n",
    "\n",
    "    def msdd_infer_collate_fn(self, batch):\n",
    "        return _msdd_infer_collate_fn(self, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.emb_sess_test_dict, self.emb_seq_test, self.clus_test_label_dict, _ = self.run_clustering_diarizer(\n",
    "    self._cfg_msdd.test_ds.manifest_filepath, self._cfg_msdd.test_ds.emb_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.data.audio_to_diar_label import AudioToSpeechMSDDInferDataset\n",
    "\n",
    "dataset = AudioToSpeechMSDDInferDataset(\n",
    "    manifest_filepath=\"infer_out_dir/speaker_outputs/subsegments_scale4.json\",\n",
    "    emb_dict=emb_dict,\n",
    "    clus_label_dict=clus_label_dict,\n",
    "    emb_seq=emb_seq,\n",
    "    soft_label_thres=config.soft_label_thres,\n",
    "    seq_eval_mode=config.seq_eval_mode,\n",
    "    window_stride=self._cfg.preprocessor.window_stride,\n",
    "    use_single_scale_clus=False,\n",
    "    pairwise_infer=pairwise_infer,\n",
    ")\n",
    "data_collection = dataset.collection\n",
    "collate_ds = dataset\n",
    "collate_fn = collate_ds.msdd_infer_collate_fn\n",
    "batch_size = config['batch_size']\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=config.get('drop_last', False),\n",
    "    shuffle=shuffle,\n",
    "    num_workers=config.get('num_workers', 0),\n",
    "    pin_memory=config.get('pin_memory', False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "\n",
    "speaker_model = EncDecSpeakerLabelModel.from_pretrained(\n",
    "    model_name=\"titanet_large\", map_location=None\n",
    ")\n",
    "\n",
    "def _extract_embeddings(scale_segments: List[dict], scale_idx: int, num_scales: int):\n",
    "    \"\"\"\n",
    "    This method extracts speaker embeddings from segments passed through manifest_file\n",
    "    Optionally you may save the intermediate speaker embeddings for debugging or any use. \n",
    "    \"\"\"\n",
    "\n",
    "    # self._setup_spkr_test_data(manifest_file)\n",
    "    # self.embeddings = {}\n",
    "    # self._speaker_model.eval()\n",
    "    # self.time_stamps = {}\n",
    "\n",
    "    # all_embs = torch.empty([0])\n",
    "    # for test_batch in tqdm(\n",
    "    #     self._speaker_model.test_dataloader(),\n",
    "    #     desc=f'[{scale_idx+1}/{num_scales}] extract embeddings',\n",
    "    #     leave=True,\n",
    "    #     disable=not self.verbose,\n",
    "    # ):\n",
    "    #     test_batch = [x.to(self._speaker_model.device) for x in test_batch]\n",
    "    #     audio_signal, audio_signal_len, labels, slices = test_batch\n",
    "    #     with autocast():\n",
    "    #         _, embs = self._speaker_model.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
    "    #         emb_shape = embs.shape[-1]\n",
    "    #         embs = embs.view(-1, emb_shape)\n",
    "    #         all_embs = torch.cat((all_embs, embs.cpu().detach()), dim=0)\n",
    "    #     del test_batch\n",
    "    \n",
    "    # TODO: Create a dataloader with the audio file\n",
    "    # TODO: Run the speaker model on the dataloader to get embeddings\n",
    "\n",
    "    embeddings, time_stamps = [], []\n",
    "    for i, segment in enumerate(scale_segments):\n",
    "        if i == 0:\n",
    "            embeddings = all_embs[i].view(1, -1)\n",
    "        else:\n",
    "            embeddings = torch.cat((embeddings, all_embs[i].view(1, -1)))\n",
    "\n",
    "        time_stamps.append([segment['offset'], segment['duration']])\n",
    "        \n",
    "        # uniq_name = get_uniqname_from_filepath(dic['audio_filepath'])\n",
    "        # if uniq_name in self.embeddings:\n",
    "        #     self.embeddings[uniq_name] = torch.cat((self.embeddings[uniq_name], all_embs[i].view(1, -1)))\n",
    "        # else:\n",
    "        #     self.embeddings[uniq_name] = all_embs[i].view(1, -1)\n",
    "        # if uniq_name not in self.time_stamps:\n",
    "        #     self.time_stamps[uniq_name] = []\n",
    "        \n",
    "        # start = dic['offset']\n",
    "        # end = start + dic['duration']\n",
    "        # self.time_stamps[uniq_name].append([start, end])\n",
    "\n",
    "    if self._speaker_params.save_embeddings:\n",
    "        embedding_dir = os.path.join(self._speaker_dir, 'embeddings')\n",
    "        if not os.path.exists(embedding_dir):\n",
    "            os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "        prefix = get_uniqname_from_filepath(manifest_file)\n",
    "        name = os.path.join(embedding_dir, prefix)\n",
    "        self._embeddings_file = name + f'_embeddings.pkl'\n",
    "        pkl.dump(self.embeddings, open(self._embeddings_file, 'wb'))\n",
    "        logging.info(\"Saved embedding files to {}\".format(embedding_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5 0.75\n",
      "[{'offset': 12.514, 'duration': 0.47600000000000087}, {'offset': 13.25, 'duration': 1.1159999999999997}, {'offset': 15.042, 'duration': 1.5000000000000018}, {'offset': 15.792, 'duration': 1.0699999999999985}, {'offset': 17.762, 'duration': 1.0519999999999996}, {'offset': 19.426, 'duration': 1.3399999999999999}, {'offset': 21.666, 'duration': 1.5}, {'offset': 22.416, 'duration': 1.5}, {'offset': 23.166, 'duration': 1.5}, {'offset': 23.916, 'duration': 0.9139999999999979}, {'offset': 26.178, 'duration': 1.5}, {'offset': 26.928, 'duration': 1.5}, {'offset': 27.678, 'duration': 1.5}, {'offset': 28.428, 'duration': 1.4579999999999984}, {'offset': 30.786, 'duration': 1.5}, {'offset': 31.536, 'duration': 1.485999999999997}, {'offset': 34.146, 'duration': 1.5}, {'offset': 34.896, 'duration': 1.5}, {'offset': 35.646, 'duration': 1.5}, {'offset': 36.396, 'duration': 0.8179999999999978}, {'offset': 38.338, 'duration': 1.5}, {'offset': 39.088, 'duration': 1.2299999999999969}, {'offset': 41.218, 'duration': 1.5}, {'offset': 41.968, 'duration': 0.813999999999993}, {'offset': 43.682, 'duration': 0.6359999999999957}, {'offset': 45.73, 'duration': 0.7640000000000029}, {'offset': 47.778, 'duration': 1.5}, {'offset': 48.528, 'duration': 1.5}, {'offset': 49.278, 'duration': 1.088000000000001}, {'offset': 51.106, 'duration': 1.5}, {'offset': 51.856, 'duration': 1.0700000000000003}, {'offset': 53.954, 'duration': 1.5}, {'offset': 54.704, 'duration': 0.8780000000000001}, {'offset': 55.682, 'duration': 1.2439999999999998}, {'offset': 57.954, 'duration': 1.5}, {'offset': 58.704, 'duration': 1.5}, {'offset': 59.454, 'duration': 0.8320000000000007}, {'offset': 61.154, 'duration': 1.5}, {'offset': 61.904, 'duration': 1.5}, {'offset': 62.654, 'duration': 1.499999999999993}, {'offset': 63.404, 'duration': 0.8500000000000014}, {'offset': 65.026, 'duration': 1.5}, {'offset': 65.776, 'duration': 1.5}, {'offset': 66.526, 'duration': 1.088000000000008}, {'offset': 68.418, 'duration': 0.5719999999999885}, {'offset': 69.922, 'duration': 1.5}, {'offset': 70.672, 'duration': 0.8780000000000001}, {'offset': 72.578, 'duration': 1.5}, {'offset': 73.328, 'duration': 1.5}, {'offset': 74.078, 'duration': 1.5}, {'offset': 74.828, 'duration': 1.009999999999991}, {'offset': 76.61, 'duration': 1.308000000000007}, {'offset': 78.562, 'duration': 0.8919999999999959}, {'offset': 79.746, 'duration': 1.3400000000000034}, {'offset': 82.05, 'duration': 1.5}, {'offset': 82.8, 'duration': 1.1020000000000039}, {'offset': 84.738, 'duration': 1.5}, {'offset': 85.488, 'duration': 0.9740000000000038}, {'offset': 87.586, 'duration': 1.5}, {'offset': 88.336, 'duration': 1.5}, {'offset': 89.086, 'duration': 1.5}, {'offset': 89.836, 'duration': 0.945999999999998}, {'offset': 91.746, 'duration': 1.5}, {'offset': 92.496, 'duration': 1.5}, {'offset': 93.246, 'duration': 1.5}, {'offset': 93.996, 'duration': 1.5}, {'offset': 94.746, 'duration': 1.5}, {'offset': 95.496, 'duration': 1.0460000000000065}, {'offset': 97.73, 'duration': 0.539999999999992}, {'offset': 99.586, 'duration': 0.4440000000000026}, {'offset': 100.162, 'duration': 0.6999999999999886}, {'offset': 101.794, 'duration': 1.5}, {'offset': 102.544, 'duration': 0.9099999999999966}, {'offset': 104.898, 'duration': 1.5}, {'offset': 105.648, 'duration': 1.5}, {'offset': 106.398, 'duration': 1.088000000000008}, {'offset': 108.226, 'duration': 1.5}, {'offset': 108.976, 'duration': 0.8780000000000001}, {'offset': 114.274, 'duration': 0.4759999999999991}, {'offset': 114.914, 'duration': 1.2759999999999962}, {'offset': 117.154, 'duration': 1.5}, {'offset': 117.904, 'duration': 1.4540000000000077}, {'offset': 120.194, 'duration': 0.4759999999999991}, {'offset': 120.834, 'duration': 1.0519999999999925}, {'offset': 122.978, 'duration': 1.5}, {'offset': 123.728, 'duration': 1.5}, {'offset': 124.478, 'duration': 1.5}, {'offset': 125.228, 'duration': 1.5}, {'offset': 125.978, 'duration': 1.1240000000000094}]\n"
     ]
    }
   ],
   "source": [
    "waveform, _ = read_audio(\"./mono_file.wav\")\n",
    "vad_service = VadService()\n",
    "\n",
    "vad_outputs, _ = vad_service(waveform, False)\n",
    "\n",
    "scales = scale_dict.items()\n",
    "for scale_idx, (window, shift) in scales:\n",
    "    print(scale_idx, window, shift)\n",
    "    # Segmentation for the current scale (scale_idx)\n",
    "    scale_segments = _run_segmentation(vad_outputs, window, shift)\n",
    "\n",
    "    # Embedding Extraction for the current scale (scale_idx)\n",
    "    embeddings = _extract_embeddings(scale_segments, scale_idx, len(scales))\n",
    "\n",
    "    # self.multiscale_embeddings_and_timestamps[scale_idx] = [self.embeddings, self.time_stamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker model for cluster embedding is EncDecSpeakerLabelModel\n",
    "# check: https://github.com/NVIDIA/NeMo/blob/2cc09425aba3e9b3cfdba43a3188eaef58227055/nemo/collections/asr/models/label_models.py#L201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2134518]) <class 'torch.Tensor'>\n",
      "(2134518,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from nemo.collections.asr.parts.preprocessing import AudioSegment\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def read_audio(filepath: str, sample_rate: int = 16000) -> Tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Read an audio file and return the audio tensor.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the audio file.\n",
    "        sample_rate (int): The sample rate of the audio file. Defaults to 16000.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, float]: The audio tensor and the audio duration.\n",
    "    \"\"\"\n",
    "    wav, sr = torchaudio.load(filepath)\n",
    "\n",
    "    if wav.size(0) > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if sr != sample_rate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)\n",
    "        wav = transform(wav)\n",
    "        sr = sample_rate\n",
    "\n",
    "    audio_duration = float(wav.shape[1]) / sample_rate\n",
    "\n",
    "    return wav.squeeze(0), audio_duration\n",
    "\n",
    "def split_audio_to_samples(\n",
    "    waveform: torch.Tensor, sample_rate: int, duration: float, offset: float = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\"\"\"\n",
    "    if offset > 0:\n",
    "        offset_samples = int(offset * sample_rate)\n",
    "        waveform = waveform[:, offset_samples:]\n",
    "\n",
    "    if duration > 0:\n",
    "        duration_samples = int(duration * sample_rate)\n",
    "        waveform = waveform[:, :duration_samples]\n",
    "\n",
    "    return waveform\n",
    "\n",
    "audio_file = \"mono_file.wav\"\n",
    "offset = 0  # Offset in seconds\n",
    "\n",
    "wav, duration = read_audio(audio_file)\n",
    "samples = wav\n",
    "print(samples.shape, type(samples))\n",
    "\n",
    "samples_nemo = AudioSegment.from_file(audio_file)\n",
    "print(samples_nemo.samples.shape, type(samples_nemo.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_dl_config = {\n",
    "    'manifest_filepath': \"\",\n",
    "    'sample_rate': 16000,\n",
    "    'batch_size': 64,\n",
    "    'trim_silence': False,\n",
    "    'labels': None,\n",
    "    'num_workers': 1,\n",
    "}\n",
    "\n",
    "def __setup_dataloader_from_config(self, config: dict):\n",
    "\n",
    "    dataset = AudioToSpeechLabelDataset(\n",
    "        manifest_filepath=config['manifest_filepath'],\n",
    "        labels=config['labels'],\n",
    "        featurizer=featurizer,\n",
    "        max_duration=config.get('max_duration', None),\n",
    "        min_duration=config.get('min_duration', None),\n",
    "        trim=config.get('trim_silence', False),\n",
    "        normalize_audio=config.get('normalize_audio', False),\n",
    "        cal_labels_occurrence=config.get('cal_labels_occurrence', False),\n",
    "    )\n",
    "    if dataset.labels_occurrence:\n",
    "        self.labels_occurrence = dataset.labels_occurrence\n",
    "\n",
    "    if hasattr(dataset, 'fixed_seq_collate_fn'):\n",
    "        collate_fn = dataset.fixed_seq_collate_fn\n",
    "    else:\n",
    "        collate_fn = dataset.datasets[0].fixed_seq_collate_fn\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=config.get('drop_last', False),\n",
    "        shuffle=shuffle,\n",
    "        num_workers=config.get('num_workers', 0),\n",
    "        pin_memory=config.get('pin_memory', False),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
